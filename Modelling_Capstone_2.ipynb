{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60db2989",
   "metadata": {
    "id": "60db2989"
   },
   "source": [
    "\n",
    " =====\n",
    "### Author: Stephanie Lo \n",
    "#### Date: 3.17.2023"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe321703",
   "metadata": {
    "id": "fe321703"
   },
   "source": [
    "# Table of Contents \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "U7n4EIfiVcf7",
   "metadata": {
    "id": "U7n4EIfiVcf7"
   },
   "outputs": [],
   "source": [
    "%pip install XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "47bba446",
   "metadata": {
    "id": "47bba446"
   },
   "outputs": [],
   "source": [
    "#importing base packages\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import os \n",
    "import time \n",
    "import boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "56c1613b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<botocore.client.S3 at 0x123df9950>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate an S3 client\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "# Check\n",
    "s3_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219defdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#instantiating the client\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# Setting the bucket and file name \n",
    "bucket_name = 'deliverable-slo-bstn-bucket'\n",
    "file_path = 'clean_data.csv'\n",
    "\n",
    "# Use the S3 client to get the object\n",
    "\n",
    "response = s3.get_object(Bucket=bucket_name, Key=file_path)\n",
    "# Read the object data into a Pandas DataFrame\n",
    "df_combined = pd.read_csv(response['Body'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b3fe4ab",
   "metadata": {
    "id": "1b3fe4ab"
   },
   "outputs": [],
   "source": [
    "#Setting figure size & background \n",
    "plt.rcParams['figure.figsize'] = (8.0, 6.0)\n",
    "sns.set_theme(style=\"darkgrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2369ee4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0a8b6e1f",
   "metadata": {
    "id": "0a8b6e1f"
   },
   "source": [
    "## 1.Introduction <a id=\"Intro\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5DAR158xct2F",
   "metadata": {
    "id": "5DAR158xct2F"
   },
   "outputs": [],
   "source": [
    "df_combined = df_combined.drop(columns =\"Unnamed: 0\")\n",
    "df_combined\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e56d9af",
   "metadata": {
    "id": "8e56d9af"
   },
   "outputs": [],
   "source": [
    "#directing to the right file path\n",
    "#os.chdir(\"/Users/mac/Desktop/Data/CAPSTONE/clean/\")\n",
    "#cwd = os.getcwd() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3429c40c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 310
    },
    "id": "3429c40c",
    "outputId": "fb3d1d35-9b00-428a-b596-f39fec6dc0b6"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>overall</th>\n",
       "      <th>vote</th>\n",
       "      <th>verified</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>trending_asin</th>\n",
       "      <th>price_USD</th>\n",
       "      <th>ranking</th>\n",
       "      <th>product_description</th>\n",
       "      <th>also_buy_counts</th>\n",
       "      <th>...</th>\n",
       "      <th>m_Pureology</th>\n",
       "      <th>m_RUSK</th>\n",
       "      <th>m_Red Flower</th>\n",
       "      <th>m_Rene Furterer</th>\n",
       "      <th>m_Stila</th>\n",
       "      <th>m_StriVectin</th>\n",
       "      <th>m_TS</th>\n",
       "      <th>m_The Art of Shaving</th>\n",
       "      <th>m_Vichy</th>\n",
       "      <th>m_theBalm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>Bought for my daughter.</td>\n",
       "      <td>0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>4324.0</td>\n",
       "      <td>After a long day of handling thorny situations...</td>\n",
       "      <td>56.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>Great cream for the skin.</td>\n",
       "      <td>0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>4324.0</td>\n",
       "      <td>After a long day of handling thorny situations...</td>\n",
       "      <td>56.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 63 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  overall  vote  verified                 reviewText  \\\n",
       "0           0      5.0   0.0         1    Bought for my daughter.   \n",
       "1           1      5.0   0.0         1  Great cream for the skin.   \n",
       "\n",
       "   trending_asin  price_USD  ranking  \\\n",
       "0              0       30.0   4324.0   \n",
       "1              0       30.0   4324.0   \n",
       "\n",
       "                                 product_description  also_buy_counts  ...  \\\n",
       "0  After a long day of handling thorny situations...             56.0  ...   \n",
       "1  After a long day of handling thorny situations...             56.0  ...   \n",
       "\n",
       "   m_Pureology  m_RUSK  m_Red Flower  m_Rene Furterer  m_Stila  m_StriVectin  \\\n",
       "0          0.0     0.0           0.0              0.0      0.0           0.0   \n",
       "1          0.0     0.0           0.0              0.0      0.0           0.0   \n",
       "\n",
       "   m_TS  m_The Art of Shaving  m_Vichy  m_theBalm  \n",
       "0   0.0                   0.0      0.0        0.0  \n",
       "1   0.0                   0.0      0.0        0.0  \n",
       "\n",
       "[2 rows x 63 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#reading in the datasets\n",
    "#df_combined = pd.read_csv(\"clean_data.csv\")\n",
    "\n",
    "#df_combined.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "113a7099",
   "metadata": {},
   "outputs": [],
   "source": [
    "isNumeric = df_combined.select_dtypes('number')\n",
    "short_isNumeric = isNumeric.iloc[:,:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "79836e90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1080x1080 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(15,15))\n",
    "sns.heatmap(short_isNumeric.corr(), annot=True, center=0, vmin=-1, vmax=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "U__kxY0N_28A",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U__kxY0N_28A",
    "outputId": "e1d39ba9-6e82-4f66-c0fe-77be902ce857"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cp: cannot stat '/content/drive/MyDrive/colab/Capstone/Clean/clean_data2.csv': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!cp  \"/content/drive/MyDrive/colab/Capstone/Clean/clean_data2.csv\" \"clean_data.csv\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "JEw8ltq7__KF",
   "metadata": {
    "id": "JEw8ltq7__KF"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d7b89950",
   "metadata": {
    "id": "d7b89950"
   },
   "source": [
    "#### Null and Duplicate Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e7315ae",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9e7315ae",
    "outputId": "4dfcf2b5-efed-458b-868a-6d10b4829679"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The final data shape has 139348 columns and 63 rows\n"
     ]
    }
   ],
   "source": [
    "print(f\"The final data shape has {df_combined.shape[0]} columns and {df_combined.shape[1]} rows\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b39e5a60",
   "metadata": {
    "id": "b39e5a60"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df_combined= df_combined.drop(columns = \"Unnamed: 0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "506ae4ca",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "506ae4ca",
    "outputId": "338ce807-38d3-466a-bf63-119381753f3b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 139348 entries, 0 to 139347\n",
      "Data columns (total 62 columns):\n",
      " #   Column                Non-Null Count   Dtype  \n",
      "---  ------                --------------   -----  \n",
      " 0   overall               139348 non-null  float64\n",
      " 1   vote                  139348 non-null  float64\n",
      " 2   verified              139348 non-null  int64  \n",
      " 3   reviewText            139348 non-null  object \n",
      " 4   trending_asin         139348 non-null  int64  \n",
      " 5   price_USD             139348 non-null  float64\n",
      " 6   ranking               139348 non-null  float64\n",
      " 7   product_description   139348 non-null  object \n",
      " 8   also_buy_counts       139348 non-null  float64\n",
      " 9   also_view_counts      139348 non-null  float64\n",
      " 10  c_accessories         139348 non-null  float64\n",
      " 11  c_bath and shower     139348 non-null  float64\n",
      " 12  c_fragance            139348 non-null  float64\n",
      " 13  c_haircare            139348 non-null  float64\n",
      " 14  c_makeup              139348 non-null  float64\n",
      " 15  c_menscare            139348 non-null  float64\n",
      " 16  c_nails and hands     139348 non-null  float64\n",
      " 17  c_other               139348 non-null  float64\n",
      " 18  c_skincare            139348 non-null  float64\n",
      " 19  c_suncare             139348 non-null  float64\n",
      " 20  m_AHAVA               139348 non-null  float64\n",
      " 21  m_Archipelago         139348 non-null  float64\n",
      " 22  m_BaBylissPro         139348 non-null  float64\n",
      " 23  m_Bioelements         139348 non-null  float64\n",
      " 24  m_Bliss               139348 non-null  float64\n",
      " 25  m_Burberry            139348 non-null  float64\n",
      " 26  m_Butter London       139348 non-null  float64\n",
      " 27  m_CHI                 139348 non-null  float64\n",
      " 28  m_CND                 139348 non-null  float64\n",
      " 29  m_Calvin Klein        139348 non-null  float64\n",
      " 30  m_Crabtree & Evelyn   139348 non-null  float64\n",
      " 31  m_Davines             139348 non-null  float64\n",
      " 32  m_Elemis              139348 non-null  float64\n",
      " 33  m_Elizabeth Arden     139348 non-null  float64\n",
      " 34  m_Glo Skin            139348 non-null  float64\n",
      " 35  m_Jane Iredale        139348 non-null  float64\n",
      " 36  m_Japonesque          139348 non-null  float64\n",
      " 37  m_Julep               139348 non-null  float64\n",
      " 38  m_L'occitane          139348 non-null  float64\n",
      " 39  m_LAFCO               139348 non-null  float64\n",
      " 40  m_LORAC               139348 non-null  float64\n",
      " 41  m_La Roche Posay      139348 non-null  float64\n",
      " 42  m_Laura Geller        139348 non-null  float64\n",
      " 43  m_Mario Badescu       139348 non-null  float64\n",
      " 44  m_Mustela             139348 non-null  float64\n",
      " 45  m_NUXE                139348 non-null  float64\n",
      " 46  m_Natura Bisse        139348 non-null  float64\n",
      " 47  m_OPI                 139348 non-null  float64\n",
      " 48  m_Oribe               139348 non-null  float64\n",
      " 49  m_Other               139348 non-null  float64\n",
      " 50  m_PCA Skin            139348 non-null  float64\n",
      " 51  m_Paul Mitchell       139348 non-null  float64\n",
      " 52  m_Pureology           139348 non-null  float64\n",
      " 53  m_RUSK                139348 non-null  float64\n",
      " 54  m_Red Flower          139348 non-null  float64\n",
      " 55  m_Rene Furterer       139348 non-null  float64\n",
      " 56  m_Stila               139348 non-null  float64\n",
      " 57  m_StriVectin          139348 non-null  float64\n",
      " 58  m_TS                  139348 non-null  float64\n",
      " 59  m_The Art of Shaving  139348 non-null  float64\n",
      " 60  m_Vichy               139348 non-null  float64\n",
      " 61  m_theBalm             139348 non-null  float64\n",
      "dtypes: float64(58), int64(2), object(2)\n",
      "memory usage: 65.9+ MB\n"
     ]
    }
   ],
   "source": [
    "#only taking numerical values \n",
    "df_combined.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c75fbd0",
   "metadata": {
    "id": "0c75fbd0"
   },
   "source": [
    "Let's just quickly check for any null values and duplicates quickly: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "522169ae",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "522169ae",
    "outputId": "5cf869a4-cfd9-428a-ee10-946f0b26bf37"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "null values = overall                 0\n",
      "vote                    0\n",
      "verified                0\n",
      "reviewText              0\n",
      "trending_asin           0\n",
      "                       ..\n",
      "m_StriVectin            0\n",
      "m_TS                    0\n",
      "m_The Art of Shaving    0\n",
      "m_Vichy                 0\n",
      "m_theBalm               0\n",
      "Length: 62, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#null values check \n",
    "print(f\"null values = {df_combined.isna().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "21016bd1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 956
    },
    "id": "21016bd1",
    "outputId": "901b0e59-9bf9-4e94-b843-a9c6fd32cf65"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>overall</th>\n",
       "      <th>vote</th>\n",
       "      <th>verified</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>trending_asin</th>\n",
       "      <th>price_USD</th>\n",
       "      <th>ranking</th>\n",
       "      <th>product_description</th>\n",
       "      <th>also_buy_counts</th>\n",
       "      <th>also_view_counts</th>\n",
       "      <th>...</th>\n",
       "      <th>m_Pureology</th>\n",
       "      <th>m_RUSK</th>\n",
       "      <th>m_Red Flower</th>\n",
       "      <th>m_Rene Furterer</th>\n",
       "      <th>m_Stila</th>\n",
       "      <th>m_StriVectin</th>\n",
       "      <th>m_TS</th>\n",
       "      <th>m_The Art of Shaving</th>\n",
       "      <th>m_Vichy</th>\n",
       "      <th>m_theBalm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>Bought for my daughter.</td>\n",
       "      <td>0</td>\n",
       "      <td>30.00</td>\n",
       "      <td>4324.0</td>\n",
       "      <td>After a long day of handling thorny situations...</td>\n",
       "      <td>56.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>Great cream for the skin.</td>\n",
       "      <td>0</td>\n",
       "      <td>30.00</td>\n",
       "      <td>4324.0</td>\n",
       "      <td>After a long day of handling thorny situations...</td>\n",
       "      <td>56.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>Maybe Im just not familiar with this brand, bu...</td>\n",
       "      <td>0</td>\n",
       "      <td>30.00</td>\n",
       "      <td>4324.0</td>\n",
       "      <td>After a long day of handling thorny situations...</td>\n",
       "      <td>56.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>This is one of my favorite creams. Highly reco...</td>\n",
       "      <td>0</td>\n",
       "      <td>30.00</td>\n",
       "      <td>4324.0</td>\n",
       "      <td>After a long day of handling thorny situations...</td>\n",
       "      <td>56.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>I simply love this lotion!  Originally, I rece...</td>\n",
       "      <td>0</td>\n",
       "      <td>30.00</td>\n",
       "      <td>4324.0</td>\n",
       "      <td>After a long day of handling thorny situations...</td>\n",
       "      <td>56.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139343</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>Meh!  Flat iron works better</td>\n",
       "      <td>0</td>\n",
       "      <td>21.29</td>\n",
       "      <td>171884.0</td>\n",
       "      <td>Thermally activated smoothing treatment combin...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139344</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>It doesn't grip the hair - I'm very disappointed.</td>\n",
       "      <td>0</td>\n",
       "      <td>21.29</td>\n",
       "      <td>171884.0</td>\n",
       "      <td>Thermally activated smoothing treatment combin...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139345</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>Really works! I have curly hair and you can ge...</td>\n",
       "      <td>0</td>\n",
       "      <td>21.29</td>\n",
       "      <td>171884.0</td>\n",
       "      <td>Thermally activated smoothing treatment combin...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139346</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>DIDNT STRAIGHTEN HAIR</td>\n",
       "      <td>0</td>\n",
       "      <td>21.29</td>\n",
       "      <td>171884.0</td>\n",
       "      <td>Thermally activated smoothing treatment combin...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139347</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>love this product for wavy hair</td>\n",
       "      <td>0</td>\n",
       "      <td>27.00</td>\n",
       "      <td>15099.0</td>\n",
       "      <td>A lightweight styling primer. TWISTER adds moi...</td>\n",
       "      <td>32.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>139348 rows Ã— 62 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        overall  vote  verified  \\\n",
       "0           5.0   0.0         1   \n",
       "1           5.0   0.0         1   \n",
       "2           1.0   0.0         0   \n",
       "3           5.0   0.0         1   \n",
       "4           5.0   0.0         1   \n",
       "...         ...   ...       ...   \n",
       "139343      2.0   0.0         1   \n",
       "139344      1.0   0.0         1   \n",
       "139345      4.0   0.0         1   \n",
       "139346      1.0   0.0         1   \n",
       "139347      5.0   3.0         1   \n",
       "\n",
       "                                               reviewText  trending_asin  \\\n",
       "0                                 Bought for my daughter.              0   \n",
       "1                               Great cream for the skin.              0   \n",
       "2       Maybe Im just not familiar with this brand, bu...              0   \n",
       "3       This is one of my favorite creams. Highly reco...              0   \n",
       "4       I simply love this lotion!  Originally, I rece...              0   \n",
       "...                                                   ...            ...   \n",
       "139343                       Meh!  Flat iron works better              0   \n",
       "139344  It doesn't grip the hair - I'm very disappointed.              0   \n",
       "139345  Really works! I have curly hair and you can ge...              0   \n",
       "139346                              DIDNT STRAIGHTEN HAIR              0   \n",
       "139347                    love this product for wavy hair              0   \n",
       "\n",
       "        price_USD   ranking  \\\n",
       "0           30.00    4324.0   \n",
       "1           30.00    4324.0   \n",
       "2           30.00    4324.0   \n",
       "3           30.00    4324.0   \n",
       "4           30.00    4324.0   \n",
       "...           ...       ...   \n",
       "139343      21.29  171884.0   \n",
       "139344      21.29  171884.0   \n",
       "139345      21.29  171884.0   \n",
       "139346      21.29  171884.0   \n",
       "139347      27.00   15099.0   \n",
       "\n",
       "                                      product_description  also_buy_counts  \\\n",
       "0       After a long day of handling thorny situations...             56.0   \n",
       "1       After a long day of handling thorny situations...             56.0   \n",
       "2       After a long day of handling thorny situations...             56.0   \n",
       "3       After a long day of handling thorny situations...             56.0   \n",
       "4       After a long day of handling thorny situations...             56.0   \n",
       "...                                                   ...              ...   \n",
       "139343  Thermally activated smoothing treatment combin...              0.0   \n",
       "139344  Thermally activated smoothing treatment combin...              0.0   \n",
       "139345  Thermally activated smoothing treatment combin...              0.0   \n",
       "139346  Thermally activated smoothing treatment combin...              0.0   \n",
       "139347  A lightweight styling primer. TWISTER adds moi...             32.0   \n",
       "\n",
       "        also_view_counts  ...  m_Pureology  m_RUSK  m_Red Flower  \\\n",
       "0                   48.0  ...          0.0     0.0           0.0   \n",
       "1                   48.0  ...          0.0     0.0           0.0   \n",
       "2                   48.0  ...          0.0     0.0           0.0   \n",
       "3                   48.0  ...          0.0     0.0           0.0   \n",
       "4                   48.0  ...          0.0     0.0           0.0   \n",
       "...                  ...  ...          ...     ...           ...   \n",
       "139343               0.0  ...          0.0     0.0           0.0   \n",
       "139344               0.0  ...          0.0     0.0           0.0   \n",
       "139345               0.0  ...          0.0     0.0           0.0   \n",
       "139346               0.0  ...          0.0     0.0           0.0   \n",
       "139347              35.0  ...          0.0     0.0           0.0   \n",
       "\n",
       "        m_Rene Furterer  m_Stila  m_StriVectin  m_TS  m_The Art of Shaving  \\\n",
       "0                   0.0      0.0           0.0   0.0                   0.0   \n",
       "1                   0.0      0.0           0.0   0.0                   0.0   \n",
       "2                   0.0      0.0           0.0   0.0                   0.0   \n",
       "3                   0.0      0.0           0.0   0.0                   0.0   \n",
       "4                   0.0      0.0           0.0   0.0                   0.0   \n",
       "...                 ...      ...           ...   ...                   ...   \n",
       "139343              0.0      0.0           0.0   1.0                   0.0   \n",
       "139344              0.0      0.0           0.0   1.0                   0.0   \n",
       "139345              0.0      0.0           0.0   1.0                   0.0   \n",
       "139346              0.0      0.0           0.0   1.0                   0.0   \n",
       "139347              0.0      0.0           0.0   0.0                   0.0   \n",
       "\n",
       "        m_Vichy  m_theBalm  \n",
       "0           0.0        0.0  \n",
       "1           0.0        0.0  \n",
       "2           0.0        0.0  \n",
       "3           0.0        0.0  \n",
       "4           0.0        0.0  \n",
       "...         ...        ...  \n",
       "139343      0.0        0.0  \n",
       "139344      0.0        0.0  \n",
       "139345      0.0        0.0  \n",
       "139346      0.0        0.0  \n",
       "139347      0.0        0.0  \n",
       "\n",
       "[139348 rows x 62 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_combined = df_combined.dropna()\n",
    "df_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f4c197bd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f4c197bd",
    "outputId": "05e093b0-b68f-4d71-8606-4ad70b65c02b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 139348 entries, 0 to 139347\n",
      "Data columns (total 62 columns):\n",
      " #   Column                Non-Null Count   Dtype  \n",
      "---  ------                --------------   -----  \n",
      " 0   overall               139348 non-null  float64\n",
      " 1   vote                  139348 non-null  float64\n",
      " 2   verified              139348 non-null  int64  \n",
      " 3   reviewText            139348 non-null  object \n",
      " 4   trending_asin         139348 non-null  int64  \n",
      " 5   price_USD             139348 non-null  float64\n",
      " 6   ranking               139348 non-null  float64\n",
      " 7   product_description   139348 non-null  object \n",
      " 8   also_buy_counts       139348 non-null  float64\n",
      " 9   also_view_counts      139348 non-null  float64\n",
      " 10  c_accessories         139348 non-null  float64\n",
      " 11  c_bath and shower     139348 non-null  float64\n",
      " 12  c_fragance            139348 non-null  float64\n",
      " 13  c_haircare            139348 non-null  float64\n",
      " 14  c_makeup              139348 non-null  float64\n",
      " 15  c_menscare            139348 non-null  float64\n",
      " 16  c_nails and hands     139348 non-null  float64\n",
      " 17  c_other               139348 non-null  float64\n",
      " 18  c_skincare            139348 non-null  float64\n",
      " 19  c_suncare             139348 non-null  float64\n",
      " 20  m_AHAVA               139348 non-null  float64\n",
      " 21  m_Archipelago         139348 non-null  float64\n",
      " 22  m_BaBylissPro         139348 non-null  float64\n",
      " 23  m_Bioelements         139348 non-null  float64\n",
      " 24  m_Bliss               139348 non-null  float64\n",
      " 25  m_Burberry            139348 non-null  float64\n",
      " 26  m_Butter London       139348 non-null  float64\n",
      " 27  m_CHI                 139348 non-null  float64\n",
      " 28  m_CND                 139348 non-null  float64\n",
      " 29  m_Calvin Klein        139348 non-null  float64\n",
      " 30  m_Crabtree & Evelyn   139348 non-null  float64\n",
      " 31  m_Davines             139348 non-null  float64\n",
      " 32  m_Elemis              139348 non-null  float64\n",
      " 33  m_Elizabeth Arden     139348 non-null  float64\n",
      " 34  m_Glo Skin            139348 non-null  float64\n",
      " 35  m_Jane Iredale        139348 non-null  float64\n",
      " 36  m_Japonesque          139348 non-null  float64\n",
      " 37  m_Julep               139348 non-null  float64\n",
      " 38  m_L'occitane          139348 non-null  float64\n",
      " 39  m_LAFCO               139348 non-null  float64\n",
      " 40  m_LORAC               139348 non-null  float64\n",
      " 41  m_La Roche Posay      139348 non-null  float64\n",
      " 42  m_Laura Geller        139348 non-null  float64\n",
      " 43  m_Mario Badescu       139348 non-null  float64\n",
      " 44  m_Mustela             139348 non-null  float64\n",
      " 45  m_NUXE                139348 non-null  float64\n",
      " 46  m_Natura Bisse        139348 non-null  float64\n",
      " 47  m_OPI                 139348 non-null  float64\n",
      " 48  m_Oribe               139348 non-null  float64\n",
      " 49  m_Other               139348 non-null  float64\n",
      " 50  m_PCA Skin            139348 non-null  float64\n",
      " 51  m_Paul Mitchell       139348 non-null  float64\n",
      " 52  m_Pureology           139348 non-null  float64\n",
      " 53  m_RUSK                139348 non-null  float64\n",
      " 54  m_Red Flower          139348 non-null  float64\n",
      " 55  m_Rene Furterer       139348 non-null  float64\n",
      " 56  m_Stila               139348 non-null  float64\n",
      " 57  m_StriVectin          139348 non-null  float64\n",
      " 58  m_TS                  139348 non-null  float64\n",
      " 59  m_The Art of Shaving  139348 non-null  float64\n",
      " 60  m_Vichy               139348 non-null  float64\n",
      " 61  m_theBalm             139348 non-null  float64\n",
      "dtypes: float64(58), int64(2), object(2)\n",
      "memory usage: 65.9+ MB\n"
     ]
    }
   ],
   "source": [
    "df_combined.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc6ea24",
   "metadata": {
    "id": "afc6ea24"
   },
   "source": [
    "Looks like the dataset is completely clean so let's move onto the modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcce6b6e",
   "metadata": {
    "id": "bcce6b6e"
   },
   "source": [
    "## 2. Modeling <a id=\"Modeling\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "99483aa2",
   "metadata": {
    "id": "99483aa2"
   },
   "outputs": [],
   "source": [
    "#loading the modules \n",
    "\n",
    "# Scalars\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler \n",
    "\n",
    "# To make our sets\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, RandomizedSearchCV\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "# The classifiers \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aabc9b6",
   "metadata": {
    "id": "7aabc9b6"
   },
   "source": [
    "Creating 2 models one with and one without the product description column, let's call them df_with_prod and df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "25765d9d",
   "metadata": {
    "id": "25765d9d"
   },
   "outputs": [],
   "source": [
    "df_with_desc = df_combined.copy()\n",
    "df = df_combined.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dadc231e",
   "metadata": {
    "id": "dadc231e"
   },
   "outputs": [],
   "source": [
    "#dropping remaining text columns\n",
    "columns = ['product_description']\n",
    "for column in columns: \n",
    "     df= df_combined.drop([column],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "83d17b1b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 362
    },
    "id": "83d17b1b",
    "outputId": "5a3eb180-ca00-4a59-98c2-32898cc1e99a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>overall</th>\n",
       "      <th>vote</th>\n",
       "      <th>verified</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>trending_asin</th>\n",
       "      <th>price_USD</th>\n",
       "      <th>ranking</th>\n",
       "      <th>also_buy_counts</th>\n",
       "      <th>also_view_counts</th>\n",
       "      <th>c_accessories</th>\n",
       "      <th>...</th>\n",
       "      <th>m_Pureology</th>\n",
       "      <th>m_RUSK</th>\n",
       "      <th>m_Red Flower</th>\n",
       "      <th>m_Rene Furterer</th>\n",
       "      <th>m_Stila</th>\n",
       "      <th>m_StriVectin</th>\n",
       "      <th>m_TS</th>\n",
       "      <th>m_The Art of Shaving</th>\n",
       "      <th>m_Vichy</th>\n",
       "      <th>m_theBalm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>91440</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>I have horrible hormonal cystic acne on my fac...</td>\n",
       "      <td>0</td>\n",
       "      <td>31.99</td>\n",
       "      <td>3055.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96171</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>I love this stuff.  I was using the MDSolarSci...</td>\n",
       "      <td>0</td>\n",
       "      <td>32.00</td>\n",
       "      <td>5880.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       overall  vote  verified  \\\n",
       "91440      5.0   0.0         1   \n",
       "96171      5.0   0.0         1   \n",
       "\n",
       "                                              reviewText  trending_asin  \\\n",
       "91440  I have horrible hormonal cystic acne on my fac...              0   \n",
       "96171  I love this stuff.  I was using the MDSolarSci...              0   \n",
       "\n",
       "       price_USD  ranking  also_buy_counts  also_view_counts  c_accessories  \\\n",
       "91440      31.99   3055.0             72.0              79.0            0.0   \n",
       "96171      32.00   5880.0             17.0              50.0            0.0   \n",
       "\n",
       "       ...  m_Pureology  m_RUSK  m_Red Flower  m_Rene Furterer  m_Stila  \\\n",
       "91440  ...          0.0     0.0           0.0              0.0      0.0   \n",
       "96171  ...          0.0     0.0           0.0              0.0      0.0   \n",
       "\n",
       "       m_StriVectin  m_TS  m_The Art of Shaving  m_Vichy  m_theBalm  \n",
       "91440           0.0   0.0                   0.0      0.0        0.0  \n",
       "96171           0.0   0.0                   0.0      0.0        0.0  \n",
       "\n",
       "[2 rows x 61 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45fac276",
   "metadata": {
    "id": "45fac276"
   },
   "source": [
    "Working with just the review df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a82abcc7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a82abcc7",
    "outputId": "8d6142e0-4d03-4e4a-aad0-a986b890c3ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remainder shape (97543, 61)\n",
      "Test shape (41805, 61)\n"
     ]
    }
   ],
   "source": [
    "#for Train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "y = df_combined[\"trending_asin\"]\n",
    "remainder1, test1  = train_test_split(df, test_size=0.3, stratify= y, random_state = 8)\n",
    "print(f\"Remainder shape {remainder1.shape}\")\n",
    "print(f\"Test shape {test1.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "95469997",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remainder sample shape (19508, 61)\n",
      "Test shape (78035, 61)\n"
     ]
    }
   ],
   "source": [
    "#for Train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "y_sample = remainder1[\"trending_asin\"]\n",
    "remainder1_sample, none1 = train_test_split(remainder1, test_size=0.8, stratify= y_sample, random_state = 8)\n",
    "print(f\"Remainder sample shape {remainder1_sample.shape}\")\n",
    "print(f\"Test shape {none1.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "83e7ad8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_remainder1_sample = remainder1_sample[\"trending_asin\"]\n",
    "X_remainder1_sample = remainder1_sample.drop([\"trending_asin\"],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400c885e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "99c2adbb",
   "metadata": {
    "id": "99c2adbb"
   },
   "outputs": [],
   "source": [
    "#assigning remainder dataset for X and y \n",
    "y_remainder1 = remainder1[\"trending_asin\"]\n",
    "X_remainder1  = remainder1.drop([\"trending_asin\"],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "50f84ae4",
   "metadata": {
    "id": "50f84ae4"
   },
   "outputs": [],
   "source": [
    "#assigning train dataset for X and y \n",
    "y_test1 = test1[\"trending_asin\"]\n",
    "X_test1  = test1.drop([\"trending_asin\"],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "46ebd777",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "46ebd777",
    "outputId": "25bc64ac-76ca-4d39-a654-3d014ab15af3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape (68280, 60)\n",
      "Validation shape (29263, 60)\n"
     ]
    }
   ],
   "source": [
    "#for train/validation split \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train1, X_val1, y_train1, y_val1 = train_test_split(X_remainder1, y_remainder1, test_size=0.3, random_state=1, stratify=y_remainder1)\n",
    "print(f\"Train shape {X_train1.shape}\")\n",
    "print(f\"Validation shape {X_val1.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "308a6691",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "308a6691",
    "outputId": "9eb11a0b-4c9a-4879-ad15-a1cc011a079a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportions in original data:\n",
      "0    0.888574\n",
      "1    0.111426\n",
      "Name: trending_asin, dtype: float64 \n",
      "\n",
      "Proportions in remainder set:\n",
      "0    0.888572\n",
      "1    0.111428\n",
      "Name: trending_asin, dtype: float64 \n",
      "\n",
      "Proportions in test set:\n",
      "0    0.888578\n",
      "1    0.111422\n",
      "Name: trending_asin, dtype: float64 \n",
      "\n",
      "Proportions in train set:\n",
      "0    0.888576\n",
      "1    0.111424\n",
      "Name: trending_asin, dtype: float64 \n",
      "\n",
      "Proportions in validation set:\n",
      "0    0.888562\n",
      "1    0.111438\n",
      "Name: trending_asin, dtype: float64 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Proportions in original data:')\n",
    "print(y.value_counts(normalize=True), '\\n')\n",
    "\n",
    "print('Proportions in remainder set:')\n",
    "print(y_remainder1.value_counts(normalize=True), '\\n')\n",
    "\n",
    "print('Proportions in test set:')\n",
    "print(y_test1.value_counts(normalize=True), '\\n')\n",
    "\n",
    "print('Proportions in train set:')\n",
    "print(y_train1.value_counts(normalize=True), '\\n')\n",
    "\n",
    "print('Proportions in validation set:')\n",
    "print(y_val1.value_counts(normalize=True), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f716d25",
   "metadata": {
    "id": "7f716d25"
   },
   "source": [
    "### Splitting out model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6152688e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6152688e",
    "outputId": "429b4ac8-e0cf-4c89-f720-7d9316b577ea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remainder shape (97543, 62)\n",
      "Test shape (41805, 62)\n"
     ]
    }
   ],
   "source": [
    "#for Train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "y = df_combined[\"trending_asin\"]\n",
    "remainder2, test2  = train_test_split(df_with_desc, test_size=0.3, stratify= y, random_state = 8)\n",
    "print(f\"Remainder shape {remainder2.shape}\")\n",
    "print(f\"Test shape {test2.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4c5926af",
   "metadata": {
    "id": "4c5926af"
   },
   "outputs": [],
   "source": [
    "#assigning remainder dataset for X and y \n",
    "y_remainder2 = remainder2[\"trending_asin\"]\n",
    "X_remainder2  = remainder2.drop([\"trending_asin\"],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "06b91b0c",
   "metadata": {
    "id": "06b91b0c"
   },
   "outputs": [],
   "source": [
    "#assigning train dataset for X and y \n",
    "y_test2 = test2[\"trending_asin\"]\n",
    "X_test2  = test2.drop([\"trending_asin\"],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "45a6f8c2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "45a6f8c2",
    "outputId": "13429afc-96ee-4c30-91fc-8261ed88f716"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape (68280, 61)\n",
      "Validation shape (29263, 61)\n"
     ]
    }
   ],
   "source": [
    "#for train/validation split \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train2, X_val2, y_train2, y_val2 = train_test_split(X_remainder2, y_remainder2, test_size=0.3, random_state=2, stratify=y_remainder2)\n",
    "print(f\"Train shape {X_train2.shape}\")\n",
    "print(f\"Validation shape {X_val2.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ad63fa",
   "metadata": {
    "id": "31ad63fa"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5f724e8c",
   "metadata": {
    "id": "5f724e8c"
   },
   "source": [
    "## Review and Description Analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c55c27b",
   "metadata": {
    "id": "8c55c27b"
   },
   "source": [
    "How might we determine what is the best method to apply to our columns of review summaries and description? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c8a1c8",
   "metadata": {
    "id": "04c8a1c8"
   },
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d0b2bf29",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d0b2bf29",
    "outputId": "18b967da-cee6-4d56-dc4a-8c5d534af97f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<97543x65 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 423478 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# 1. Instantiate \n",
    "bagofwords = CountVectorizer(stop_words=\"english\",#removing stop words\n",
    "                             min_df=0.03,\n",
    "                             max_features= 2000,       \n",
    "                             )    \n",
    "\n",
    "# 2. Fit \n",
    "bagofwords.fit(X_remainder1['reviewText'])\n",
    "\n",
    "# 3. Transform\n",
    "remainder_transformed = bagofwords.transform(remainder1['reviewText'])\n",
    "remainder_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6a7b55d5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6a7b55d5",
    "outputId": "d366e12b-9ad2-4427-9e84-4f30d73cf2f9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(97543, 65)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#making into an array\n",
    "remainder_transformed.toarray().sum(axis=0)\n",
    "remainder_transformed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a3b528b1",
   "metadata": {
    "id": "a3b528b1"
   },
   "outputs": [],
   "source": [
    "review_word_counts = pd.DataFrame(\n",
    "    {\"words\": bagofwords.get_feature_names_out(),\n",
    "    \"counts\": remainder_transformed.toarray().sum(axis=0)\n",
    "     }\n",
    ").sort_values(\"counts\", ascending=False).reset_index().drop(columns = \"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b3bc4874",
   "metadata": {
    "id": "b3bc4874"
   },
   "outputs": [],
   "source": [
    "review_word_counts[\"words\"] = 'r_' + review_word_counts['words'].astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a359a264",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 488
    },
    "id": "a359a264",
    "outputId": "3016df41-551e-4244-9b00-e6a10ffbf86b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>words</th>\n",
       "      <th>r_product</th>\n",
       "      <th>r_hair</th>\n",
       "      <th>r_love</th>\n",
       "      <th>r_great</th>\n",
       "      <th>r_skin</th>\n",
       "      <th>r_use</th>\n",
       "      <th>r_like</th>\n",
       "      <th>r_good</th>\n",
       "      <th>r_just</th>\n",
       "      <th>r_color</th>\n",
       "      <th>...</th>\n",
       "      <th>r_apply</th>\n",
       "      <th>r_definitely</th>\n",
       "      <th>r_try</th>\n",
       "      <th>r_worth</th>\n",
       "      <th>r_looks</th>\n",
       "      <th>r_goes</th>\n",
       "      <th>r_smooth</th>\n",
       "      <th>r_think</th>\n",
       "      <th>r_soft</th>\n",
       "      <th>r_feels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97538</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97539</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97540</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97541</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97542</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>97543 rows Ã— 65 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "words  r_product  r_hair  r_love  r_great  r_skin  r_use  r_like  r_good  \\\n",
       "0              0       0       1        1       0      0       0       0   \n",
       "1              0       0       0        0       0      0       0       0   \n",
       "2              1       0       1        0       0      0       0       0   \n",
       "3              0       0       0        0       0      1       0       0   \n",
       "4              0       0       0        0       0      0       0       0   \n",
       "...          ...     ...     ...      ...     ...    ...     ...     ...   \n",
       "97538          0       0       0        0       0      0       0       0   \n",
       "97539          0       0       0        0       0      0       0       0   \n",
       "97540          0       0       0        1       1      1       0       0   \n",
       "97541          0       0       0        0       0      0       0       0   \n",
       "97542          0       0       0        0       0      0       0       1   \n",
       "\n",
       "words  r_just  r_color  ...  r_apply  r_definitely  r_try  r_worth  r_looks  \\\n",
       "0           0        0  ...        0             0      0        0        0   \n",
       "1           0        0  ...        0             0      2        0        0   \n",
       "2           0        0  ...        0             0      0        0        0   \n",
       "3           0        0  ...        0             0      0        0        0   \n",
       "4           0        0  ...        0             0      0        0        0   \n",
       "...       ...      ...  ...      ...           ...    ...      ...      ...   \n",
       "97538       0        0  ...        0             0      0        0        0   \n",
       "97539       0        0  ...        0             0      0        0        0   \n",
       "97540       0        0  ...        1             1      2        0        1   \n",
       "97541       0        0  ...        0             0      0        0        0   \n",
       "97542       0        1  ...        0             0      0        0        0   \n",
       "\n",
       "words  r_goes  r_smooth  r_think  r_soft  r_feels  \n",
       "0           0         1        0       0        0  \n",
       "1           0         0        0       0        0  \n",
       "2           0         0        0       0        0  \n",
       "3           1         0        0       0        0  \n",
       "4           0         0        0       0        0  \n",
       "...       ...       ...      ...     ...      ...  \n",
       "97538       0         0        0       0        0  \n",
       "97539       0         0        0       0        0  \n",
       "97540       0         1        0       0        0  \n",
       "97541       0         0        1       0        0  \n",
       "97542       0         0        0       0        0  \n",
       "\n",
       "[97543 rows x 65 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_vect_df = pd.DataFrame(remainder_transformed.todense(), columns= review_word_counts[\"words\"])\n",
    "review_vect_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8f23caa0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 240
    },
    "id": "8f23caa0",
    "outputId": "7c03f0e2-08af-4737-b711-19accbfcfc22"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>overall</th>\n",
       "      <th>vote</th>\n",
       "      <th>verified</th>\n",
       "      <th>price_USD</th>\n",
       "      <th>ranking</th>\n",
       "      <th>also_buy_counts</th>\n",
       "      <th>also_view_counts</th>\n",
       "      <th>c_accessories</th>\n",
       "      <th>c_bath and shower</th>\n",
       "      <th>c_fragance</th>\n",
       "      <th>...</th>\n",
       "      <th>r_apply</th>\n",
       "      <th>r_definitely</th>\n",
       "      <th>r_try</th>\n",
       "      <th>r_worth</th>\n",
       "      <th>r_looks</th>\n",
       "      <th>r_goes</th>\n",
       "      <th>r_smooth</th>\n",
       "      <th>r_think</th>\n",
       "      <th>r_soft</th>\n",
       "      <th>r_feels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>68639</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>14.0</td>\n",
       "      <td>52221.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50227</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>12.0</td>\n",
       "      <td>9112.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 124 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       overall  vote  verified  price_USD  ranking  also_buy_counts  \\\n",
       "68639      5.0   0.0         1       14.0  52221.0              0.0   \n",
       "50227      4.0   0.0         1       12.0   9112.0             13.0   \n",
       "\n",
       "       also_view_counts  c_accessories  c_bath and shower  c_fragance  ...  \\\n",
       "68639               0.0            0.0                0.0         0.0  ...   \n",
       "50227              14.0            0.0                0.0         0.0  ...   \n",
       "\n",
       "       r_apply  r_definitely  r_try  r_worth  r_looks  r_goes  r_smooth  \\\n",
       "68639        0             0      0        0        0       0         0   \n",
       "50227        0             0      0        0        0       0         0   \n",
       "\n",
       "       r_think  r_soft  r_feels  \n",
       "68639        0       0        0  \n",
       "50227        0       0        0  \n",
       "\n",
       "[2 rows x 124 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_remainder1_pca = pd.concat([X_remainder1.reset_index(drop=True), review_vect_df.reset_index(drop=True)], axis=1).drop(columns = 'reviewText')\n",
    "X_remainder1_pca.sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "43618313",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "43618313",
    "outputId": "542730e4-52ea-4bf6-935e-517fc4690f61"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of scaled and features with applied PCA is (97543, 4)\n"
     ]
    }
   ],
   "source": [
    "#scaling data \n",
    "scaler_pca = StandardScaler()\n",
    "scaler_pca.fit(X_remainder1_pca)\n",
    "feature_scaled = scaler_pca.transform(X_remainder1_pca)\n",
    "\n",
    "#applying PCA \n",
    "pca1 = PCA(n_components=4)\n",
    "pca1.fit(feature_scaled)\n",
    "feature_scaled_pca = pca1.transform(feature_scaled)\n",
    "print(f\"shape of scaled and features with applied PCA is {np.shape(feature_scaled_pca)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e356c3bd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e356c3bd",
    "outputId": "9a5deca1-359b-4e58-b784-06c2531fbb43"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variance Ratio of the 4 principal components analysis: [0.47247088 0.20034694 0.18259631 0.14458586]\n",
      "0.8554141392073837\n"
     ]
    }
   ],
   "source": [
    "#variance to see which out of the 4 components are contributing most\n",
    "feat_var = np.var(feature_scaled_pca, axis=0)\n",
    "feat_var_rat = feat_var/(np.sum(feat_var))\n",
    "\n",
    "print(f\"variance Ratio of the 4 principal components analysis: {feat_var_rat}\")\n",
    "\n",
    "print(sum(feat_var_rat[0:3]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5492f443",
   "metadata": {},
   "source": [
    "### How to process text data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ba68edfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>after using</th>\n",
       "      <th>all day</th>\n",
       "      <th>all the</th>\n",
       "      <th>and does</th>\n",
       "      <th>and doesn</th>\n",
       "      <th>and has</th>\n",
       "      <th>and have</th>\n",
       "      <th>and love</th>\n",
       "      <th>and not</th>\n",
       "      <th>and the</th>\n",
       "      <th>...</th>\n",
       "      <th>works great</th>\n",
       "      <th>works well</th>\n",
       "      <th>worth the</th>\n",
       "      <th>years and</th>\n",
       "      <th>you are</th>\n",
       "      <th>you can</th>\n",
       "      <th>you have</th>\n",
       "      <th>your face</th>\n",
       "      <th>your hair</th>\n",
       "      <th>your skin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97538</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97539</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97540</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97541</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97542</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>97543 rows Ã— 106 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       after using  all day  all the  and does  and doesn  and has  and have  \\\n",
       "0                0        0        0         0          0        0         0   \n",
       "1                0        0        0         0          0        0         0   \n",
       "2                0        0        0         0          0        0         1   \n",
       "3                0        0        0         0          0        0         0   \n",
       "4                0        0        0         0          0        0         0   \n",
       "...            ...      ...      ...       ...        ...      ...       ...   \n",
       "97538            0        0        0         0          0        0         0   \n",
       "97539            0        0        0         0          0        0         0   \n",
       "97540            0        0        0         0          0        0         0   \n",
       "97541            0        0        0         0          0        0         0   \n",
       "97542            0        0        0         0          0        0         0   \n",
       "\n",
       "       and love  and not  and the  ...  works great  works well  worth the  \\\n",
       "0             0        0        0  ...            0           0          0   \n",
       "1             0        0        0  ...            0           0          0   \n",
       "2             0        0        0  ...            0           0          0   \n",
       "3             0        0        0  ...            0           0          0   \n",
       "4             0        0        0  ...            0           0          0   \n",
       "...         ...      ...      ...  ...          ...         ...        ...   \n",
       "97538         0        0        0  ...            0           0          0   \n",
       "97539         0        0        0  ...            0           0          0   \n",
       "97540         0        0        0  ...            0           0          0   \n",
       "97541         0        0        0  ...            0           0          0   \n",
       "97542         0        0        0  ...            0           0          0   \n",
       "\n",
       "       years and  you are  you can  you have  your face  your hair  your skin  \n",
       "0              0        0        0         0          0          0          0  \n",
       "1              0        0        0         0          0          0          0  \n",
       "2              0        0        0         0          0          0          0  \n",
       "3              0        0        0         0          0          0          0  \n",
       "4              0        0        0         0          0          0          0  \n",
       "...          ...      ...      ...       ...        ...        ...        ...  \n",
       "97538          0        0        0         0          0          0          0  \n",
       "97539          0        0        0         0          0          0          0  \n",
       "97540          0        0        0         0          0          2          0  \n",
       "97541          0        0        0         0          0          0          0  \n",
       "97542          0        0        0         0          0          0          0  \n",
       "\n",
       "[97543 rows x 106 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import (CountVectorizer, \n",
    "                                             TfidfVectorizer,\n",
    "                                             TfidfTransformer)\n",
    "\n",
    "vectorizer = CountVectorizer(analyzer='word', \n",
    "                              token_pattern=r'\\b[a-zA-Z]{3,}\\b',  \n",
    "                              ngram_range=(2, 2),\n",
    "                              min_df = 0.01)  # only bigrams\n",
    "vectorized = vectorizer.fit_transform(X_remainder1['reviewText'])\n",
    "pd.DataFrame(vectorized.toarray(), \n",
    "             columns=vectorizer.get_feature_names_out())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "647eb267",
   "metadata": {},
   "source": [
    "### TF-IDF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "beaf61f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>after using</th>\n",
       "      <th>all day</th>\n",
       "      <th>all the</th>\n",
       "      <th>and does</th>\n",
       "      <th>and doesn</th>\n",
       "      <th>and has</th>\n",
       "      <th>and have</th>\n",
       "      <th>and love</th>\n",
       "      <th>and not</th>\n",
       "      <th>and the</th>\n",
       "      <th>...</th>\n",
       "      <th>works great</th>\n",
       "      <th>works well</th>\n",
       "      <th>worth the</th>\n",
       "      <th>years and</th>\n",
       "      <th>you are</th>\n",
       "      <th>you can</th>\n",
       "      <th>you have</th>\n",
       "      <th>your face</th>\n",
       "      <th>your hair</th>\n",
       "      <th>your skin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.540145</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97538</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97539</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97540</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.602043</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97541</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97542</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>97543 rows Ã— 106 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       after using  all day  all the  and does  and doesn  and has  and have  \\\n",
       "0              0.0      0.0      0.0       0.0        0.0      0.0  0.000000   \n",
       "1              0.0      0.0      0.0       0.0        0.0      0.0  0.000000   \n",
       "2              0.0      0.0      0.0       0.0        0.0      0.0  0.540145   \n",
       "3              0.0      0.0      0.0       0.0        0.0      0.0  0.000000   \n",
       "4              0.0      0.0      0.0       0.0        0.0      0.0  0.000000   \n",
       "...            ...      ...      ...       ...        ...      ...       ...   \n",
       "97538          0.0      0.0      0.0       0.0        0.0      0.0  0.000000   \n",
       "97539          0.0      0.0      0.0       0.0        0.0      0.0  0.000000   \n",
       "97540          0.0      0.0      0.0       0.0        0.0      0.0  0.000000   \n",
       "97541          0.0      0.0      0.0       0.0        0.0      0.0  0.000000   \n",
       "97542          0.0      0.0      0.0       0.0        0.0      0.0  0.000000   \n",
       "\n",
       "       and love  and not  and the  ...  works great  works well  worth the  \\\n",
       "0           0.0      0.0      0.0  ...          0.0         0.0        0.0   \n",
       "1           0.0      0.0      0.0  ...          0.0         0.0        0.0   \n",
       "2           0.0      0.0      0.0  ...          0.0         0.0        0.0   \n",
       "3           0.0      0.0      0.0  ...          0.0         0.0        0.0   \n",
       "4           0.0      0.0      0.0  ...          0.0         0.0        0.0   \n",
       "...         ...      ...      ...  ...          ...         ...        ...   \n",
       "97538       0.0      0.0      0.0  ...          0.0         0.0        0.0   \n",
       "97539       0.0      0.0      0.0  ...          0.0         0.0        0.0   \n",
       "97540       0.0      0.0      0.0  ...          0.0         0.0        0.0   \n",
       "97541       0.0      0.0      0.0  ...          0.0         0.0        0.0   \n",
       "97542       0.0      0.0      0.0  ...          0.0         0.0        0.0   \n",
       "\n",
       "       years and  you are  you can  you have  your face  your hair  your skin  \n",
       "0            0.0      0.0      0.0       0.0        0.0   0.000000        0.0  \n",
       "1            0.0      0.0      0.0       0.0        0.0   0.000000        0.0  \n",
       "2            0.0      0.0      0.0       0.0        0.0   0.000000        0.0  \n",
       "3            0.0      0.0      0.0       0.0        0.0   0.000000        0.0  \n",
       "4            0.0      0.0      0.0       0.0        0.0   0.000000        0.0  \n",
       "...          ...      ...      ...       ...        ...        ...        ...  \n",
       "97538        0.0      0.0      0.0       0.0        0.0   0.000000        0.0  \n",
       "97539        0.0      0.0      0.0       0.0        0.0   0.000000        0.0  \n",
       "97540        0.0      0.0      0.0       0.0        0.0   0.602043        0.0  \n",
       "97541        0.0      0.0      0.0       0.0        0.0   0.000000        0.0  \n",
       "97542        0.0      0.0      0.0       0.0        0.0   0.000000        0.0  \n",
       "\n",
       "[97543 rows x 106 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import (CountVectorizer, \n",
    "                                             TfidfVectorizer,\n",
    "                                             TfidfTransformer)\n",
    "\n",
    "vectorizer = TfidfVectorizer(analyzer='word', \n",
    "                              token_pattern=r'\\b[a-zA-Z]{3,}\\b',  \n",
    "                              ngram_range=(2, 2),\n",
    "                              min_df = 0.01)  # only bigrams\n",
    "vectorized = vectorizer.fit_transform(X_remainder1['reviewText'])\n",
    "pd.DataFrame(vectorized.toarray(), \n",
    "             columns=vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d7cbbb40",
   "metadata": {},
   "outputs": [],
   "source": [
    "review_word_counts_TFIDF = pd.DataFrame(\n",
    "    {\"words\": vectorizer.get_feature_names_out(),\n",
    "    \"counts\": vectorized.toarray().sum(axis=0)\n",
    "     }\n",
    ").sort_values(\"counts\", ascending=False).reset_index().drop(columns = \"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b9148d1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>this product</td>\n",
       "      <td>5088.781478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>love this</td>\n",
       "      <td>4707.565899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>great product</td>\n",
       "      <td>2532.613917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the best</td>\n",
       "      <td>2369.733561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>and the</td>\n",
       "      <td>2178.265576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>the other</td>\n",
       "      <td>506.302426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>when you</td>\n",
       "      <td>494.207032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>because the</td>\n",
       "      <td>493.501642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>you are</td>\n",
       "      <td>493.434429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>first time</td>\n",
       "      <td>480.794397</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>106 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             words       counts\n",
       "0     this product  5088.781478\n",
       "1        love this  4707.565899\n",
       "2    great product  2532.613917\n",
       "3         the best  2369.733561\n",
       "4          and the  2178.265576\n",
       "..             ...          ...\n",
       "101      the other   506.302426\n",
       "102       when you   494.207032\n",
       "103    because the   493.501642\n",
       "104        you are   493.434429\n",
       "105     first time   480.794397\n",
       "\n",
       "[106 rows x 2 columns]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_word_counts_TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "2e71b32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#count vectorizer function\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "cv = ColumnTransformer([\n",
    "        ('TFIDF', TfidfVectorizer(min_df=6, max_features=None, strip_accents='unicode', \n",
    "                                  analyzer=\"word\", token_pattern=r'\\w{1,}', ngram_range=(1, 2)), 'reviewText')\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e4811e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from tempfile import mkdtemp\n",
    "cachedir = mkdtemp()\n",
    "\n",
    "#we give our estimators as a list of tuples: name:function.\n",
    "pipe_vec = [('vectorize', cv)]\n",
    "pipe_vec_test = Pipeline(pipe_vec,verbose=4, memory=cachedir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "a16cc0dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ......... (step 1 of 1) Processing vectorize, total=  13.5s\n"
     ]
    }
   ],
   "source": [
    "Xtr = pipe_vec_test.fit_transform(X_remainder1)\n",
    "vec = pipe_vec_test.named_steps['vectorize']\n",
    "features = vec.get_feature_names_out()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "cfb3386a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_tfidf_feats(row, features, top_n=25):\n",
    "    ''' Get top n tfidf values in row and return them with their corresponding feature names.'''\n",
    "    topn_ids = np.argsort(row)[::-1][:top_n]\n",
    "    top_feats = [(features[i], row[i]) for i in topn_ids]\n",
    "    df = pd.DataFrame(top_feats)\n",
    "    df.columns = ['feature', 'tfidf']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a7a099d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_feats_in_doc(Xtr, features, row_id, top_n=25):\n",
    "#Top tfidf features in specific document (matrix row) \n",
    "    row = np.squeeze(Xtr[row_id].toarray())\n",
    "    return top_tfidf_feats(row, features, top_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "309473c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>tfidf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>remainder__ranking</td>\n",
       "      <td>752.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>remainder__price_USD</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>remainder__overall</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>remainder__m_Other</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>remainder__c_skincare</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>remainder__verified</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>TFIDF__cost</td>\n",
       "      <td>0.129623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>TFIDF__that</td>\n",
       "      <td>0.120972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>TFIDF__old razor</td>\n",
       "      <td>0.118558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>TFIDF__purchase price</td>\n",
       "      <td>0.117056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>TFIDF__shaving</td>\n",
       "      <td>0.116665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>TFIDF__razor</td>\n",
       "      <td>0.116096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>TFIDF__now look</td>\n",
       "      <td>0.115732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>TFIDF__two week</td>\n",
       "      <td>0.115732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>TFIDF__of for</td>\n",
       "      <td>0.115732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>TFIDF__the total</td>\n",
       "      <td>0.114547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>TFIDF__a pen</td>\n",
       "      <td>0.112496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>TFIDF__speak of</td>\n",
       "      <td>0.111596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>TFIDF__different shaving</td>\n",
       "      <td>0.111596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>TFIDF__have used</td>\n",
       "      <td>0.110911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>TFIDF__week old</td>\n",
       "      <td>0.110763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>TFIDF__refreshing to</td>\n",
       "      <td>0.109261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>TFIDF__can think</td>\n",
       "      <td>0.107937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>TFIDF__no razor</td>\n",
       "      <td>0.106752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>TFIDF__get is</td>\n",
       "      <td>0.106752</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     feature       tfidf\n",
       "0         remainder__ranking  752.000000\n",
       "1       remainder__price_USD   10.000000\n",
       "2         remainder__overall    5.000000\n",
       "3         remainder__m_Other    1.000000\n",
       "4      remainder__c_skincare    1.000000\n",
       "5        remainder__verified    1.000000\n",
       "6                TFIDF__cost    0.129623\n",
       "7                TFIDF__that    0.120972\n",
       "8           TFIDF__old razor    0.118558\n",
       "9      TFIDF__purchase price    0.117056\n",
       "10            TFIDF__shaving    0.116665\n",
       "11              TFIDF__razor    0.116096\n",
       "12           TFIDF__now look    0.115732\n",
       "13           TFIDF__two week    0.115732\n",
       "14             TFIDF__of for    0.115732\n",
       "15          TFIDF__the total    0.114547\n",
       "16              TFIDF__a pen    0.112496\n",
       "17           TFIDF__speak of    0.111596\n",
       "18  TFIDF__different shaving    0.111596\n",
       "19          TFIDF__have used    0.110911\n",
       "20           TFIDF__week old    0.110763\n",
       "21      TFIDF__refreshing to    0.109261\n",
       "22          TFIDF__can think    0.107937\n",
       "23           TFIDF__no razor    0.106752\n",
       "24             TFIDF__get is    0.106752"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_feats_in_doc(Xtr, features, 1, top_n=25)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "69b1c46f",
   "metadata": {},
   "source": [
    "### Using stemming "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de285e68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911aa0de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8d92a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf4f011",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e59336e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "28854011",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "id": "28854011",
    "outputId": "6e931058-cc63-4d0a-d69a-3ce6e1195f33"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndef top_feats_by_class(Xtr, y, features, min_tfidf=0.1, top_n=25):\\n    ''' Return a list of dfs, where each df holds top_n features and their mean tfidf value\\n        calculated across documents with the same class label. '''\\n    dfs = []\\n    labels = np.unique(y)\\n    for label in labels:\\n        ids = np.where(y==label)\\n        feats_df = top_mean_feats(Xtr, features, ids, min_tfidf=min_tfidf, top_n=top_n)\\n        feats_df.label = label\\n        dfs.append(feats_df)\\n    return dfs\\n    \""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "def top_feats_by_class(Xtr, y, features, min_tfidf=0.1, top_n=25):\n",
    "    ''' Return a list of dfs, where each df holds top_n features and their mean tfidf value\n",
    "        calculated across documents with the same class label. '''\n",
    "    dfs = []\n",
    "    labels = np.unique(y)\n",
    "    for label in labels:\n",
    "        ids = np.where(y==label)\n",
    "        feats_df = top_mean_feats(Xtr, features, ids, min_tfidf=min_tfidf, top_n=top_n)\n",
    "        feats_df.label = label\n",
    "        dfs.append(feats_df)\n",
    "    return dfs\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cdcd7b1",
   "metadata": {
    "id": "2cdcd7b1"
   },
   "source": [
    "## Setting up pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "1069a88d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 275
    },
    "id": "1069a88d",
    "outputId": "80284d6e-0054-4cd7-831a-318c014e6446"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>overall</th>\n",
       "      <th>vote</th>\n",
       "      <th>verified</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>trending_asin</th>\n",
       "      <th>price_USD</th>\n",
       "      <th>ranking</th>\n",
       "      <th>also_buy_counts</th>\n",
       "      <th>also_view_counts</th>\n",
       "      <th>c_accessories</th>\n",
       "      <th>...</th>\n",
       "      <th>m_Pureology</th>\n",
       "      <th>m_RUSK</th>\n",
       "      <th>m_Red Flower</th>\n",
       "      <th>m_Rene Furterer</th>\n",
       "      <th>m_Stila</th>\n",
       "      <th>m_StriVectin</th>\n",
       "      <th>m_TS</th>\n",
       "      <th>m_The Art of Shaving</th>\n",
       "      <th>m_Vichy</th>\n",
       "      <th>m_theBalm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>Bought for my daughter.</td>\n",
       "      <td>0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>4324.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>Great cream for the skin.</td>\n",
       "      <td>0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>4324.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   overall  vote  verified                 reviewText  trending_asin  \\\n",
       "0      5.0   0.0         1    Bought for my daughter.              0   \n",
       "1      5.0   0.0         1  Great cream for the skin.              0   \n",
       "\n",
       "   price_USD  ranking  also_buy_counts  also_view_counts  c_accessories  ...  \\\n",
       "0       30.0   4324.0             56.0              48.0            0.0  ...   \n",
       "1       30.0   4324.0             56.0              48.0            0.0  ...   \n",
       "\n",
       "   m_Pureology  m_RUSK  m_Red Flower  m_Rene Furterer  m_Stila  m_StriVectin  \\\n",
       "0          0.0     0.0           0.0              0.0      0.0           0.0   \n",
       "1          0.0     0.0           0.0              0.0      0.0           0.0   \n",
       "\n",
       "   m_TS  m_The Art of Shaving  m_Vichy  m_theBalm  \n",
       "0   0.0                   0.0      0.0        0.0  \n",
       "1   0.0                   0.0      0.0        0.0  \n",
       "\n",
       "[2 rows x 61 columns]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "015ae9b1",
   "metadata": {
    "id": "015ae9b1"
   },
   "outputs": [],
   "source": [
    "num_features = ['overall', 'vote', 'verified',\n",
    "       'price_USD', 'ranking', 'also_buy_counts', 'also_view_counts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "020ad699",
   "metadata": {
    "id": "020ad699"
   },
   "outputs": [],
   "source": [
    "num_features = df.select_dtypes(include=['int64','float64']).columns\n",
    "num_features = num_features.drop(\"trending_asin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "c6986519",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 118
    },
    "id": "c6986519",
    "outputId": "d256f34e-d017-4b2c-e6d7-630822b84dad"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>ColumnTransformer(remainder=&#x27;passthrough&#x27;,\n",
       "                  transformers=[(&#x27;cv&#x27;,\n",
       "                                 CountVectorizer(min_df=0.05,\n",
       "                                                 stop_words=&#x27;english&#x27;),\n",
       "                                 &#x27;reviewText&#x27;),\n",
       "                                (&#x27;scale&#x27;, StandardScaler(),\n",
       "                                 Index([&#x27;overall&#x27;, &#x27;vote&#x27;, &#x27;verified&#x27;, &#x27;price_USD&#x27;, &#x27;ranking&#x27;,\n",
       "       &#x27;also_buy_counts&#x27;, &#x27;also_view_counts&#x27;, &#x27;c_accessories&#x27;,\n",
       "       &#x27;c_bath and shower&#x27;, &#x27;c_fragance&#x27;, &#x27;c_haircare&#x27;, &#x27;c_makeup&#x27;,\n",
       "       &#x27;c_menscare&#x27;, &#x27;c_nails and hands&#x27;, &#x27;c...\n",
       "       &#x27;m_Glo Skin&#x27;, &#x27;m_Jane Iredale&#x27;, &#x27;m_Japonesque&#x27;, &#x27;m_Julep&#x27;,\n",
       "       &#x27;m_L&#x27;occitane &#x27;, &#x27;m_LAFCO&#x27;, &#x27;m_LORAC&#x27;, &#x27;m_La Roche Posay&#x27;,\n",
       "       &#x27;m_Laura Geller&#x27;, &#x27;m_Mario Badescu&#x27;, &#x27;m_Mustela&#x27;, &#x27;m_NUXE&#x27;,\n",
       "       &#x27;m_Natura Bisse&#x27;, &#x27;m_OPI&#x27;, &#x27;m_Oribe&#x27;, &#x27;m_Other&#x27;, &#x27;m_PCA Skin&#x27;,\n",
       "       &#x27;m_Paul Mitchell&#x27;, &#x27;m_Pureology&#x27;, &#x27;m_RUSK&#x27;, &#x27;m_Red Flower&#x27;,\n",
       "       &#x27;m_Rene Furterer&#x27;, &#x27;m_Stila&#x27;, &#x27;m_StriVectin&#x27;, &#x27;m_TS&#x27;,\n",
       "       &#x27;m_The Art of Shaving&#x27;, &#x27;m_Vichy&#x27;, &#x27;m_theBalm&#x27;],\n",
       "      dtype=&#x27;object&#x27;))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-15\" type=\"checkbox\" ><label for=\"sk-estimator-id-15\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">ColumnTransformer</label><div class=\"sk-toggleable__content\"><pre>ColumnTransformer(remainder=&#x27;passthrough&#x27;,\n",
       "                  transformers=[(&#x27;cv&#x27;,\n",
       "                                 CountVectorizer(min_df=0.05,\n",
       "                                                 stop_words=&#x27;english&#x27;),\n",
       "                                 &#x27;reviewText&#x27;),\n",
       "                                (&#x27;scale&#x27;, StandardScaler(),\n",
       "                                 Index([&#x27;overall&#x27;, &#x27;vote&#x27;, &#x27;verified&#x27;, &#x27;price_USD&#x27;, &#x27;ranking&#x27;,\n",
       "       &#x27;also_buy_counts&#x27;, &#x27;also_view_counts&#x27;, &#x27;c_accessories&#x27;,\n",
       "       &#x27;c_bath and shower&#x27;, &#x27;c_fragance&#x27;, &#x27;c_haircare&#x27;, &#x27;c_makeup&#x27;,\n",
       "       &#x27;c_menscare&#x27;, &#x27;c_nails and hands&#x27;, &#x27;c...\n",
       "       &#x27;m_Glo Skin&#x27;, &#x27;m_Jane Iredale&#x27;, &#x27;m_Japonesque&#x27;, &#x27;m_Julep&#x27;,\n",
       "       &#x27;m_L&#x27;occitane &#x27;, &#x27;m_LAFCO&#x27;, &#x27;m_LORAC&#x27;, &#x27;m_La Roche Posay&#x27;,\n",
       "       &#x27;m_Laura Geller&#x27;, &#x27;m_Mario Badescu&#x27;, &#x27;m_Mustela&#x27;, &#x27;m_NUXE&#x27;,\n",
       "       &#x27;m_Natura Bisse&#x27;, &#x27;m_OPI&#x27;, &#x27;m_Oribe&#x27;, &#x27;m_Other&#x27;, &#x27;m_PCA Skin&#x27;,\n",
       "       &#x27;m_Paul Mitchell&#x27;, &#x27;m_Pureology&#x27;, &#x27;m_RUSK&#x27;, &#x27;m_Red Flower&#x27;,\n",
       "       &#x27;m_Rene Furterer&#x27;, &#x27;m_Stila&#x27;, &#x27;m_StriVectin&#x27;, &#x27;m_TS&#x27;,\n",
       "       &#x27;m_The Art of Shaving&#x27;, &#x27;m_Vichy&#x27;, &#x27;m_theBalm&#x27;],\n",
       "      dtype=&#x27;object&#x27;))])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-16\" type=\"checkbox\" ><label for=\"sk-estimator-id-16\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">cv</label><div class=\"sk-toggleable__content\"><pre>reviewText</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-17\" type=\"checkbox\" ><label for=\"sk-estimator-id-17\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer(min_df=0.05, stop_words=&#x27;english&#x27;)</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-18\" type=\"checkbox\" ><label for=\"sk-estimator-id-18\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">scale</label><div class=\"sk-toggleable__content\"><pre>Index([&#x27;overall&#x27;, &#x27;vote&#x27;, &#x27;verified&#x27;, &#x27;price_USD&#x27;, &#x27;ranking&#x27;,\n",
       "       &#x27;also_buy_counts&#x27;, &#x27;also_view_counts&#x27;, &#x27;c_accessories&#x27;,\n",
       "       &#x27;c_bath and shower&#x27;, &#x27;c_fragance&#x27;, &#x27;c_haircare&#x27;, &#x27;c_makeup&#x27;,\n",
       "       &#x27;c_menscare&#x27;, &#x27;c_nails and hands&#x27;, &#x27;c_other&#x27;, &#x27;c_skincare&#x27;, &#x27;c_suncare&#x27;,\n",
       "       &#x27;m_AHAVA&#x27;, &#x27;m_Archipelago&#x27;, &#x27;m_BaBylissPro&#x27;, &#x27;m_Bioelements&#x27;, &#x27;m_Bliss&#x27;,\n",
       "       &#x27;m_Burberry&#x27;, &#x27;m_Butter London&#x27;, &#x27;m_CHI&#x27;, &#x27;m_CND&#x27;, &#x27;m_Calvin Klein&#x27;,\n",
       "       &#x27;m_Crabtree &amp; Evelyn&#x27;, &#x27;m_Davines&#x27;, &#x27;m_Elemis&#x27;, &#x27;m_Elizabeth Arden&#x27;,\n",
       "       &#x27;m_Glo Skin&#x27;, &#x27;m_Jane Iredale&#x27;, &#x27;m_Japonesque&#x27;, &#x27;m_Julep&#x27;,\n",
       "       &#x27;m_L&#x27;occitane &#x27;, &#x27;m_LAFCO&#x27;, &#x27;m_LORAC&#x27;, &#x27;m_La Roche Posay&#x27;,\n",
       "       &#x27;m_Laura Geller&#x27;, &#x27;m_Mario Badescu&#x27;, &#x27;m_Mustela&#x27;, &#x27;m_NUXE&#x27;,\n",
       "       &#x27;m_Natura Bisse&#x27;, &#x27;m_OPI&#x27;, &#x27;m_Oribe&#x27;, &#x27;m_Other&#x27;, &#x27;m_PCA Skin&#x27;,\n",
       "       &#x27;m_Paul Mitchell&#x27;, &#x27;m_Pureology&#x27;, &#x27;m_RUSK&#x27;, &#x27;m_Red Flower&#x27;,\n",
       "       &#x27;m_Rene Furterer&#x27;, &#x27;m_Stila&#x27;, &#x27;m_StriVectin&#x27;, &#x27;m_TS&#x27;,\n",
       "       &#x27;m_The Art of Shaving&#x27;, &#x27;m_Vichy&#x27;, &#x27;m_theBalm&#x27;],\n",
       "      dtype=&#x27;object&#x27;)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-19\" type=\"checkbox\" ><label for=\"sk-estimator-id-19\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-20\" type=\"checkbox\" ><label for=\"sk-estimator-id-20\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">remainder</label><div class=\"sk-toggleable__content\"><pre>[]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-21\" type=\"checkbox\" ><label for=\"sk-estimator-id-21\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">passthrough</label><div class=\"sk-toggleable__content\"><pre>passthrough</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "ColumnTransformer(remainder='passthrough',\n",
       "                  transformers=[('cv',\n",
       "                                 CountVectorizer(min_df=0.05,\n",
       "                                                 stop_words='english'),\n",
       "                                 'reviewText'),\n",
       "                                ('scale', StandardScaler(),\n",
       "                                 Index(['overall', 'vote', 'verified', 'price_USD', 'ranking',\n",
       "       'also_buy_counts', 'also_view_counts', 'c_accessories',\n",
       "       'c_bath and shower', 'c_fragance', 'c_haircare', 'c_makeup',\n",
       "       'c_menscare', 'c_nails and hands', 'c...\n",
       "       'm_Glo Skin', 'm_Jane Iredale', 'm_Japonesque', 'm_Julep',\n",
       "       'm_L'occitane ', 'm_LAFCO', 'm_LORAC', 'm_La Roche Posay',\n",
       "       'm_Laura Geller', 'm_Mario Badescu', 'm_Mustela', 'm_NUXE',\n",
       "       'm_Natura Bisse', 'm_OPI', 'm_Oribe', 'm_Other', 'm_PCA Skin',\n",
       "       'm_Paul Mitchell', 'm_Pureology', 'm_RUSK', 'm_Red Flower',\n",
       "       'm_Rene Furterer', 'm_Stila', 'm_StriVectin', 'm_TS',\n",
       "       'm_The Art of Shaving', 'm_Vichy', 'm_theBalm'],\n",
       "      dtype='object'))])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#count vectorizer function\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "ct = ColumnTransformer([\n",
    "        ('cv', CountVectorizer( stop_words=\"english\",min_df=0.05), 'reviewText'),\n",
    "        ('scale' , StandardScaler(), num_features)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# 2. Fit \n",
    "ct.fit(X_remainder1_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "a12af5e6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a12af5e6",
    "outputId": "48c33d7d-cc06-4113-dd76-f8d8c4010fbc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19508, 91)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. Transform\n",
    "X_remainder_cv = ct.transform(X_remainder1_sample)\n",
    "X_remainder_cv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "c8ee5021",
   "metadata": {
    "id": "c8ee5021"
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from tempfile import mkdtemp\n",
    "from sklearn import svm\n",
    "\n",
    "# Set up a directory to cache the pipeline results\n",
    "\n",
    "cachedir = mkdtemp()\n",
    "\n",
    "#we give our estimators as a list of tuples: name:function.\n",
    "pipe_svm = [('transform', ct),\n",
    "           ('dim_reducer', PCA() ),\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('clf', SVC(random_state=10))]\n",
    "\n",
    "pipe = Pipeline(pipe_svm,verbose=4, memory=cachedir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6bbe15",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 201
    },
    "id": "bd6bbe15",
    "outputId": "868c8f17-e21b-4ed2-a75b-a650114ca308"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-a60cfe94d3f7>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpipe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#change to X_train, y_train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpipe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "#pipe.fit(X_train, y_train) #change to X_train, y_train\n",
    "\n",
    "#pipe.score(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "61c4b94d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "61c4b94d",
    "outputId": "32fe273b-8ecb-4c7b-87aa-5bb2ba20ce2b"
   },
   "outputs": [],
   "source": [
    "#figuring out params\n",
    "# pipe.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f121f33",
   "metadata": {
    "id": "4f121f33"
   },
   "source": [
    "## Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b86f61",
   "metadata": {
    "id": "23b86f61"
   },
   "source": [
    "Let's label the initial pieces like the min_df features that will apply across the each model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "9dbcbdc4",
   "metadata": {
    "id": "9dbcbdc4"
   },
   "outputs": [],
   "source": [
    "vectorize_mindf_list = [0.03,0.04,0.05]\n",
    "pca_n_components = [0.95,0.90,0.85,0.80]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f49072",
   "metadata": {
    "id": "80f49072"
   },
   "source": [
    "Since I have an imbalanced data set it would be good to look at F1 score, the harmonic mean between recall and precision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b1cf16",
   "metadata": {
    "id": "d4b1cf16"
   },
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782572a0",
   "metadata": {
    "id": "782572a0"
   },
   "outputs": [],
   "source": [
    "# Range of c-values \n",
    "c_values = [.001 ,0.1, 1, 10, 100, 1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8ffdb1",
   "metadata": {
    "id": "ad8ffdb1"
   },
   "outputs": [],
   "source": [
    "# Parameter grid\n",
    "log_param_grid = [\n",
    "  \n",
    "      # l2 without PCA\n",
    "    {'transform__cv__min_df' : vectorize_mindf_list ,\n",
    "     'transform__cv__max_features': [500],\n",
    "     'scaler' : ['passthrough', StandardScaler(with_mean=False),MinMaxScaler()],\n",
    "     'dim_reducer': ['passthrough'],\n",
    "     'clf': [LogisticRegression(solver='lbfgs', random_state=8, n_jobs=-1,max_iter=10000)],\n",
    "     'clf__C': c_values\n",
    "    },\n",
    "    #l2 with PCA\n",
    "     {'transform__cv__min_df' : vectorize_mindf_list ,\n",
    "     'transform__cv__max_features': [500],\n",
    "     'scaler' : ['passthrough', StandardScaler(with_mean=False),MinMaxScaler()],\n",
    "     'dim_reducer': [PCA()],\n",
    "     'dim_reducer__n_components': pca_n_components, \n",
    "     'clf': [LogisticRegression(solver='lbfgs', random_state=8, n_jobs=-1,max_iter=10000)],\n",
    "     'clf__C': c_values\n",
    "    },\n",
    "   \n",
    "     #L1 without PCA \n",
    "    {'transform__cv__min_df' : vectorize_mindf_list ,\n",
    "     'transform__cv__max_features': [500],\n",
    "     'scaler': [StandardScaler(), MinMaxScaler()], \n",
    "     'dim_reducer': ['passthrough'],\n",
    "     'clf' : [LogisticRegression(solver = 'liblinear', penalty='l1', random_state=8, n_jobs=-1,max_iter=10000)],\n",
    "     'clf__C': c_values }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6YaALlyWYIY",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e6YaALlyWYIY",
    "outputId": "2fabcd93-f81c-4728-c699-317468faff3a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n",
      "[Pipeline] ......... (step 1 of 4) Processing transform, total=   1.9s\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.0s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 12.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  23.9s\n",
      "[CV 1/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, penalty='l1', random_state=8,\n",
      "                   solver='liblinear'), clf__C=1, dim_reducer=passthrough, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.05;, score=0.903 total time=  28.0s\n",
      "[Pipeline] ......... (step 1 of 4) Processing transform, total=   1.9s\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.0s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 12.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  23.9s\n",
      "[CV 2/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, penalty='l1', random_state=8,\n",
      "                   solver='liblinear'), clf__C=1, dim_reducer=passthrough, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.05;, score=0.900 total time=  28.0s\n",
      "[Pipeline] ......... (step 1 of 4) Processing transform, total=   1.9s\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.0s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 12.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  24.4s\n",
      "[CV 3/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, penalty='l1', random_state=8,\n",
      "                   solver='liblinear'), clf__C=1, dim_reducer=passthrough, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.05;, score=0.902 total time=  28.5s\n",
      "[Pipeline] ......... (step 1 of 4) Processing transform, total=   1.9s\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.0s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 12.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  24.3s\n",
      "[CV 4/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, penalty='l1', random_state=8,\n",
      "                   solver='liblinear'), clf__C=1, dim_reducer=passthrough, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.05;, score=0.899 total time=  28.4s\n",
      "[Pipeline] ......... (step 1 of 4) Processing transform, total=   1.9s\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.0s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 12.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  23.4s\n",
      "[CV 5/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, penalty='l1', random_state=8,\n",
      "                   solver='liblinear'), clf__C=1, dim_reducer=passthrough, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.05;, score=0.901 total time=  27.4s\n",
      "[Pipeline] ......... (step 1 of 4) Processing transform, total=   1.9s\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.5s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  17.9s\n",
      "[CV 1/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=1, dim_reducer=PCA(), dim_reducer__n_components=0.95, scaler=passthrough, transform__cv__max_features=500, transform__cv__min_df=0.03;, score=0.903 total time=  22.6s\n",
      "[Pipeline] ......... (step 1 of 4) Processing transform, total=   1.9s\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.6s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  21.0s\n",
      "[CV 2/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=1, dim_reducer=PCA(), dim_reducer__n_components=0.95, scaler=passthrough, transform__cv__max_features=500, transform__cv__min_df=0.03;, score=0.900 total time=  25.8s\n",
      "[Pipeline] ......... (step 1 of 4) Processing transform, total=   1.9s\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.7s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  18.8s\n",
      "[CV 3/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=1, dim_reducer=PCA(), dim_reducer__n_components=0.95, scaler=passthrough, transform__cv__max_features=500, transform__cv__min_df=0.03;, score=0.902 total time=  23.7s\n",
      "[Pipeline] ......... (step 1 of 4) Processing transform, total=   1.9s\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.5s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  18.9s\n",
      "[CV 4/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=1, dim_reducer=PCA(), dim_reducer__n_components=0.95, scaler=passthrough, transform__cv__max_features=500, transform__cv__min_df=0.03;, score=0.899 total time=  23.6s\n",
      "[Pipeline] ......... (step 1 of 4) Processing transform, total=   1.9s\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.7s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  21.9s\n",
      "[CV 5/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=1, dim_reducer=PCA(), dim_reducer__n_components=0.95, scaler=passthrough, transform__cv__max_features=500, transform__cv__min_df=0.03;, score=0.901 total time=  26.7s\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.4s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  13.2s\n",
      "[CV 1/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=1, dim_reducer=PCA(), dim_reducer__n_components=0.9, scaler=passthrough, transform__cv__max_features=500, transform__cv__min_df=0.05;, score=0.904 total time=  14.9s\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.5s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  11.1s\n",
      "[CV 2/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=1, dim_reducer=PCA(), dim_reducer__n_components=0.9, scaler=passthrough, transform__cv__max_features=500, transform__cv__min_df=0.05;, score=0.901 total time=  12.8s\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.5s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  11.5s\n",
      "[CV 3/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=1, dim_reducer=PCA(), dim_reducer__n_components=0.9, scaler=passthrough, transform__cv__max_features=500, transform__cv__min_df=0.05;, score=0.902 total time=  13.3s\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.4s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  14.1s\n",
      "[CV 4/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=1, dim_reducer=PCA(), dim_reducer__n_components=0.9, scaler=passthrough, transform__cv__max_features=500, transform__cv__min_df=0.05;, score=0.900 total time=  15.7s\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.3s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  11.4s\n",
      "[CV 5/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=1, dim_reducer=PCA(), dim_reducer__n_components=0.9, scaler=passthrough, transform__cv__max_features=500, transform__cv__min_df=0.05;, score=0.902 total time=  12.9s\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.5s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  13.4s\n",
      "[CV 1/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=1, dim_reducer=PCA(), dim_reducer__n_components=0.9, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.03;, score=0.905 total time=  15.6s\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.5s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  14.9s\n",
      "[CV 2/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=1, dim_reducer=PCA(), dim_reducer__n_components=0.9, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.03;, score=0.902 total time=  17.1s\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.5s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  17.6s\n",
      "[CV 3/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=1, dim_reducer=PCA(), dim_reducer__n_components=0.9, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.03;, score=0.903 total time=  19.8s\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.6s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  16.4s\n",
      "[CV 4/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=1, dim_reducer=PCA(), dim_reducer__n_components=0.9, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.03;, score=0.901 total time=  18.6s\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.6s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  15.1s\n",
      "[CV 5/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=1, dim_reducer=PCA(), dim_reducer__n_components=0.9, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.03;, score=0.902 total time=  17.4s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  26.1s\n",
      "[CV 1/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=10, dim_reducer=PCA(), dim_reducer__n_components=0.9, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.03;, score=0.904 total time=  27.2s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  25.6s\n",
      "[CV 2/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=10, dim_reducer=PCA(), dim_reducer__n_components=0.9, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.03;, score=0.901 total time=  26.7s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  33.2s\n",
      "[CV 3/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=10, dim_reducer=PCA(), dim_reducer__n_components=0.9, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.03;, score=0.903 total time=  34.4s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  26.6s\n",
      "[CV 4/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=10, dim_reducer=PCA(), dim_reducer__n_components=0.9, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.03;, score=0.901 total time=  27.7s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  27.4s\n",
      "[CV 5/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=10, dim_reducer=PCA(), dim_reducer__n_components=0.9, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.03;, score=0.902 total time=  28.6s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.1s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   5.1s\n",
      "[CV 1/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=0.1, dim_reducer=PCA(), dim_reducer__n_components=0.95, scaler=StandardScaler(with_mean=False), transform__cv__max_features=500, transform__cv__min_df=0.03;, score=0.903 total time=   6.5s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.1s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   5.2s\n",
      "[CV 2/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=0.1, dim_reducer=PCA(), dim_reducer__n_components=0.95, scaler=StandardScaler(with_mean=False), transform__cv__max_features=500, transform__cv__min_df=0.03;, score=0.900 total time=   6.6s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.1s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   5.1s\n",
      "[CV 3/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=0.1, dim_reducer=PCA(), dim_reducer__n_components=0.95, scaler=StandardScaler(with_mean=False), transform__cv__max_features=500, transform__cv__min_df=0.03;, score=0.902 total time=   6.6s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.1s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   4.9s\n",
      "[CV 4/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=0.1, dim_reducer=PCA(), dim_reducer__n_components=0.95, scaler=StandardScaler(with_mean=False), transform__cv__max_features=500, transform__cv__min_df=0.03;, score=0.900 total time=   6.3s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.1s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   6.2s\n",
      "[CV 5/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=0.1, dim_reducer=PCA(), dim_reducer__n_components=0.95, scaler=StandardScaler(with_mean=False), transform__cv__max_features=500, transform__cv__min_df=0.03;, score=0.901 total time=   7.6s\n",
      "[Pipeline] ......... (step 1 of 4) Processing transform, total=   1.9s\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.4s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   9.0s\n",
      "[CV 1/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=1, dim_reducer=PCA(), dim_reducer__n_components=0.85, scaler=passthrough, transform__cv__max_features=500, transform__cv__min_df=0.04;, score=0.904 total time=  13.4s\n",
      "[Pipeline] ......... (step 1 of 4) Processing transform, total=   1.9s\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.4s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   8.8s\n",
      "[CV 2/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=1, dim_reducer=PCA(), dim_reducer__n_components=0.85, scaler=passthrough, transform__cv__max_features=500, transform__cv__min_df=0.04;, score=0.901 total time=  13.3s\n",
      "[Pipeline] ......... (step 1 of 4) Processing transform, total=   1.9s\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.4s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   9.2s\n",
      "[CV 3/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=1, dim_reducer=PCA(), dim_reducer__n_components=0.85, scaler=passthrough, transform__cv__max_features=500, transform__cv__min_df=0.04;, score=0.903 total time=  13.7s\n",
      "[Pipeline] ......... (step 1 of 4) Processing transform, total=   1.9s\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.4s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  11.3s\n",
      "[CV 4/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=1, dim_reducer=PCA(), dim_reducer__n_components=0.85, scaler=passthrough, transform__cv__max_features=500, transform__cv__min_df=0.04;, score=0.901 total time=  15.7s\n",
      "[Pipeline] ......... (step 1 of 4) Processing transform, total=   1.9s\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.4s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   6.8s\n",
      "[CV 5/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=1, dim_reducer=PCA(), dim_reducer__n_components=0.85, scaler=passthrough, transform__cv__max_features=500, transform__cv__min_df=0.04;, score=0.902 total time=  11.3s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   5.1s\n",
      "[CV 1/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=0.1, dim_reducer=PCA(), dim_reducer__n_components=0.85, scaler=passthrough, transform__cv__max_features=500, transform__cv__min_df=0.04;, score=0.904 total time=   6.1s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   5.6s\n",
      "[CV 2/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=0.1, dim_reducer=PCA(), dim_reducer__n_components=0.85, scaler=passthrough, transform__cv__max_features=500, transform__cv__min_df=0.04;, score=0.901 total time=   6.6s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   5.9s\n",
      "[CV 3/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=0.1, dim_reducer=PCA(), dim_reducer__n_components=0.85, scaler=passthrough, transform__cv__max_features=500, transform__cv__min_df=0.04;, score=0.903 total time=   6.9s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   5.6s\n",
      "[CV 4/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=0.1, dim_reducer=PCA(), dim_reducer__n_components=0.85, scaler=passthrough, transform__cv__max_features=500, transform__cv__min_df=0.04;, score=0.901 total time=   6.6s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   5.1s\n",
      "[CV 5/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=0.1, dim_reducer=PCA(), dim_reducer__n_components=0.85, scaler=passthrough, transform__cv__max_features=500, transform__cv__min_df=0.04;, score=0.902 total time=   6.1s\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.6s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  32.8s\n",
      "[CV 1/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=100, dim_reducer=PCA(), dim_reducer__n_components=0.8, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.03;, score=0.904 total time=  34.9s\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.5s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  28.7s\n",
      "[CV 2/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=100, dim_reducer=PCA(), dim_reducer__n_components=0.8, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.03;, score=0.901 total time=  30.9s\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.6s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  34.2s\n",
      "[CV 3/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=100, dim_reducer=PCA(), dim_reducer__n_components=0.8, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.03;, score=0.902 total time=  36.3s\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.5s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  45.4s\n",
      "[CV 4/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=100, dim_reducer=PCA(), dim_reducer__n_components=0.8, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.03;, score=0.901 total time=  47.5s\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   1.0s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  25.2s\n",
      "[CV 5/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=100, dim_reducer=PCA(), dim_reducer__n_components=0.8, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.03;, score=0.901 total time=  27.9s\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.4s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total= 2.3min\n",
      "[CV 1/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=1000, dim_reducer=PCA(), dim_reducer__n_components=0.9, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.04;, score=0.904 total time= 2.4min\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.4s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total= 1.0min\n",
      "[CV 2/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=1000, dim_reducer=PCA(), dim_reducer__n_components=0.9, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.04;, score=0.901 total time= 1.1min\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.4s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total= 1.6min\n",
      "[CV 3/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=1000, dim_reducer=PCA(), dim_reducer__n_components=0.9, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.04;, score=0.902 total time= 1.7min\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.4s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total= 2.0min\n",
      "[CV 4/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=1000, dim_reducer=PCA(), dim_reducer__n_components=0.9, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.04;, score=0.900 total time= 2.0min\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.6s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total= 1.6min\n",
      "[CV 5/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=1000, dim_reducer=PCA(), dim_reducer__n_components=0.9, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.04;, score=0.902 total time= 1.6min\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total= 1.9min\n",
      "[CV 1/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=1000, dim_reducer=PCA(), dim_reducer__n_components=0.85, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.04;, score=0.904 total time= 2.0min\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  50.7s\n",
      "[CV 2/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=1000, dim_reducer=PCA(), dim_reducer__n_components=0.85, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.04;, score=0.901 total time=  52.1s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  43.7s\n",
      "[CV 3/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=1000, dim_reducer=PCA(), dim_reducer__n_components=0.85, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.04;, score=0.903 total time=  45.1s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  49.6s\n",
      "[CV 4/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=1000, dim_reducer=PCA(), dim_reducer__n_components=0.85, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.04;, score=0.901 total time=  50.9s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total= 1.3min\n",
      "[CV 5/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=1000, dim_reducer=PCA(), dim_reducer__n_components=0.85, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.04;, score=0.902 total time= 1.3min\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  46.8s\n",
      "[CV 1/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=1000, dim_reducer=passthrough, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.05;, score=0.903 total time=  47.9s\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  35.5s\n",
      "[CV 2/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=1000, dim_reducer=passthrough, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.05;, score=0.899 total time=  36.5s\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  42.2s\n",
      "[CV 3/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=1000, dim_reducer=passthrough, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.05;, score=0.902 total time=  43.3s\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  42.6s\n",
      "[CV 4/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=1000, dim_reducer=passthrough, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.05;, score=0.899 total time=  43.6s\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  44.3s\n",
      "[CV 5/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=1000, dim_reducer=passthrough, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.05;, score=0.901 total time=  45.3s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  20.7s\n",
      "[CV 1/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=1000, dim_reducer=PCA(), dim_reducer__n_components=0.9, scaler=StandardScaler(with_mean=False), transform__cv__max_features=500, transform__cv__min_df=0.03;, score=0.904 total time=  22.1s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.1s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  22.0s\n",
      "[CV 2/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=1000, dim_reducer=PCA(), dim_reducer__n_components=0.9, scaler=StandardScaler(with_mean=False), transform__cv__max_features=500, transform__cv__min_df=0.03;, score=0.901 total time=  23.4s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  22.9s\n",
      "[CV 3/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=1000, dim_reducer=PCA(), dim_reducer__n_components=0.9, scaler=StandardScaler(with_mean=False), transform__cv__max_features=500, transform__cv__min_df=0.03;, score=0.903 total time=  24.3s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  25.0s\n",
      "[CV 4/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=1000, dim_reducer=PCA(), dim_reducer__n_components=0.9, scaler=StandardScaler(with_mean=False), transform__cv__max_features=500, transform__cv__min_df=0.03;, score=0.900 total time=  26.3s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  24.4s\n",
      "[CV 5/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=1000, dim_reducer=PCA(), dim_reducer__n_components=0.9, scaler=StandardScaler(with_mean=False), transform__cv__max_features=500, transform__cv__min_df=0.03;, score=0.901 total time=  25.7s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total= 1.9min\n",
      "[CV 1/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=1000, dim_reducer=PCA(), dim_reducer__n_components=0.95, scaler=passthrough, transform__cv__max_features=500, transform__cv__min_df=0.03;, score=0.903 total time= 2.0min\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total= 1.7min\n",
      "[CV 2/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=1000, dim_reducer=PCA(), dim_reducer__n_components=0.95, scaler=passthrough, transform__cv__max_features=500, transform__cv__min_df=0.03;, score=0.900 total time= 1.7min\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total= 2.2min\n",
      "[CV 3/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=1000, dim_reducer=PCA(), dim_reducer__n_components=0.95, scaler=passthrough, transform__cv__max_features=500, transform__cv__min_df=0.03;, score=0.902 total time= 2.2min\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total= 2.5min\n",
      "[CV 4/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=1000, dim_reducer=PCA(), dim_reducer__n_components=0.95, scaler=passthrough, transform__cv__max_features=500, transform__cv__min_df=0.03;, score=0.900 total time= 2.5min\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total= 1.8min\n",
      "[CV 5/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=1000, dim_reducer=PCA(), dim_reducer__n_components=0.95, scaler=passthrough, transform__cv__max_features=500, transform__cv__min_df=0.03;, score=0.901 total time= 1.8min\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.3s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   9.9s\n",
      "[CV 1/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=1, dim_reducer=PCA(), dim_reducer__n_components=0.85, scaler=passthrough, transform__cv__max_features=500, transform__cv__min_df=0.05;, score=0.904 total time=  11.5s\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.3s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  10.7s\n",
      "[CV 2/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=1, dim_reducer=PCA(), dim_reducer__n_components=0.85, scaler=passthrough, transform__cv__max_features=500, transform__cv__min_df=0.05;, score=0.901 total time=  12.2s\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.3s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   7.8s\n",
      "[CV 3/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=1, dim_reducer=PCA(), dim_reducer__n_components=0.85, scaler=passthrough, transform__cv__max_features=500, transform__cv__min_df=0.05;, score=0.903 total time=   9.4s\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.4s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   9.3s\n",
      "[CV 4/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=1, dim_reducer=PCA(), dim_reducer__n_components=0.85, scaler=passthrough, transform__cv__max_features=500, transform__cv__min_df=0.05;, score=0.901 total time=  10.9s\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.4s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  10.6s\n",
      "[CV 5/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=1, dim_reducer=PCA(), dim_reducer__n_components=0.85, scaler=passthrough, transform__cv__max_features=500, transform__cv__min_df=0.05;, score=0.902 total time=  12.2s\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.3s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.1s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  18.8s\n",
      "[CV 1/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=1000, dim_reducer=PCA(), dim_reducer__n_components=0.95, scaler=StandardScaler(with_mean=False), transform__cv__max_features=500, transform__cv__min_df=0.05;, score=0.903 total time=  20.7s\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.3s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.1s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  15.8s\n",
      "[CV 2/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=1000, dim_reducer=PCA(), dim_reducer__n_components=0.95, scaler=StandardScaler(with_mean=False), transform__cv__max_features=500, transform__cv__min_df=0.05;, score=0.900 total time=  17.7s\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.3s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.1s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  15.9s\n",
      "[CV 3/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=1000, dim_reducer=PCA(), dim_reducer__n_components=0.95, scaler=StandardScaler(with_mean=False), transform__cv__max_features=500, transform__cv__min_df=0.05;, score=0.902 total time=  17.8s\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.3s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.1s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  42.4s\n",
      "[CV 4/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=1000, dim_reducer=PCA(), dim_reducer__n_components=0.95, scaler=StandardScaler(with_mean=False), transform__cv__max_features=500, transform__cv__min_df=0.05;, score=0.899 total time=  44.2s\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.3s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.1s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  42.3s\n",
      "[CV 5/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=1000, dim_reducer=PCA(), dim_reducer__n_components=0.95, scaler=StandardScaler(with_mean=False), transform__cv__max_features=500, transform__cv__min_df=0.05;, score=0.900 total time=  44.2s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   1.7s\n",
      "[CV 1/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=0.001, dim_reducer=PCA(), dim_reducer__n_components=0.95, scaler=passthrough, transform__cv__max_features=500, transform__cv__min_df=0.05;, score=0.905 total time=   2.7s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   1.6s\n",
      "[CV 2/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=0.001, dim_reducer=PCA(), dim_reducer__n_components=0.95, scaler=passthrough, transform__cv__max_features=500, transform__cv__min_df=0.05;, score=0.902 total time=   2.6s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   1.7s\n",
      "[CV 3/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=0.001, dim_reducer=PCA(), dim_reducer__n_components=0.95, scaler=passthrough, transform__cv__max_features=500, transform__cv__min_df=0.05;, score=0.903 total time=   2.7s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   1.7s\n",
      "[CV 4/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=0.001, dim_reducer=PCA(), dim_reducer__n_components=0.95, scaler=passthrough, transform__cv__max_features=500, transform__cv__min_df=0.05;, score=0.902 total time=   2.7s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   1.6s\n",
      "[CV 5/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=0.001, dim_reducer=PCA(), dim_reducer__n_components=0.95, scaler=passthrough, transform__cv__max_features=500, transform__cv__min_df=0.05;, score=0.903 total time=   2.6s\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.4s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   0.9s\n",
      "[CV 1/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=0.001, dim_reducer=PCA(), dim_reducer__n_components=0.8, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.04;, score=0.889 total time=   2.9s\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.4s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   1.3s\n",
      "[CV 2/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=0.001, dim_reducer=PCA(), dim_reducer__n_components=0.8, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.04;, score=0.889 total time=   3.2s\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.4s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   1.0s\n",
      "[CV 3/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=0.001, dim_reducer=PCA(), dim_reducer__n_components=0.8, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.04;, score=0.889 total time=   3.0s\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.4s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   1.1s\n",
      "[CV 4/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=0.001, dim_reducer=PCA(), dim_reducer__n_components=0.8, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.04;, score=0.889 total time=   3.0s\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.5s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   1.0s\n",
      "[CV 5/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=0.001, dim_reducer=PCA(), dim_reducer__n_components=0.8, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.04;, score=0.889 total time=   3.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   1.4s\n",
      "[CV 1/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=0.001, dim_reducer=PCA(), dim_reducer__n_components=0.85, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.04;, score=0.889 total time=   2.5s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   1.5s\n",
      "[CV 2/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=0.001, dim_reducer=PCA(), dim_reducer__n_components=0.85, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.04;, score=0.889 total time=   2.6s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   1.5s\n",
      "[CV 3/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=0.001, dim_reducer=PCA(), dim_reducer__n_components=0.85, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.04;, score=0.889 total time=   2.6s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   1.3s\n",
      "[CV 4/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=0.001, dim_reducer=PCA(), dim_reducer__n_components=0.85, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.04;, score=0.889 total time=   2.5s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   1.4s\n",
      "[CV 5/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=0.001, dim_reducer=PCA(), dim_reducer__n_components=0.85, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.04;, score=0.889 total time=   2.5s\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.0s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.1s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   6.7s\n",
      "[CV 1/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=10, dim_reducer=passthrough, scaler=StandardScaler(with_mean=False), transform__cv__max_features=500, transform__cv__min_df=0.03;, score=0.904 total time=   8.2s\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.0s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.1s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   6.3s\n",
      "[CV 2/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=10, dim_reducer=passthrough, scaler=StandardScaler(with_mean=False), transform__cv__max_features=500, transform__cv__min_df=0.03;, score=0.900 total time=   7.9s\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.0s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.1s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   6.6s\n",
      "[CV 3/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=10, dim_reducer=passthrough, scaler=StandardScaler(with_mean=False), transform__cv__max_features=500, transform__cv__min_df=0.03;, score=0.902 total time=   8.1s\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.0s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.1s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   6.4s\n",
      "[CV 4/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=10, dim_reducer=passthrough, scaler=StandardScaler(with_mean=False), transform__cv__max_features=500, transform__cv__min_df=0.03;, score=0.900 total time=   7.9s\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.0s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.1s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   6.5s\n",
      "[CV 5/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=10, dim_reducer=passthrough, scaler=StandardScaler(with_mean=False), transform__cv__max_features=500, transform__cv__min_df=0.03;, score=0.902 total time=   8.0s\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.0s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.1s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  25.6s\n",
      "[CV 1/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=1, dim_reducer=passthrough, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.04;, score=0.903 total time=  27.0s\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.0s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.1s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  24.1s\n",
      "[CV 2/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=1, dim_reducer=passthrough, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.04;, score=0.900 total time=  25.5s\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.0s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.1s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  26.0s\n",
      "[CV 3/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=1, dim_reducer=passthrough, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.04;, score=0.903 total time=  27.3s\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.0s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.1s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  22.7s\n",
      "[CV 4/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=1, dim_reducer=passthrough, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.04;, score=0.900 total time=  24.1s\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.0s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.1s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  24.1s\n",
      "[CV 5/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=1, dim_reducer=passthrough, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.04;, score=0.902 total time=  25.5s\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.5s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   5.9s\n",
      "[CV 1/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=0.1, dim_reducer=PCA(), dim_reducer__n_components=0.85, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.03;, score=0.891 total time=   8.0s\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.6s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   4.6s\n",
      "[CV 2/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=0.1, dim_reducer=PCA(), dim_reducer__n_components=0.85, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.03;, score=0.892 total time=   6.8s\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.5s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   5.8s\n",
      "[CV 3/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=0.1, dim_reducer=PCA(), dim_reducer__n_components=0.85, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.03;, score=0.892 total time=   8.0s\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.5s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   5.4s\n",
      "[CV 4/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=0.1, dim_reducer=PCA(), dim_reducer__n_components=0.85, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.03;, score=0.891 total time=   7.5s\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.7s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   5.5s\n",
      "[CV 5/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=0.1, dim_reducer=PCA(), dim_reducer__n_components=0.85, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.03;, score=0.892 total time=   7.9s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  47.0s\n",
      "[CV 1/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=10, dim_reducer=PCA(), dim_reducer__n_components=0.95, scaler=StandardScaler(with_mean=False), transform__cv__max_features=500, transform__cv__min_df=0.03;, score=0.903 total time=  48.2s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  45.9s\n",
      "[CV 2/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=10, dim_reducer=PCA(), dim_reducer__n_components=0.95, scaler=StandardScaler(with_mean=False), transform__cv__max_features=500, transform__cv__min_df=0.03;, score=0.900 total time=  47.1s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  50.7s\n",
      "[CV 3/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=10, dim_reducer=PCA(), dim_reducer__n_components=0.95, scaler=StandardScaler(with_mean=False), transform__cv__max_features=500, transform__cv__min_df=0.03;, score=0.902 total time=  51.9s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  48.0s\n",
      "[CV 4/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=10, dim_reducer=PCA(), dim_reducer__n_components=0.95, scaler=StandardScaler(with_mean=False), transform__cv__max_features=500, transform__cv__min_df=0.03;, score=0.899 total time=  49.2s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  42.5s\n",
      "[CV 5/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=10, dim_reducer=PCA(), dim_reducer__n_components=0.95, scaler=StandardScaler(with_mean=False), transform__cv__max_features=500, transform__cv__min_df=0.03;, score=0.901 total time=  43.7s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   1.9s\n",
      "[CV 1/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=0.001, dim_reducer=PCA(), dim_reducer__n_components=0.9, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.03;, score=0.889 total time=   3.1s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   1.9s\n",
      "[CV 2/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=0.001, dim_reducer=PCA(), dim_reducer__n_components=0.9, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.03;, score=0.889 total time=   3.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   1.9s\n",
      "[CV 3/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=0.001, dim_reducer=PCA(), dim_reducer__n_components=0.9, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.03;, score=0.889 total time=   3.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   2.0s\n",
      "[CV 4/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=0.001, dim_reducer=PCA(), dim_reducer__n_components=0.9, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.03;, score=0.889 total time=   3.1s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   2.0s\n",
      "[CV 5/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=0.001, dim_reducer=PCA(), dim_reducer__n_components=0.9, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.03;, score=0.889 total time=   3.1s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   7.6s\n",
      "[CV 1/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=1, dim_reducer=PCA(), dim_reducer__n_components=0.85, scaler=StandardScaler(with_mean=False), transform__cv__max_features=500, transform__cv__min_df=0.04;, score=0.904 total time=   8.9s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   7.9s\n",
      "[CV 2/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=1, dim_reducer=PCA(), dim_reducer__n_components=0.85, scaler=StandardScaler(with_mean=False), transform__cv__max_features=500, transform__cv__min_df=0.04;, score=0.901 total time=   9.2s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   7.2s\n",
      "[CV 3/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=1, dim_reducer=PCA(), dim_reducer__n_components=0.85, scaler=StandardScaler(with_mean=False), transform__cv__max_features=500, transform__cv__min_df=0.04;, score=0.903 total time=   8.5s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   9.3s\n",
      "[CV 4/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=1, dim_reducer=PCA(), dim_reducer__n_components=0.85, scaler=StandardScaler(with_mean=False), transform__cv__max_features=500, transform__cv__min_df=0.04;, score=0.901 total time=  10.6s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   6.4s\n",
      "[CV 5/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=1, dim_reducer=PCA(), dim_reducer__n_components=0.85, scaler=StandardScaler(with_mean=False), transform__cv__max_features=500, transform__cv__min_df=0.04;, score=0.902 total time=   7.7s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  27.5s\n",
      "[CV 1/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=10, dim_reducer=PCA(), dim_reducer__n_components=0.9, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.04;, score=0.904 total time=  28.6s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  26.3s\n",
      "[CV 2/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=10, dim_reducer=PCA(), dim_reducer__n_components=0.9, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.04;, score=0.901 total time=  27.4s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  29.7s\n",
      "[CV 3/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=10, dim_reducer=PCA(), dim_reducer__n_components=0.9, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.04;, score=0.903 total time=  30.8s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  34.0s\n",
      "[CV 4/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=10, dim_reducer=PCA(), dim_reducer__n_components=0.9, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.04;, score=0.900 total time=  35.1s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  31.7s\n",
      "[CV 5/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=10, dim_reducer=PCA(), dim_reducer__n_components=0.9, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.04;, score=0.902 total time=  32.8s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total= 1.2min\n",
      "[CV 1/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=100, dim_reducer=PCA(), dim_reducer__n_components=0.95, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.03;, score=0.903 total time= 1.2min\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  50.4s\n",
      "[CV 2/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=100, dim_reducer=PCA(), dim_reducer__n_components=0.95, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.03;, score=0.900 total time=  51.8s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  50.0s\n",
      "[CV 3/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=100, dim_reducer=PCA(), dim_reducer__n_components=0.95, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.03;, score=0.902 total time=  51.4s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  45.2s\n",
      "[CV 4/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=100, dim_reducer=PCA(), dim_reducer__n_components=0.95, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.03;, score=0.899 total time=  46.6s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total= 1.0min\n",
      "[CV 5/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=100, dim_reducer=PCA(), dim_reducer__n_components=0.95, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.03;, score=0.901 total time= 1.1min\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  44.0s\n",
      "[CV 1/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=100, dim_reducer=passthrough, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.05;, score=0.903 total time=  45.1s\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  31.3s\n",
      "[CV 2/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=100, dim_reducer=passthrough, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.05;, score=0.899 total time=  32.3s\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  37.4s\n",
      "[CV 3/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=100, dim_reducer=passthrough, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.05;, score=0.902 total time=  38.4s\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  53.3s\n",
      "[CV 4/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=100, dim_reducer=passthrough, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.05;, score=0.899 total time=  54.4s\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  47.6s\n",
      "[CV 5/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=100, dim_reducer=passthrough, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.05;, score=0.901 total time=  48.6s\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.6s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total= 2.1min\n",
      "[CV 1/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=1000, dim_reducer=PCA(), dim_reducer__n_components=0.95, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.04;, score=0.903 total time= 2.2min\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.4s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total= 1.7min\n",
      "[CV 2/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=1000, dim_reducer=PCA(), dim_reducer__n_components=0.95, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.04;, score=0.900 total time= 1.7min\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.4s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total= 1.8min\n",
      "[CV 3/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=1000, dim_reducer=PCA(), dim_reducer__n_components=0.95, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.04;, score=0.901 total time= 1.9min\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.6s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total= 1.2min\n",
      "[CV 4/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=1000, dim_reducer=PCA(), dim_reducer__n_components=0.95, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.04;, score=0.899 total time= 1.3min\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.6s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total= 2.9min\n",
      "[CV 5/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=1000, dim_reducer=PCA(), dim_reducer__n_components=0.95, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.04;, score=0.901 total time= 2.9min\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total= 1.2min\n",
      "[CV 1/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=100, dim_reducer=PCA(), dim_reducer__n_components=0.9, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.03;, score=0.904 total time= 1.3min\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total= 1.1min\n",
      "[CV 2/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=100, dim_reducer=PCA(), dim_reducer__n_components=0.9, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.03;, score=0.901 total time= 1.1min\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  59.9s\n",
      "[CV 3/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=100, dim_reducer=PCA(), dim_reducer__n_components=0.9, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.03;, score=0.902 total time= 1.0min\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total= 1.0min\n",
      "[CV 4/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=100, dim_reducer=PCA(), dim_reducer__n_components=0.9, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.03;, score=0.900 total time= 1.0min\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total= 1.1min\n",
      "[CV 5/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=100, dim_reducer=PCA(), dim_reducer__n_components=0.9, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.03;, score=0.902 total time= 1.1min\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   5.4s\n",
      "[CV 1/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=0.1, dim_reducer=PCA(), dim_reducer__n_components=0.9, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.05;, score=0.896 total time=   6.6s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   3.9s\n",
      "[CV 2/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=0.1, dim_reducer=PCA(), dim_reducer__n_components=0.9, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.05;, score=0.898 total time=   5.2s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   5.5s\n",
      "[CV 3/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=0.1, dim_reducer=PCA(), dim_reducer__n_components=0.9, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.05;, score=0.898 total time=   6.7s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   4.0s\n",
      "[CV 4/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=0.1, dim_reducer=PCA(), dim_reducer__n_components=0.9, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.05;, score=0.901 total time=   5.2s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   5.6s\n",
      "[CV 5/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=0.1, dim_reducer=PCA(), dim_reducer__n_components=0.9, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.05;, score=0.899 total time=   6.9s\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.0s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 12.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  53.2s\n",
      "[CV 1/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, penalty='l1', random_state=8,\n",
      "                   solver='liblinear'), clf__C=1, dim_reducer=passthrough, scaler=StandardScaler(), transform__cv__max_features=500, transform__cv__min_df=0.05;, score=0.903 total time=  54.5s\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.0s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 12.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  36.7s\n",
      "[CV 2/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, penalty='l1', random_state=8,\n",
      "                   solver='liblinear'), clf__C=1, dim_reducer=passthrough, scaler=StandardScaler(), transform__cv__max_features=500, transform__cv__min_df=0.05;, score=0.899 total time=  38.0s\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.0s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 12.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  45.3s\n",
      "[CV 3/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, penalty='l1', random_state=8,\n",
      "                   solver='liblinear'), clf__C=1, dim_reducer=passthrough, scaler=StandardScaler(), transform__cv__max_features=500, transform__cv__min_df=0.05;, score=0.902 total time=  46.7s\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.0s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 12.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  46.9s\n",
      "[CV 4/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, penalty='l1', random_state=8,\n",
      "                   solver='liblinear'), clf__C=1, dim_reducer=passthrough, scaler=StandardScaler(), transform__cv__max_features=500, transform__cv__min_df=0.05;, score=0.899 total time=  48.2s\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.0s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 12.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  49.7s\n",
      "[CV 5/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, penalty='l1', random_state=8,\n",
      "                   solver='liblinear'), clf__C=1, dim_reducer=passthrough, scaler=StandardScaler(), transform__cv__max_features=500, transform__cv__min_df=0.05;, score=0.901 total time=  51.0s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   5.6s\n",
      "[CV 1/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=0.1, dim_reducer=PCA(), dim_reducer__n_components=0.85, scaler=passthrough, transform__cv__max_features=500, transform__cv__min_df=0.05;, score=0.904 total time=   6.6s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   5.5s\n",
      "[CV 2/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=0.1, dim_reducer=PCA(), dim_reducer__n_components=0.85, scaler=passthrough, transform__cv__max_features=500, transform__cv__min_df=0.05;, score=0.901 total time=   6.5s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   4.5s\n",
      "[CV 3/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=0.1, dim_reducer=PCA(), dim_reducer__n_components=0.85, scaler=passthrough, transform__cv__max_features=500, transform__cv__min_df=0.05;, score=0.903 total time=   5.5s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   5.1s\n",
      "[CV 4/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=0.1, dim_reducer=PCA(), dim_reducer__n_components=0.85, scaler=passthrough, transform__cv__max_features=500, transform__cv__min_df=0.05;, score=0.901 total time=   6.2s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   4.9s\n",
      "[CV 5/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=0.1, dim_reducer=PCA(), dim_reducer__n_components=0.85, scaler=passthrough, transform__cv__max_features=500, transform__cv__min_df=0.05;, score=0.901 total time=   5.9s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  11.4s\n",
      "[CV 1/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=1, dim_reducer=PCA(), dim_reducer__n_components=0.85, scaler=StandardScaler(with_mean=False), transform__cv__max_features=500, transform__cv__min_df=0.05;, score=0.904 total time=  12.7s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   9.2s\n",
      "[CV 2/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=1, dim_reducer=PCA(), dim_reducer__n_components=0.85, scaler=StandardScaler(with_mean=False), transform__cv__max_features=500, transform__cv__min_df=0.05;, score=0.901 total time=  10.5s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   6.9s\n",
      "[CV 3/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=1, dim_reducer=PCA(), dim_reducer__n_components=0.85, scaler=StandardScaler(with_mean=False), transform__cv__max_features=500, transform__cv__min_df=0.05;, score=0.903 total time=   8.2s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   8.2s\n",
      "[CV 4/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=1, dim_reducer=PCA(), dim_reducer__n_components=0.85, scaler=StandardScaler(with_mean=False), transform__cv__max_features=500, transform__cv__min_df=0.05;, score=0.901 total time=   9.4s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   9.7s\n",
      "[CV 5/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=1, dim_reducer=PCA(), dim_reducer__n_components=0.85, scaler=StandardScaler(with_mean=False), transform__cv__max_features=500, transform__cv__min_df=0.05;, score=0.902 total time=  11.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  10.2s\n",
      "[CV 1/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=1, dim_reducer=PCA(), dim_reducer__n_components=0.85, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.04;, score=0.894 total time=  11.3s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   8.4s\n",
      "[CV 2/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=1, dim_reducer=PCA(), dim_reducer__n_components=0.85, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.04;, score=0.901 total time=   9.4s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   9.4s\n",
      "[CV 3/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=1, dim_reducer=PCA(), dim_reducer__n_components=0.85, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.04;, score=0.903 total time=  10.5s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  10.1s\n",
      "[CV 4/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=1, dim_reducer=PCA(), dim_reducer__n_components=0.85, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.04;, score=0.892 total time=  11.2s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  10.4s\n",
      "[CV 5/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=1, dim_reducer=PCA(), dim_reducer__n_components=0.85, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.04;, score=0.893 total time=  11.5s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   1.6s\n",
      "[CV 1/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=0.001, dim_reducer=PCA(), dim_reducer__n_components=0.9, scaler=passthrough, transform__cv__max_features=500, transform__cv__min_df=0.04;, score=0.905 total time=   2.6s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   1.6s\n",
      "[CV 2/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=0.001, dim_reducer=PCA(), dim_reducer__n_components=0.9, scaler=passthrough, transform__cv__max_features=500, transform__cv__min_df=0.04;, score=0.903 total time=   2.6s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   1.5s\n",
      "[CV 3/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=0.001, dim_reducer=PCA(), dim_reducer__n_components=0.9, scaler=passthrough, transform__cv__max_features=500, transform__cv__min_df=0.04;, score=0.903 total time=   2.6s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   1.5s\n",
      "[CV 4/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=0.001, dim_reducer=PCA(), dim_reducer__n_components=0.9, scaler=passthrough, transform__cv__max_features=500, transform__cv__min_df=0.04;, score=0.902 total time=   2.6s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   1.5s\n",
      "[CV 5/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=0.001, dim_reducer=PCA(), dim_reducer__n_components=0.9, scaler=passthrough, transform__cv__max_features=500, transform__cv__min_df=0.04;, score=0.903 total time=   2.6s\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.3s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   3.1s\n",
      "[CV 1/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=1, dim_reducer=PCA(), dim_reducer__n_components=0.8, scaler=StandardScaler(with_mean=False), transform__cv__max_features=500, transform__cv__min_df=0.05;, score=0.904 total time=   4.8s\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.3s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   3.9s\n",
      "[CV 2/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=1, dim_reducer=PCA(), dim_reducer__n_components=0.8, scaler=StandardScaler(with_mean=False), transform__cv__max_features=500, transform__cv__min_df=0.05;, score=0.901 total time=   5.7s\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.4s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   3.8s\n",
      "[CV 3/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=1, dim_reducer=PCA(), dim_reducer__n_components=0.8, scaler=StandardScaler(with_mean=False), transform__cv__max_features=500, transform__cv__min_df=0.05;, score=0.902 total time=   5.6s\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.3s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   3.4s\n",
      "[CV 4/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=1, dim_reducer=PCA(), dim_reducer__n_components=0.8, scaler=StandardScaler(with_mean=False), transform__cv__max_features=500, transform__cv__min_df=0.05;, score=0.900 total time=   5.2s\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.3s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   3.2s\n",
      "[CV 5/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=1, dim_reducer=PCA(), dim_reducer__n_components=0.8, scaler=StandardScaler(with_mean=False), transform__cv__max_features=500, transform__cv__min_df=0.05;, score=0.900 total time=   5.0s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  13.9s\n",
      "[CV 1/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=1, dim_reducer=PCA(), dim_reducer__n_components=0.9, scaler=passthrough, transform__cv__max_features=500, transform__cv__min_df=0.03;, score=0.904 total time=  14.9s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  15.3s\n",
      "[CV 2/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=1, dim_reducer=PCA(), dim_reducer__n_components=0.9, scaler=passthrough, transform__cv__max_features=500, transform__cv__min_df=0.03;, score=0.901 total time=  16.3s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  15.3s\n",
      "[CV 3/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=1, dim_reducer=PCA(), dim_reducer__n_components=0.9, scaler=passthrough, transform__cv__max_features=500, transform__cv__min_df=0.03;, score=0.903 total time=  16.4s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  19.0s\n",
      "[CV 4/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=1, dim_reducer=PCA(), dim_reducer__n_components=0.9, scaler=passthrough, transform__cv__max_features=500, transform__cv__min_df=0.03;, score=0.900 total time=  20.1s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  17.7s\n",
      "[CV 5/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=1, dim_reducer=PCA(), dim_reducer__n_components=0.9, scaler=passthrough, transform__cv__max_features=500, transform__cv__min_df=0.03;, score=0.902 total time=  18.8s\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 12.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   0.4s\n",
      "[CV 1/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, penalty='l1', random_state=8,\n",
      "                   solver='liblinear'), clf__C=1000, dim_reducer=passthrough, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.04;, score=0.903 total time=   1.4s\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 12.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   0.4s\n",
      "[CV 2/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, penalty='l1', random_state=8,\n",
      "                   solver='liblinear'), clf__C=1000, dim_reducer=passthrough, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.04;, score=0.900 total time=   1.4s\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 12.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   0.4s\n",
      "[CV 3/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, penalty='l1', random_state=8,\n",
      "                   solver='liblinear'), clf__C=1000, dim_reducer=passthrough, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.04;, score=0.902 total time=   1.4s\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 12.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   0.4s\n",
      "[CV 4/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, penalty='l1', random_state=8,\n",
      "                   solver='liblinear'), clf__C=1000, dim_reducer=passthrough, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.04;, score=0.900 total time=   1.4s\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 12.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   0.4s\n",
      "[CV 5/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, penalty='l1', random_state=8,\n",
      "                   solver='liblinear'), clf__C=1000, dim_reducer=passthrough, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.04;, score=0.901 total time=   1.4s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   2.0s\n",
      "[CV 1/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=0.001, dim_reducer=PCA(), dim_reducer__n_components=0.95, scaler=passthrough, transform__cv__max_features=500, transform__cv__min_df=0.04;, score=0.904 total time=   3.0s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   2.0s\n",
      "[CV 2/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=0.001, dim_reducer=PCA(), dim_reducer__n_components=0.95, scaler=passthrough, transform__cv__max_features=500, transform__cv__min_df=0.04;, score=0.902 total time=   3.0s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   2.1s\n",
      "[CV 3/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=0.001, dim_reducer=PCA(), dim_reducer__n_components=0.95, scaler=passthrough, transform__cv__max_features=500, transform__cv__min_df=0.04;, score=0.903 total time=   3.2s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   2.1s\n",
      "[CV 4/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=0.001, dim_reducer=PCA(), dim_reducer__n_components=0.95, scaler=passthrough, transform__cv__max_features=500, transform__cv__min_df=0.04;, score=0.902 total time=   3.1s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   2.0s\n",
      "[CV 5/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=0.001, dim_reducer=PCA(), dim_reducer__n_components=0.95, scaler=passthrough, transform__cv__max_features=500, transform__cv__min_df=0.04;, score=0.903 total time=   3.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   8.6s\n",
      "[CV 1/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=1000, dim_reducer=PCA(), dim_reducer__n_components=0.85, scaler=StandardScaler(with_mean=False), transform__cv__max_features=500, transform__cv__min_df=0.04;, score=0.904 total time=   9.6s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   9.6s\n",
      "[CV 2/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=1000, dim_reducer=PCA(), dim_reducer__n_components=0.85, scaler=StandardScaler(with_mean=False), transform__cv__max_features=500, transform__cv__min_df=0.04;, score=0.901 total time=  10.7s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   8.7s\n",
      "[CV 3/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=1000, dim_reducer=PCA(), dim_reducer__n_components=0.85, scaler=StandardScaler(with_mean=False), transform__cv__max_features=500, transform__cv__min_df=0.04;, score=0.903 total time=   9.8s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  14.3s\n",
      "[CV 4/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=1000, dim_reducer=PCA(), dim_reducer__n_components=0.85, scaler=StandardScaler(with_mean=False), transform__cv__max_features=500, transform__cv__min_df=0.04;, score=0.901 total time=  15.4s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   6.3s\n",
      "[CV 5/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=1000, dim_reducer=PCA(), dim_reducer__n_components=0.85, scaler=StandardScaler(with_mean=False), transform__cv__max_features=500, transform__cv__min_df=0.04;, score=0.902 total time=   7.4s\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   4.2s\n",
      "[CV 1/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=0.001, dim_reducer=passthrough, scaler=StandardScaler(with_mean=False), transform__cv__max_features=500, transform__cv__min_df=0.03;, score=0.905 total time=   5.3s\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   3.9s\n",
      "[CV 2/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=0.001, dim_reducer=passthrough, scaler=StandardScaler(with_mean=False), transform__cv__max_features=500, transform__cv__min_df=0.03;, score=0.902 total time=   5.0s\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   3.7s\n",
      "[CV 3/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=0.001, dim_reducer=passthrough, scaler=StandardScaler(with_mean=False), transform__cv__max_features=500, transform__cv__min_df=0.03;, score=0.903 total time=   4.8s\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   3.7s\n",
      "[CV 4/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=0.001, dim_reducer=passthrough, scaler=StandardScaler(with_mean=False), transform__cv__max_features=500, transform__cv__min_df=0.03;, score=0.901 total time=   4.8s\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   4.1s\n",
      "[CV 5/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=0.001, dim_reducer=passthrough, scaler=StandardScaler(with_mean=False), transform__cv__max_features=500, transform__cv__min_df=0.03;, score=0.903 total time=   5.2s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  18.9s\n",
      "[CV 1/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=10, dim_reducer=PCA(), dim_reducer__n_components=0.95, scaler=passthrough, transform__cv__max_features=500, transform__cv__min_df=0.05;, score=0.903 total time=  19.9s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  16.6s\n",
      "[CV 2/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=10, dim_reducer=PCA(), dim_reducer__n_components=0.95, scaler=passthrough, transform__cv__max_features=500, transform__cv__min_df=0.05;, score=0.900 total time=  17.6s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  23.1s\n",
      "[CV 3/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=10, dim_reducer=PCA(), dim_reducer__n_components=0.95, scaler=passthrough, transform__cv__max_features=500, transform__cv__min_df=0.05;, score=0.902 total time=  24.1s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  35.5s\n",
      "[CV 4/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=10, dim_reducer=PCA(), dim_reducer__n_components=0.95, scaler=passthrough, transform__cv__max_features=500, transform__cv__min_df=0.05;, score=0.899 total time=  36.6s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  30.6s\n",
      "[CV 5/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=10, dim_reducer=PCA(), dim_reducer__n_components=0.95, scaler=passthrough, transform__cv__max_features=500, transform__cv__min_df=0.05;, score=0.901 total time=  31.6s\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.0s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.1s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   6.7s\n",
      "[CV 1/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=0.1, dim_reducer=passthrough, scaler=StandardScaler(with_mean=False), transform__cv__max_features=500, transform__cv__min_df=0.04;, score=0.903 total time=   8.1s\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.0s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.1s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   7.0s\n",
      "[CV 2/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=0.1, dim_reducer=passthrough, scaler=StandardScaler(with_mean=False), transform__cv__max_features=500, transform__cv__min_df=0.04;, score=0.900 total time=   8.4s\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.0s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.1s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   6.7s\n",
      "[CV 3/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=0.1, dim_reducer=passthrough, scaler=StandardScaler(with_mean=False), transform__cv__max_features=500, transform__cv__min_df=0.04;, score=0.902 total time=   8.1s\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.0s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.1s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   7.1s\n",
      "[CV 4/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=0.1, dim_reducer=passthrough, scaler=StandardScaler(with_mean=False), transform__cv__max_features=500, transform__cv__min_df=0.04;, score=0.900 total time=   8.5s\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.0s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.1s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   6.6s\n",
      "[CV 5/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=0.1, dim_reducer=passthrough, scaler=StandardScaler(with_mean=False), transform__cv__max_features=500, transform__cv__min_df=0.04;, score=0.901 total time=   8.1s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  19.3s\n",
      "[CV 1/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=100, dim_reducer=PCA(), dim_reducer__n_components=0.9, scaler=StandardScaler(with_mean=False), transform__cv__max_features=500, transform__cv__min_df=0.05;, score=0.904 total time=  20.6s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  17.6s\n",
      "[CV 2/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=100, dim_reducer=PCA(), dim_reducer__n_components=0.9, scaler=StandardScaler(with_mean=False), transform__cv__max_features=500, transform__cv__min_df=0.05;, score=0.901 total time=  18.9s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  17.4s\n",
      "[CV 3/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=100, dim_reducer=PCA(), dim_reducer__n_components=0.9, scaler=StandardScaler(with_mean=False), transform__cv__max_features=500, transform__cv__min_df=0.05;, score=0.902 total time=  18.7s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  19.6s\n",
      "[CV 4/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=100, dim_reducer=PCA(), dim_reducer__n_components=0.9, scaler=StandardScaler(with_mean=False), transform__cv__max_features=500, transform__cv__min_df=0.05;, score=0.900 total time=  20.9s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  18.5s\n",
      "[CV 5/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=100, dim_reducer=PCA(), dim_reducer__n_components=0.9, scaler=StandardScaler(with_mean=False), transform__cv__max_features=500, transform__cv__min_df=0.05;, score=0.902 total time=  19.8s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   6.6s\n",
      "[CV 1/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=0.1, dim_reducer=PCA(), dim_reducer__n_components=0.95, scaler=passthrough, transform__cv__max_features=500, transform__cv__min_df=0.05;, score=0.903 total time=   7.6s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   6.2s\n",
      "[CV 2/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=0.1, dim_reducer=PCA(), dim_reducer__n_components=0.95, scaler=passthrough, transform__cv__max_features=500, transform__cv__min_df=0.05;, score=0.900 total time=   7.2s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   7.1s\n",
      "[CV 3/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=0.1, dim_reducer=PCA(), dim_reducer__n_components=0.95, scaler=passthrough, transform__cv__max_features=500, transform__cv__min_df=0.05;, score=0.902 total time=   8.1s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   6.4s\n",
      "[CV 4/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=0.1, dim_reducer=PCA(), dim_reducer__n_components=0.95, scaler=passthrough, transform__cv__max_features=500, transform__cv__min_df=0.05;, score=0.900 total time=   7.4s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   7.1s\n",
      "[CV 5/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=0.1, dim_reducer=PCA(), dim_reducer__n_components=0.95, scaler=passthrough, transform__cv__max_features=500, transform__cv__min_df=0.05;, score=0.901 total time=   8.1s\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 12.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   0.5s\n",
      "[CV 1/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, penalty='l1', random_state=8,\n",
      "                   solver='liblinear'), clf__C=0.001, dim_reducer=passthrough, scaler=StandardScaler(), transform__cv__max_features=500, transform__cv__min_df=0.05;, score=0.893 total time=   1.5s\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 12.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   0.5s\n",
      "[CV 2/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, penalty='l1', random_state=8,\n",
      "                   solver='liblinear'), clf__C=0.001, dim_reducer=passthrough, scaler=StandardScaler(), transform__cv__max_features=500, transform__cv__min_df=0.05;, score=0.895 total time=   1.5s\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 12.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   0.6s\n",
      "[CV 3/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, penalty='l1', random_state=8,\n",
      "                   solver='liblinear'), clf__C=0.001, dim_reducer=passthrough, scaler=StandardScaler(), transform__cv__max_features=500, transform__cv__min_df=0.05;, score=0.896 total time=   1.6s\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 12.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   0.5s\n",
      "[CV 4/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, penalty='l1', random_state=8,\n",
      "                   solver='liblinear'), clf__C=0.001, dim_reducer=passthrough, scaler=StandardScaler(), transform__cv__max_features=500, transform__cv__min_df=0.05;, score=0.895 total time=   1.6s\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 12.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   0.6s\n",
      "[CV 5/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, penalty='l1', random_state=8,\n",
      "                   solver='liblinear'), clf__C=0.001, dim_reducer=passthrough, scaler=StandardScaler(), transform__cv__max_features=500, transform__cv__min_df=0.05;, score=0.896 total time=   1.6s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   6.2s\n",
      "[CV 1/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=0.1, dim_reducer=PCA(), dim_reducer__n_components=0.95, scaler=StandardScaler(with_mean=False), transform__cv__max_features=500, transform__cv__min_df=0.05;, score=0.903 total time=   7.3s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   7.3s\n",
      "[CV 2/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=0.1, dim_reducer=PCA(), dim_reducer__n_components=0.95, scaler=StandardScaler(with_mean=False), transform__cv__max_features=500, transform__cv__min_df=0.05;, score=0.900 total time=   8.4s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   6.6s\n",
      "[CV 3/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=0.1, dim_reducer=PCA(), dim_reducer__n_components=0.95, scaler=StandardScaler(with_mean=False), transform__cv__max_features=500, transform__cv__min_df=0.05;, score=0.902 total time=   7.7s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   7.7s\n",
      "[CV 4/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=0.1, dim_reducer=PCA(), dim_reducer__n_components=0.95, scaler=StandardScaler(with_mean=False), transform__cv__max_features=500, transform__cv__min_df=0.05;, score=0.900 total time=   8.8s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   7.3s\n",
      "[CV 5/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=0.1, dim_reducer=PCA(), dim_reducer__n_components=0.95, scaler=StandardScaler(with_mean=False), transform__cv__max_features=500, transform__cv__min_df=0.05;, score=0.901 total time=   8.4s\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.0s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 12.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ............... (step 4 of 4) Processing clf, total= 1.4min\n",
      "[CV 1/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, penalty='l1', random_state=8,\n",
      "                   solver='liblinear'), clf__C=100, dim_reducer=passthrough, scaler=StandardScaler(), transform__cv__max_features=500, transform__cv__min_df=0.03;, score=0.904 total time= 1.4min\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.0s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 12.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ............... (step 4 of 4) Processing clf, total= 1.4min\n",
      "[CV 2/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, penalty='l1', random_state=8,\n",
      "                   solver='liblinear'), clf__C=100, dim_reducer=passthrough, scaler=StandardScaler(), transform__cv__max_features=500, transform__cv__min_df=0.03;, score=0.900 total time= 1.4min\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.0s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 12.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ............... (step 4 of 4) Processing clf, total= 1.5min\n",
      "[CV 3/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, penalty='l1', random_state=8,\n",
      "                   solver='liblinear'), clf__C=100, dim_reducer=passthrough, scaler=StandardScaler(), transform__cv__max_features=500, transform__cv__min_df=0.03;, score=0.902 total time= 1.5min\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.0s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 12.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ............... (step 4 of 4) Processing clf, total= 1.5min\n",
      "[CV 4/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, penalty='l1', random_state=8,\n",
      "                   solver='liblinear'), clf__C=100, dim_reducer=passthrough, scaler=StandardScaler(), transform__cv__max_features=500, transform__cv__min_df=0.03;, score=0.900 total time= 1.5min\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.0s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 12.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ............... (step 4 of 4) Processing clf, total= 1.5min\n",
      "[CV 5/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, penalty='l1', random_state=8,\n",
      "                   solver='liblinear'), clf__C=100, dim_reducer=passthrough, scaler=StandardScaler(), transform__cv__max_features=500, transform__cv__min_df=0.03;, score=0.902 total time= 1.5min\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   7.5s\n",
      "[CV 1/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=1, dim_reducer=passthrough, scaler=StandardScaler(with_mean=False), transform__cv__max_features=500, transform__cv__min_df=0.04;, score=0.903 total time=   8.6s\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   9.7s\n",
      "[CV 2/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=1, dim_reducer=passthrough, scaler=StandardScaler(with_mean=False), transform__cv__max_features=500, transform__cv__min_df=0.04;, score=0.900 total time=  10.7s\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   8.3s\n",
      "[CV 3/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=1, dim_reducer=passthrough, scaler=StandardScaler(with_mean=False), transform__cv__max_features=500, transform__cv__min_df=0.04;, score=0.902 total time=   9.4s\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   9.1s\n",
      "[CV 4/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=1, dim_reducer=passthrough, scaler=StandardScaler(with_mean=False), transform__cv__max_features=500, transform__cv__min_df=0.04;, score=0.900 total time=  10.1s\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   8.0s\n",
      "[CV 5/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=1, dim_reducer=passthrough, scaler=StandardScaler(with_mean=False), transform__cv__max_features=500, transform__cv__min_df=0.04;, score=0.901 total time=   9.0s\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.0s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   4.1s\n",
      "[CV 1/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=0.001, dim_reducer=passthrough, scaler=passthrough, transform__cv__max_features=500, transform__cv__min_df=0.03;, score=0.905 total time=   5.0s\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.0s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   3.9s\n",
      "[CV 2/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=0.001, dim_reducer=passthrough, scaler=passthrough, transform__cv__max_features=500, transform__cv__min_df=0.03;, score=0.902 total time=   4.8s\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.0s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   4.0s\n",
      "[CV 3/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=0.001, dim_reducer=passthrough, scaler=passthrough, transform__cv__max_features=500, transform__cv__min_df=0.03;, score=0.903 total time=   4.9s\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.0s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   3.8s\n",
      "[CV 4/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=0.001, dim_reducer=passthrough, scaler=passthrough, transform__cv__max_features=500, transform__cv__min_df=0.03;, score=0.902 total time=   4.7s\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.0s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   4.1s\n",
      "[CV 5/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=0.001, dim_reducer=passthrough, scaler=passthrough, transform__cv__max_features=500, transform__cv__min_df=0.03;, score=0.903 total time=   5.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  30.0s\n",
      "[CV 1/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=1000, dim_reducer=PCA(), dim_reducer__n_components=0.8, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.04;, score=0.904 total time=  31.1s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  33.4s\n",
      "[CV 2/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=1000, dim_reducer=PCA(), dim_reducer__n_components=0.8, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.04;, score=0.901 total time=  34.5s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  40.1s\n",
      "[CV 3/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=1000, dim_reducer=PCA(), dim_reducer__n_components=0.8, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.04;, score=0.902 total time=  41.2s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  31.4s\n",
      "[CV 4/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=1000, dim_reducer=PCA(), dim_reducer__n_components=0.8, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.04;, score=0.901 total time=  32.5s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  30.7s\n",
      "[CV 5/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=1000, dim_reducer=PCA(), dim_reducer__n_components=0.8, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.04;, score=0.901 total time=  31.8s\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   9.4s\n",
      "[CV 1/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=0.1, dim_reducer=passthrough, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.05;, score=0.905 total time=  10.4s\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  10.7s\n",
      "[CV 2/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=0.1, dim_reducer=passthrough, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.05;, score=0.902 total time=  11.7s\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  10.3s\n",
      "[CV 3/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=0.1, dim_reducer=passthrough, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.05;, score=0.902 total time=  11.3s\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   9.4s\n",
      "[CV 4/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=0.1, dim_reducer=passthrough, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.05;, score=0.901 total time=  10.4s\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  10.4s\n",
      "[CV 5/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=0.1, dim_reducer=passthrough, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.05;, score=0.902 total time=  11.4s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   1.5s\n",
      "[CV 1/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=0.001, dim_reducer=PCA(), dim_reducer__n_components=0.85, scaler=passthrough, transform__cv__max_features=500, transform__cv__min_df=0.03;, score=0.904 total time=   2.5s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   1.4s\n",
      "[CV 2/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=0.001, dim_reducer=PCA(), dim_reducer__n_components=0.85, scaler=passthrough, transform__cv__max_features=500, transform__cv__min_df=0.03;, score=0.902 total time=   2.5s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   1.4s\n",
      "[CV 3/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=0.001, dim_reducer=PCA(), dim_reducer__n_components=0.85, scaler=passthrough, transform__cv__max_features=500, transform__cv__min_df=0.03;, score=0.903 total time=   2.5s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   1.4s\n",
      "[CV 4/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=0.001, dim_reducer=PCA(), dim_reducer__n_components=0.85, scaler=passthrough, transform__cv__max_features=500, transform__cv__min_df=0.03;, score=0.895 total time=   2.4s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   1.4s\n",
      "[CV 5/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=0.001, dim_reducer=PCA(), dim_reducer__n_components=0.85, scaler=passthrough, transform__cv__max_features=500, transform__cv__min_df=0.03;, score=0.903 total time=   2.5s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   9.8s\n",
      "[CV 1/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=10, dim_reducer=PCA(), dim_reducer__n_components=0.85, scaler=passthrough, transform__cv__max_features=500, transform__cv__min_df=0.03;, score=0.904 total time=  10.8s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  11.3s\n",
      "[CV 2/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=10, dim_reducer=PCA(), dim_reducer__n_components=0.85, scaler=passthrough, transform__cv__max_features=500, transform__cv__min_df=0.03;, score=0.901 total time=  12.4s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  11.7s\n",
      "[CV 3/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=10, dim_reducer=PCA(), dim_reducer__n_components=0.85, scaler=passthrough, transform__cv__max_features=500, transform__cv__min_df=0.03;, score=0.902 total time=  12.8s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  14.3s\n",
      "[CV 4/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=10, dim_reducer=PCA(), dim_reducer__n_components=0.85, scaler=passthrough, transform__cv__max_features=500, transform__cv__min_df=0.03;, score=0.901 total time=  15.3s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  10.5s\n",
      "[CV 5/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=10, dim_reducer=PCA(), dim_reducer__n_components=0.85, scaler=passthrough, transform__cv__max_features=500, transform__cv__min_df=0.03;, score=0.902 total time=  11.6s\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.0s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.1s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  59.6s\n",
      "[CV 1/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=10, dim_reducer=passthrough, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.03;, score=0.904 total time= 1.0min\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.0s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.1s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  54.2s\n",
      "[CV 2/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=10, dim_reducer=passthrough, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.03;, score=0.901 total time=  55.7s\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.0s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.1s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total= 1.1min\n",
      "[CV 3/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=10, dim_reducer=passthrough, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.03;, score=0.902 total time= 1.1min\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.0s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.1s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  55.7s\n",
      "[CV 4/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=10, dim_reducer=passthrough, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.03;, score=0.900 total time=  57.2s\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.0s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.1s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total= 1.2min\n",
      "[CV 5/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=10, dim_reducer=passthrough, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.03;, score=0.902 total time= 1.2min\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   9.9s\n",
      "[CV 1/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=0.1, dim_reducer=PCA(), dim_reducer__n_components=0.95, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.03;, score=0.905 total time=  11.2s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  10.5s\n",
      "[CV 2/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=0.1, dim_reducer=PCA(), dim_reducer__n_components=0.95, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.03;, score=0.903 total time=  11.7s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   9.0s\n",
      "[CV 3/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=0.1, dim_reducer=PCA(), dim_reducer__n_components=0.95, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.03;, score=0.904 total time=  10.1s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  10.4s\n",
      "[CV 4/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=0.1, dim_reducer=PCA(), dim_reducer__n_components=0.95, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.03;, score=0.902 total time=  11.6s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  13.1s\n",
      "[CV 5/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=0.1, dim_reducer=PCA(), dim_reducer__n_components=0.95, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.03;, score=0.903 total time=  14.3s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   5.7s\n",
      "[CV 1/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=1, dim_reducer=PCA(), dim_reducer__n_components=0.8, scaler=passthrough, transform__cv__max_features=500, transform__cv__min_df=0.03;, score=0.904 total time=   6.8s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   4.9s\n",
      "[CV 2/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=1, dim_reducer=PCA(), dim_reducer__n_components=0.8, scaler=passthrough, transform__cv__max_features=500, transform__cv__min_df=0.03;, score=0.901 total time=   5.9s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   4.3s\n",
      "[CV 3/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=1, dim_reducer=PCA(), dim_reducer__n_components=0.8, scaler=passthrough, transform__cv__max_features=500, transform__cv__min_df=0.03;, score=0.902 total time=   5.3s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   4.7s\n",
      "[CV 4/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=1, dim_reducer=PCA(), dim_reducer__n_components=0.8, scaler=passthrough, transform__cv__max_features=500, transform__cv__min_df=0.03;, score=0.901 total time=   5.7s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   5.0s\n",
      "[CV 5/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=1, dim_reducer=PCA(), dim_reducer__n_components=0.8, scaler=passthrough, transform__cv__max_features=500, transform__cv__min_df=0.03;, score=0.901 total time=   6.0s\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 12.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  15.8s\n",
      "[CV 1/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, penalty='l1', random_state=8,\n",
      "                   solver='liblinear'), clf__C=10, dim_reducer=passthrough, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.03;, score=0.904 total time=  16.9s\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 12.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  14.5s\n",
      "[CV 2/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, penalty='l1', random_state=8,\n",
      "                   solver='liblinear'), clf__C=10, dim_reducer=passthrough, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.03;, score=0.900 total time=  15.6s\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 12.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  11.5s\n",
      "[CV 3/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, penalty='l1', random_state=8,\n",
      "                   solver='liblinear'), clf__C=10, dim_reducer=passthrough, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.03;, score=0.902 total time=  12.5s\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 12.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  11.8s\n",
      "[CV 4/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, penalty='l1', random_state=8,\n",
      "                   solver='liblinear'), clf__C=10, dim_reducer=passthrough, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.03;, score=0.900 total time=  12.8s\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 12.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  15.7s\n",
      "[CV 5/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, penalty='l1', random_state=8,\n",
      "                   solver='liblinear'), clf__C=10, dim_reducer=passthrough, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.03;, score=0.902 total time=  16.7s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   5.4s\n",
      "[CV 1/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=0.1, dim_reducer=PCA(), dim_reducer__n_components=0.85, scaler=StandardScaler(with_mean=False), transform__cv__max_features=500, transform__cv__min_df=0.03;, score=0.904 total time=   6.7s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   5.2s\n",
      "[CV 2/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=0.1, dim_reducer=PCA(), dim_reducer__n_components=0.85, scaler=StandardScaler(with_mean=False), transform__cv__max_features=500, transform__cv__min_df=0.03;, score=0.901 total time=   6.5s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   5.8s\n",
      "[CV 3/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=0.1, dim_reducer=PCA(), dim_reducer__n_components=0.85, scaler=StandardScaler(with_mean=False), transform__cv__max_features=500, transform__cv__min_df=0.03;, score=0.902 total time=   7.1s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   5.6s\n",
      "[CV 4/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=0.1, dim_reducer=PCA(), dim_reducer__n_components=0.85, scaler=StandardScaler(with_mean=False), transform__cv__max_features=500, transform__cv__min_df=0.03;, score=0.901 total time=   6.9s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   5.4s\n",
      "[CV 5/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=0.1, dim_reducer=PCA(), dim_reducer__n_components=0.85, scaler=StandardScaler(with_mean=False), transform__cv__max_features=500, transform__cv__min_df=0.03;, score=0.902 total time=   6.8s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   7.3s\n",
      "[CV 1/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=0.1, dim_reducer=PCA(), dim_reducer__n_components=0.9, scaler=passthrough, transform__cv__max_features=500, transform__cv__min_df=0.04;, score=0.904 total time=   8.3s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   7.5s\n",
      "[CV 2/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=0.1, dim_reducer=PCA(), dim_reducer__n_components=0.9, scaler=passthrough, transform__cv__max_features=500, transform__cv__min_df=0.04;, score=0.901 total time=   8.5s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   7.2s\n",
      "[CV 3/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=0.1, dim_reducer=PCA(), dim_reducer__n_components=0.9, scaler=passthrough, transform__cv__max_features=500, transform__cv__min_df=0.04;, score=0.903 total time=   8.2s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   8.2s\n",
      "[CV 4/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=0.1, dim_reducer=PCA(), dim_reducer__n_components=0.9, scaler=passthrough, transform__cv__max_features=500, transform__cv__min_df=0.04;, score=0.900 total time=   9.2s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   9.3s\n",
      "[CV 5/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=0.1, dim_reducer=PCA(), dim_reducer__n_components=0.9, scaler=passthrough, transform__cv__max_features=500, transform__cv__min_df=0.04;, score=0.902 total time=  10.4s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  14.5s\n",
      "[CV 1/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=10, dim_reducer=PCA(), dim_reducer__n_components=0.9, scaler=StandardScaler(with_mean=False), transform__cv__max_features=500, transform__cv__min_df=0.04;, score=0.904 total time=  15.8s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.1s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  22.6s\n",
      "[CV 2/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=10, dim_reducer=PCA(), dim_reducer__n_components=0.9, scaler=StandardScaler(with_mean=False), transform__cv__max_features=500, transform__cv__min_df=0.04;, score=0.901 total time=  23.9s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  24.2s\n",
      "[CV 3/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=10, dim_reducer=PCA(), dim_reducer__n_components=0.9, scaler=StandardScaler(with_mean=False), transform__cv__max_features=500, transform__cv__min_df=0.04;, score=0.902 total time=  25.5s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  25.7s\n",
      "[CV 4/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=10, dim_reducer=PCA(), dim_reducer__n_components=0.9, scaler=StandardScaler(with_mean=False), transform__cv__max_features=500, transform__cv__min_df=0.04;, score=0.900 total time=  27.1s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  29.8s\n",
      "[CV 5/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=10, dim_reducer=PCA(), dim_reducer__n_components=0.9, scaler=StandardScaler(with_mean=False), transform__cv__max_features=500, transform__cv__min_df=0.04;, score=0.901 total time=  31.1s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   4.3s\n",
      "[CV 1/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=0.1, dim_reducer=PCA(), dim_reducer__n_components=0.85, scaler=StandardScaler(with_mean=False), transform__cv__max_features=500, transform__cv__min_df=0.05;, score=0.904 total time=   5.4s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   5.1s\n",
      "[CV 2/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=0.1, dim_reducer=PCA(), dim_reducer__n_components=0.85, scaler=StandardScaler(with_mean=False), transform__cv__max_features=500, transform__cv__min_df=0.05;, score=0.901 total time=   6.2s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   4.5s\n",
      "[CV 3/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=0.1, dim_reducer=PCA(), dim_reducer__n_components=0.85, scaler=StandardScaler(with_mean=False), transform__cv__max_features=500, transform__cv__min_df=0.05;, score=0.903 total time=   5.6s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   5.2s\n",
      "[CV 4/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=0.1, dim_reducer=PCA(), dim_reducer__n_components=0.85, scaler=StandardScaler(with_mean=False), transform__cv__max_features=500, transform__cv__min_df=0.05;, score=0.901 total time=   6.3s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   5.5s\n",
      "[CV 5/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=0.1, dim_reducer=PCA(), dim_reducer__n_components=0.85, scaler=StandardScaler(with_mean=False), transform__cv__max_features=500, transform__cv__min_df=0.05;, score=0.901 total time=   6.6s\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.0s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  11.5s\n",
      "[CV 1/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=100, dim_reducer=passthrough, scaler=passthrough, transform__cv__max_features=500, transform__cv__min_df=0.04;, score=0.903 total time=  12.4s\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.0s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  10.4s\n",
      "[CV 2/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=100, dim_reducer=passthrough, scaler=passthrough, transform__cv__max_features=500, transform__cv__min_df=0.04;, score=0.900 total time=  11.3s\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.0s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  11.2s\n",
      "[CV 3/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=100, dim_reducer=passthrough, scaler=passthrough, transform__cv__max_features=500, transform__cv__min_df=0.04;, score=0.902 total time=  12.1s\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.0s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  10.0s\n",
      "[CV 4/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=100, dim_reducer=passthrough, scaler=passthrough, transform__cv__max_features=500, transform__cv__min_df=0.04;, score=0.900 total time=  10.8s\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.0s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  10.2s\n",
      "[CV 5/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=100, dim_reducer=passthrough, scaler=passthrough, transform__cv__max_features=500, transform__cv__min_df=0.04;, score=0.901 total time=  11.1s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  15.2s\n",
      "[CV 1/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=1, dim_reducer=PCA(), dim_reducer__n_components=0.9, scaler=StandardScaler(with_mean=False), transform__cv__max_features=500, transform__cv__min_df=0.04;, score=0.904 total time=  16.3s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  16.0s\n",
      "[CV 2/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=1, dim_reducer=PCA(), dim_reducer__n_components=0.9, scaler=StandardScaler(with_mean=False), transform__cv__max_features=500, transform__cv__min_df=0.04;, score=0.901 total time=  17.1s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  13.0s\n",
      "[CV 3/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=1, dim_reducer=PCA(), dim_reducer__n_components=0.9, scaler=StandardScaler(with_mean=False), transform__cv__max_features=500, transform__cv__min_df=0.04;, score=0.902 total time=  14.1s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  14.7s\n",
      "[CV 4/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=1, dim_reducer=PCA(), dim_reducer__n_components=0.9, scaler=StandardScaler(with_mean=False), transform__cv__max_features=500, transform__cv__min_df=0.04;, score=0.900 total time=  15.8s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  16.5s\n",
      "[CV 5/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=1, dim_reducer=PCA(), dim_reducer__n_components=0.9, scaler=StandardScaler(with_mean=False), transform__cv__max_features=500, transform__cv__min_df=0.04;, score=0.902 total time=  17.6s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  19.1s\n",
      "[CV 1/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=100, dim_reducer=PCA(), dim_reducer__n_components=0.95, scaler=StandardScaler(with_mean=False), transform__cv__max_features=500, transform__cv__min_df=0.05;, score=0.903 total time=  20.2s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  15.1s\n",
      "[CV 2/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=100, dim_reducer=PCA(), dim_reducer__n_components=0.95, scaler=StandardScaler(with_mean=False), transform__cv__max_features=500, transform__cv__min_df=0.05;, score=0.900 total time=  16.2s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  15.8s\n",
      "[CV 3/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=100, dim_reducer=PCA(), dim_reducer__n_components=0.95, scaler=StandardScaler(with_mean=False), transform__cv__max_features=500, transform__cv__min_df=0.05;, score=0.902 total time=  16.9s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  45.7s\n",
      "[CV 4/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=100, dim_reducer=PCA(), dim_reducer__n_components=0.95, scaler=StandardScaler(with_mean=False), transform__cv__max_features=500, transform__cv__min_df=0.05;, score=0.899 total time=  46.8s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  16.2s\n",
      "[CV 5/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=100, dim_reducer=PCA(), dim_reducer__n_components=0.95, scaler=StandardScaler(with_mean=False), transform__cv__max_features=500, transform__cv__min_df=0.05;, score=0.901 total time=  17.3s\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 12.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ............... (step 4 of 4) Processing clf, total= 1.8min\n",
      "[CV 1/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, penalty='l1', random_state=8,\n",
      "                   solver='liblinear'), clf__C=10, dim_reducer=passthrough, scaler=StandardScaler(), transform__cv__max_features=500, transform__cv__min_df=0.03;, score=0.904 total time= 1.8min\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 12.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ............... (step 4 of 4) Processing clf, total= 1.8min\n",
      "[CV 2/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, penalty='l1', random_state=8,\n",
      "                   solver='liblinear'), clf__C=10, dim_reducer=passthrough, scaler=StandardScaler(), transform__cv__max_features=500, transform__cv__min_df=0.03;, score=0.900 total time= 1.8min\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 12.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ............... (step 4 of 4) Processing clf, total= 2.4min\n",
      "[CV 3/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, penalty='l1', random_state=8,\n",
      "                   solver='liblinear'), clf__C=10, dim_reducer=passthrough, scaler=StandardScaler(), transform__cv__max_features=500, transform__cv__min_df=0.03;, score=0.902 total time= 2.4min\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 12.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ............... (step 4 of 4) Processing clf, total= 1.1min\n",
      "[CV 4/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, penalty='l1', random_state=8,\n",
      "                   solver='liblinear'), clf__C=10, dim_reducer=passthrough, scaler=StandardScaler(), transform__cv__max_features=500, transform__cv__min_df=0.03;, score=0.900 total time= 1.1min\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 12.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ............... (step 4 of 4) Processing clf, total= 2.4min\n",
      "[CV 5/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, penalty='l1', random_state=8,\n",
      "                   solver='liblinear'), clf__C=10, dim_reducer=passthrough, scaler=StandardScaler(), transform__cv__max_features=500, transform__cv__min_df=0.03;, score=0.902 total time= 2.4min\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   1.3s\n",
      "[CV 1/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=0.001, dim_reducer=PCA(), dim_reducer__n_components=0.8, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.05;, score=0.889 total time=   2.6s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   1.0s\n",
      "[CV 2/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=0.001, dim_reducer=PCA(), dim_reducer__n_components=0.8, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.05;, score=0.889 total time=   2.3s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   0.9s\n",
      "[CV 3/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=0.001, dim_reducer=PCA(), dim_reducer__n_components=0.8, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.05;, score=0.889 total time=   2.2s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   1.0s\n",
      "[CV 4/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=0.001, dim_reducer=PCA(), dim_reducer__n_components=0.8, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.05;, score=0.889 total time=   2.3s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   0.9s\n",
      "[CV 5/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=0.001, dim_reducer=PCA(), dim_reducer__n_components=0.8, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.05;, score=0.889 total time=   2.2s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   1.4s\n",
      "[CV 1/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=0.001, dim_reducer=PCA(), dim_reducer__n_components=0.85, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.03;, score=0.889 total time=   2.5s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   1.5s\n",
      "[CV 2/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=0.001, dim_reducer=PCA(), dim_reducer__n_components=0.85, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.03;, score=0.889 total time=   2.6s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   1.4s\n",
      "[CV 3/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=0.001, dim_reducer=PCA(), dim_reducer__n_components=0.85, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.03;, score=0.889 total time=   2.6s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   1.3s\n",
      "[CV 4/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=0.001, dim_reducer=PCA(), dim_reducer__n_components=0.85, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.03;, score=0.889 total time=   2.5s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   1.4s\n",
      "[CV 5/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=0.001, dim_reducer=PCA(), dim_reducer__n_components=0.85, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.03;, score=0.889 total time=   2.6s\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  45.2s\n",
      "[CV 1/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=1000, dim_reducer=passthrough, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.04;, score=0.903 total time=  46.2s\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  50.4s\n",
      "[CV 2/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=1000, dim_reducer=passthrough, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.04;, score=0.900 total time=  51.5s\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  56.1s\n",
      "[CV 3/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=1000, dim_reducer=passthrough, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.04;, score=0.902 total time=  57.1s\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  55.2s\n",
      "[CV 4/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=1000, dim_reducer=passthrough, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.04;, score=0.900 total time=  56.3s\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  48.8s\n",
      "[CV 5/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=1000, dim_reducer=passthrough, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.04;, score=0.901 total time=  49.9s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  25.3s\n",
      "[CV 1/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=100, dim_reducer=PCA(), dim_reducer__n_components=0.9, scaler=StandardScaler(with_mean=False), transform__cv__max_features=500, transform__cv__min_df=0.03;, score=0.904 total time=  26.4s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  25.8s\n",
      "[CV 2/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=100, dim_reducer=PCA(), dim_reducer__n_components=0.9, scaler=StandardScaler(with_mean=False), transform__cv__max_features=500, transform__cv__min_df=0.03;, score=0.901 total time=  27.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  24.2s\n",
      "[CV 3/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=100, dim_reducer=PCA(), dim_reducer__n_components=0.9, scaler=StandardScaler(with_mean=False), transform__cv__max_features=500, transform__cv__min_df=0.03;, score=0.903 total time=  25.4s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  25.8s\n",
      "[CV 4/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=100, dim_reducer=PCA(), dim_reducer__n_components=0.9, scaler=StandardScaler(with_mean=False), transform__cv__max_features=500, transform__cv__min_df=0.03;, score=0.900 total time=  27.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  25.6s\n",
      "[CV 5/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=100, dim_reducer=PCA(), dim_reducer__n_components=0.9, scaler=StandardScaler(with_mean=False), transform__cv__max_features=500, transform__cv__min_df=0.03;, score=0.901 total time=  26.7s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  18.1s\n",
      "[CV 1/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=10, dim_reducer=PCA(), dim_reducer__n_components=0.9, scaler=StandardScaler(with_mean=False), transform__cv__max_features=500, transform__cv__min_df=0.05;, score=0.904 total time=  19.1s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  14.7s\n",
      "[CV 2/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=10, dim_reducer=PCA(), dim_reducer__n_components=0.9, scaler=StandardScaler(with_mean=False), transform__cv__max_features=500, transform__cv__min_df=0.05;, score=0.901 total time=  15.8s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  16.6s\n",
      "[CV 3/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=10, dim_reducer=PCA(), dim_reducer__n_components=0.9, scaler=StandardScaler(with_mean=False), transform__cv__max_features=500, transform__cv__min_df=0.05;, score=0.902 total time=  17.7s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  19.1s\n",
      "[CV 4/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=10, dim_reducer=PCA(), dim_reducer__n_components=0.9, scaler=StandardScaler(with_mean=False), transform__cv__max_features=500, transform__cv__min_df=0.05;, score=0.900 total time=  20.2s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  16.1s\n",
      "[CV 5/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=10, dim_reducer=PCA(), dim_reducer__n_components=0.9, scaler=StandardScaler(with_mean=False), transform__cv__max_features=500, transform__cv__min_df=0.05;, score=0.902 total time=  17.2s\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  51.1s\n",
      "[CV 1/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=10, dim_reducer=passthrough, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.04;, score=0.903 total time=  52.1s\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  55.5s\n",
      "[CV 2/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=10, dim_reducer=passthrough, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.04;, score=0.900 total time=  56.5s\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  40.9s\n",
      "[CV 3/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=10, dim_reducer=passthrough, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.04;, score=0.902 total time=  42.0s\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  48.9s\n",
      "[CV 4/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=10, dim_reducer=passthrough, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.04;, score=0.900 total time=  49.9s\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  57.5s\n",
      "[CV 5/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=10, dim_reducer=passthrough, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.04;, score=0.901 total time=  58.6s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  26.7s\n",
      "[CV 1/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=10, dim_reducer=PCA(), dim_reducer__n_components=0.85, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.03;, score=0.903 total time=  27.9s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  21.7s\n",
      "[CV 2/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=10, dim_reducer=PCA(), dim_reducer__n_components=0.85, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.03;, score=0.901 total time=  22.8s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  27.3s\n",
      "[CV 3/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=10, dim_reducer=PCA(), dim_reducer__n_components=0.85, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.03;, score=0.902 total time=  28.4s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  28.6s\n",
      "[CV 4/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=10, dim_reducer=PCA(), dim_reducer__n_components=0.85, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.03;, score=0.901 total time=  29.8s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  27.6s\n",
      "[CV 5/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=10, dim_reducer=PCA(), dim_reducer__n_components=0.85, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.03;, score=0.901 total time=  28.7s\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.0s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  17.9s\n",
      "[CV 1/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=1000, dim_reducer=passthrough, scaler=passthrough, transform__cv__max_features=500, transform__cv__min_df=0.03;, score=0.904 total time=  18.8s\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.0s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  16.2s\n",
      "[CV 2/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=1000, dim_reducer=passthrough, scaler=passthrough, transform__cv__max_features=500, transform__cv__min_df=0.03;, score=0.900 total time=  17.1s\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.0s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  16.5s\n",
      "[CV 3/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=1000, dim_reducer=passthrough, scaler=passthrough, transform__cv__max_features=500, transform__cv__min_df=0.03;, score=0.902 total time=  17.4s\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.0s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  17.2s\n",
      "[CV 4/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=1000, dim_reducer=passthrough, scaler=passthrough, transform__cv__max_features=500, transform__cv__min_df=0.03;, score=0.900 total time=  18.1s\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.0s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  14.7s\n",
      "[CV 5/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=1000, dim_reducer=passthrough, scaler=passthrough, transform__cv__max_features=500, transform__cv__min_df=0.03;, score=0.902 total time=  15.5s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  17.4s\n",
      "[CV 1/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=10, dim_reducer=PCA(), dim_reducer__n_components=0.9, scaler=passthrough, transform__cv__max_features=500, transform__cv__min_df=0.04;, score=0.904 total time=  18.4s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  24.5s\n",
      "[CV 2/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=10, dim_reducer=PCA(), dim_reducer__n_components=0.9, scaler=passthrough, transform__cv__max_features=500, transform__cv__min_df=0.04;, score=0.901 total time=  25.5s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  20.6s\n",
      "[CV 3/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=10, dim_reducer=PCA(), dim_reducer__n_components=0.9, scaler=passthrough, transform__cv__max_features=500, transform__cv__min_df=0.04;, score=0.902 total time=  21.7s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  31.5s\n",
      "[CV 4/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=10, dim_reducer=PCA(), dim_reducer__n_components=0.9, scaler=passthrough, transform__cv__max_features=500, transform__cv__min_df=0.04;, score=0.900 total time=  32.6s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  24.3s\n",
      "[CV 5/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=10, dim_reducer=PCA(), dim_reducer__n_components=0.9, scaler=passthrough, transform__cv__max_features=500, transform__cv__min_df=0.04;, score=0.901 total time=  25.3s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   1.2s\n",
      "[CV 1/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=0.001, dim_reducer=PCA(), dim_reducer__n_components=0.85, scaler=passthrough, transform__cv__max_features=500, transform__cv__min_df=0.04;, score=0.891 total time=   2.3s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   1.3s\n",
      "[CV 2/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=0.001, dim_reducer=PCA(), dim_reducer__n_components=0.85, scaler=passthrough, transform__cv__max_features=500, transform__cv__min_df=0.04;, score=0.894 total time=   2.3s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   1.3s\n",
      "[CV 3/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=0.001, dim_reducer=PCA(), dim_reducer__n_components=0.85, scaler=passthrough, transform__cv__max_features=500, transform__cv__min_df=0.04;, score=0.896 total time=   2.3s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   1.4s\n",
      "[CV 4/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=0.001, dim_reducer=PCA(), dim_reducer__n_components=0.85, scaler=passthrough, transform__cv__max_features=500, transform__cv__min_df=0.04;, score=0.891 total time=   2.4s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   1.4s\n",
      "[CV 5/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=0.001, dim_reducer=PCA(), dim_reducer__n_components=0.85, scaler=passthrough, transform__cv__max_features=500, transform__cv__min_df=0.04;, score=0.891 total time=   2.4s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  13.1s\n",
      "[CV 1/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=10, dim_reducer=PCA(), dim_reducer__n_components=0.85, scaler=passthrough, transform__cv__max_features=500, transform__cv__min_df=0.05;, score=0.904 total time=  14.1s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  10.5s\n",
      "[CV 2/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=10, dim_reducer=PCA(), dim_reducer__n_components=0.85, scaler=passthrough, transform__cv__max_features=500, transform__cv__min_df=0.05;, score=0.901 total time=  11.5s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  10.3s\n",
      "[CV 3/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=10, dim_reducer=PCA(), dim_reducer__n_components=0.85, scaler=passthrough, transform__cv__max_features=500, transform__cv__min_df=0.05;, score=0.903 total time=  11.3s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  14.4s\n",
      "[CV 4/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=10, dim_reducer=PCA(), dim_reducer__n_components=0.85, scaler=passthrough, transform__cv__max_features=500, transform__cv__min_df=0.05;, score=0.901 total time=  15.4s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  12.1s\n",
      "[CV 5/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=10, dim_reducer=PCA(), dim_reducer__n_components=0.85, scaler=passthrough, transform__cv__max_features=500, transform__cv__min_df=0.05;, score=0.902 total time=  13.1s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   1.7s\n",
      "[CV 1/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=0.001, dim_reducer=PCA(), dim_reducer__n_components=0.95, scaler=StandardScaler(with_mean=False), transform__cv__max_features=500, transform__cv__min_df=0.05;, score=0.905 total time=   2.8s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   1.5s\n",
      "[CV 2/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=0.001, dim_reducer=PCA(), dim_reducer__n_components=0.95, scaler=StandardScaler(with_mean=False), transform__cv__max_features=500, transform__cv__min_df=0.05;, score=0.902 total time=   2.6s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   1.6s\n",
      "[CV 3/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=0.001, dim_reducer=PCA(), dim_reducer__n_components=0.95, scaler=StandardScaler(with_mean=False), transform__cv__max_features=500, transform__cv__min_df=0.05;, score=0.903 total time=   2.7s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   1.6s\n",
      "[CV 4/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=0.001, dim_reducer=PCA(), dim_reducer__n_components=0.95, scaler=StandardScaler(with_mean=False), transform__cv__max_features=500, transform__cv__min_df=0.05;, score=0.901 total time=   2.7s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   1.6s\n",
      "[CV 5/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=0.001, dim_reducer=PCA(), dim_reducer__n_components=0.95, scaler=StandardScaler(with_mean=False), transform__cv__max_features=500, transform__cv__min_df=0.05;, score=0.902 total time=   2.7s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   5.3s\n",
      "[CV 1/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=1, dim_reducer=PCA(), dim_reducer__n_components=0.8, scaler=passthrough, transform__cv__max_features=500, transform__cv__min_df=0.04;, score=0.904 total time=   6.4s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   4.6s\n",
      "[CV 2/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=1, dim_reducer=PCA(), dim_reducer__n_components=0.8, scaler=passthrough, transform__cv__max_features=500, transform__cv__min_df=0.04;, score=0.901 total time=   5.7s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   4.3s\n",
      "[CV 3/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=1, dim_reducer=PCA(), dim_reducer__n_components=0.8, scaler=passthrough, transform__cv__max_features=500, transform__cv__min_df=0.04;, score=0.902 total time=   5.4s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   4.6s\n",
      "[CV 4/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=1, dim_reducer=PCA(), dim_reducer__n_components=0.8, scaler=passthrough, transform__cv__max_features=500, transform__cv__min_df=0.04;, score=0.901 total time=   5.6s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   5.0s\n",
      "[CV 5/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=1, dim_reducer=PCA(), dim_reducer__n_components=0.8, scaler=passthrough, transform__cv__max_features=500, transform__cv__min_df=0.04;, score=0.901 total time=   6.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  11.2s\n",
      "[CV 1/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=1, dim_reducer=PCA(), dim_reducer__n_components=0.8, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.03;, score=0.890 total time=  12.4s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   8.6s\n",
      "[CV 2/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=1, dim_reducer=PCA(), dim_reducer__n_components=0.8, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.03;, score=0.890 total time=   9.7s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  11.3s\n",
      "[CV 3/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=1, dim_reducer=PCA(), dim_reducer__n_components=0.8, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.03;, score=0.891 total time=  12.5s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  11.9s\n",
      "[CV 4/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=1, dim_reducer=PCA(), dim_reducer__n_components=0.8, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.03;, score=0.891 total time=  13.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   9.3s\n",
      "[CV 5/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=1, dim_reducer=PCA(), dim_reducer__n_components=0.8, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.03;, score=0.889 total time=  10.5s\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 12.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   0.5s\n",
      "[CV 1/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, penalty='l1', random_state=8,\n",
      "                   solver='liblinear'), clf__C=100, dim_reducer=passthrough, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.03;, score=0.904 total time=   1.5s\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 12.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   0.5s\n",
      "[CV 2/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, penalty='l1', random_state=8,\n",
      "                   solver='liblinear'), clf__C=100, dim_reducer=passthrough, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.03;, score=0.900 total time=   1.5s\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 12.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   0.5s\n",
      "[CV 3/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, penalty='l1', random_state=8,\n",
      "                   solver='liblinear'), clf__C=100, dim_reducer=passthrough, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.03;, score=0.902 total time=   1.5s\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 12.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   0.4s\n",
      "[CV 4/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, penalty='l1', random_state=8,\n",
      "                   solver='liblinear'), clf__C=100, dim_reducer=passthrough, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.03;, score=0.900 total time=   1.5s\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 12.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   0.3s\n",
      "[CV 5/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, penalty='l1', random_state=8,\n",
      "                   solver='liblinear'), clf__C=100, dim_reducer=passthrough, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.03;, score=0.902 total time=   1.4s\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 12.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ............... (step 4 of 4) Processing clf, total= 1.4min\n",
      "[CV 1/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, penalty='l1', random_state=8,\n",
      "                   solver='liblinear'), clf__C=1, dim_reducer=passthrough, scaler=StandardScaler(), transform__cv__max_features=500, transform__cv__min_df=0.03;, score=0.904 total time= 1.4min\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 12.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ............... (step 4 of 4) Processing clf, total= 1.4min\n",
      "[CV 2/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, penalty='l1', random_state=8,\n",
      "                   solver='liblinear'), clf__C=1, dim_reducer=passthrough, scaler=StandardScaler(), transform__cv__max_features=500, transform__cv__min_df=0.03;, score=0.900 total time= 1.4min\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 12.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ............... (step 4 of 4) Processing clf, total= 1.7min\n",
      "[CV 3/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, penalty='l1', random_state=8,\n",
      "                   solver='liblinear'), clf__C=1, dim_reducer=passthrough, scaler=StandardScaler(), transform__cv__max_features=500, transform__cv__min_df=0.03;, score=0.902 total time= 1.8min\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 12.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ............... (step 4 of 4) Processing clf, total= 1.4min\n",
      "[CV 4/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, penalty='l1', random_state=8,\n",
      "                   solver='liblinear'), clf__C=1, dim_reducer=passthrough, scaler=StandardScaler(), transform__cv__max_features=500, transform__cv__min_df=0.03;, score=0.900 total time= 1.4min\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 12.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ............... (step 4 of 4) Processing clf, total= 1.6min\n",
      "[CV 5/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, penalty='l1', random_state=8,\n",
      "                   solver='liblinear'), clf__C=1, dim_reducer=passthrough, scaler=StandardScaler(), transform__cv__max_features=500, transform__cv__min_df=0.03;, score=0.902 total time= 1.6min\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  58.7s\n",
      "[CV 1/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=100, dim_reducer=PCA(), dim_reducer__n_components=0.85, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.05;, score=0.904 total time=  60.0s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  33.2s\n",
      "[CV 2/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=100, dim_reducer=PCA(), dim_reducer__n_components=0.85, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.05;, score=0.901 total time=  34.4s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  29.7s\n",
      "[CV 3/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=100, dim_reducer=PCA(), dim_reducer__n_components=0.85, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.05;, score=0.903 total time=  30.9s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  32.7s\n",
      "[CV 4/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=100, dim_reducer=PCA(), dim_reducer__n_components=0.85, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.05;, score=0.901 total time=  33.9s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  34.9s\n",
      "[CV 5/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=100, dim_reducer=PCA(), dim_reducer__n_components=0.85, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.05;, score=0.902 total time=  36.1s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  32.6s\n",
      "[CV 1/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=100, dim_reducer=PCA(), dim_reducer__n_components=0.8, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.04;, score=0.904 total time=  33.7s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  25.0s\n",
      "[CV 2/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=100, dim_reducer=PCA(), dim_reducer__n_components=0.8, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.04;, score=0.901 total time=  26.1s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  41.1s\n",
      "[CV 3/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=100, dim_reducer=PCA(), dim_reducer__n_components=0.8, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.04;, score=0.902 total time=  42.2s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  26.2s\n",
      "[CV 4/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=100, dim_reducer=PCA(), dim_reducer__n_components=0.8, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.04;, score=0.901 total time=  27.3s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  34.0s\n",
      "[CV 5/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=100, dim_reducer=PCA(), dim_reducer__n_components=0.8, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.04;, score=0.901 total time=  35.2s\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.0s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.1s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   5.3s\n",
      "[CV 1/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=1000, dim_reducer=passthrough, scaler=StandardScaler(with_mean=False), transform__cv__max_features=500, transform__cv__min_df=0.05;, score=0.903 total time=   6.7s\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.0s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.1s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   5.7s\n",
      "[CV 2/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=1000, dim_reducer=passthrough, scaler=StandardScaler(with_mean=False), transform__cv__max_features=500, transform__cv__min_df=0.05;, score=0.899 total time=   7.1s\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.0s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.1s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   5.4s\n",
      "[CV 3/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=1000, dim_reducer=passthrough, scaler=StandardScaler(with_mean=False), transform__cv__max_features=500, transform__cv__min_df=0.05;, score=0.902 total time=   6.8s\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.0s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.1s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   5.2s\n",
      "[CV 4/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=1000, dim_reducer=passthrough, scaler=StandardScaler(with_mean=False), transform__cv__max_features=500, transform__cv__min_df=0.05;, score=0.899 total time=   6.5s\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.0s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.1s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   5.1s\n",
      "[CV 5/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=1000, dim_reducer=passthrough, scaler=StandardScaler(with_mean=False), transform__cv__max_features=500, transform__cv__min_df=0.05;, score=0.901 total time=   6.4s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   9.3s\n",
      "[CV 1/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=100, dim_reducer=PCA(), dim_reducer__n_components=0.85, scaler=StandardScaler(with_mean=False), transform__cv__max_features=500, transform__cv__min_df=0.03;, score=0.904 total time=  10.4s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  11.6s\n",
      "[CV 2/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=100, dim_reducer=PCA(), dim_reducer__n_components=0.85, scaler=StandardScaler(with_mean=False), transform__cv__max_features=500, transform__cv__min_df=0.03;, score=0.901 total time=  12.7s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  12.4s\n",
      "[CV 3/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=100, dim_reducer=PCA(), dim_reducer__n_components=0.85, scaler=StandardScaler(with_mean=False), transform__cv__max_features=500, transform__cv__min_df=0.03;, score=0.902 total time=  13.6s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  12.0s\n",
      "[CV 4/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=100, dim_reducer=PCA(), dim_reducer__n_components=0.85, scaler=StandardScaler(with_mean=False), transform__cv__max_features=500, transform__cv__min_df=0.03;, score=0.901 total time=  13.1s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   9.5s\n",
      "[CV 5/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=100, dim_reducer=PCA(), dim_reducer__n_components=0.85, scaler=StandardScaler(with_mean=False), transform__cv__max_features=500, transform__cv__min_df=0.03;, score=0.902 total time=  10.7s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   1.5s\n",
      "[CV 1/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=0.001, dim_reducer=PCA(), dim_reducer__n_components=0.9, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.05;, score=0.889 total time=   2.6s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   1.6s\n",
      "[CV 2/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=0.001, dim_reducer=PCA(), dim_reducer__n_components=0.9, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.05;, score=0.889 total time=   2.6s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   1.4s\n",
      "[CV 3/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=0.001, dim_reducer=PCA(), dim_reducer__n_components=0.9, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.05;, score=0.889 total time=   2.5s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   1.6s\n",
      "[CV 4/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=0.001, dim_reducer=PCA(), dim_reducer__n_components=0.9, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.05;, score=0.889 total time=   2.7s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   1.6s\n",
      "[CV 5/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=0.001, dim_reducer=PCA(), dim_reducer__n_components=0.9, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.05;, score=0.889 total time=   2.8s\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 12.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   0.4s\n",
      "[CV 1/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, penalty='l1', random_state=8,\n",
      "                   solver='liblinear'), clf__C=1000, dim_reducer=passthrough, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.03;, score=0.904 total time=   1.5s\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 12.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   0.4s\n",
      "[CV 2/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, penalty='l1', random_state=8,\n",
      "                   solver='liblinear'), clf__C=1000, dim_reducer=passthrough, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.03;, score=0.900 total time=   1.4s\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 12.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   0.4s\n",
      "[CV 3/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, penalty='l1', random_state=8,\n",
      "                   solver='liblinear'), clf__C=1000, dim_reducer=passthrough, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.03;, score=0.902 total time=   1.5s\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 12.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   0.4s\n",
      "[CV 4/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, penalty='l1', random_state=8,\n",
      "                   solver='liblinear'), clf__C=1000, dim_reducer=passthrough, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.03;, score=0.900 total time=   1.5s\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 12.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   0.4s\n",
      "[CV 5/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, penalty='l1', random_state=8,\n",
      "                   solver='liblinear'), clf__C=1000, dim_reducer=passthrough, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.03;, score=0.902 total time=   1.5s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.1s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total= 1.6min\n",
      "[CV 1/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=1000, dim_reducer=PCA(), dim_reducer__n_components=0.95, scaler=StandardScaler(with_mean=False), transform__cv__max_features=500, transform__cv__min_df=0.04;, score=0.902 total time= 1.6min\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.1s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total= 1.5min\n",
      "[CV 2/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=1000, dim_reducer=PCA(), dim_reducer__n_components=0.95, scaler=StandardScaler(with_mean=False), transform__cv__max_features=500, transform__cv__min_df=0.04;, score=0.900 total time= 1.5min\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.1s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total= 1.3min\n",
      "[CV 3/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=1000, dim_reducer=PCA(), dim_reducer__n_components=0.95, scaler=StandardScaler(with_mean=False), transform__cv__max_features=500, transform__cv__min_df=0.04;, score=0.902 total time= 1.3min\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.1s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total= 1.6min\n",
      "[CV 4/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=1000, dim_reducer=PCA(), dim_reducer__n_components=0.95, scaler=StandardScaler(with_mean=False), transform__cv__max_features=500, transform__cv__min_df=0.04;, score=0.899 total time= 1.6min\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.1s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total= 2.1min\n",
      "[CV 5/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=1000, dim_reducer=PCA(), dim_reducer__n_components=0.95, scaler=StandardScaler(with_mean=False), transform__cv__max_features=500, transform__cv__min_df=0.04;, score=0.901 total time= 2.1min\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total= 2.1min\n",
      "[CV 1/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=1000, dim_reducer=PCA(), dim_reducer__n_components=0.9, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.05;, score=0.904 total time= 2.1min\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total= 1.7min\n",
      "[CV 2/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=1000, dim_reducer=PCA(), dim_reducer__n_components=0.9, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.05;, score=0.901 total time= 1.7min\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total= 2.1min\n",
      "[CV 3/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=1000, dim_reducer=PCA(), dim_reducer__n_components=0.9, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.05;, score=0.902 total time= 2.1min\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total= 1.9min\n",
      "[CV 4/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=1000, dim_reducer=PCA(), dim_reducer__n_components=0.9, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.05;, score=0.901 total time= 1.9min\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total= 1.9min\n",
      "[CV 5/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=1000, dim_reducer=PCA(), dim_reducer__n_components=0.9, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.05;, score=0.902 total time= 1.9min\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   8.4s\n",
      "[CV 1/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=0.1, dim_reducer=passthrough, scaler=StandardScaler(with_mean=False), transform__cv__max_features=500, transform__cv__min_df=0.03;, score=0.904 total time=   9.5s\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   7.6s\n",
      "[CV 2/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=0.1, dim_reducer=passthrough, scaler=StandardScaler(with_mean=False), transform__cv__max_features=500, transform__cv__min_df=0.03;, score=0.901 total time=   8.7s\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   9.3s\n",
      "[CV 3/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=0.1, dim_reducer=passthrough, scaler=StandardScaler(with_mean=False), transform__cv__max_features=500, transform__cv__min_df=0.03;, score=0.902 total time=  10.4s\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   7.6s\n",
      "[CV 4/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=0.1, dim_reducer=passthrough, scaler=StandardScaler(with_mean=False), transform__cv__max_features=500, transform__cv__min_df=0.03;, score=0.900 total time=   8.7s\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   8.1s\n",
      "[CV 5/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=0.1, dim_reducer=passthrough, scaler=StandardScaler(with_mean=False), transform__cv__max_features=500, transform__cv__min_df=0.03;, score=0.902 total time=   9.2s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   5.8s\n",
      "[CV 1/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=0.1, dim_reducer=PCA(), dim_reducer__n_components=0.95, scaler=StandardScaler(with_mean=False), transform__cv__max_features=500, transform__cv__min_df=0.04;, score=0.902 total time=   6.9s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   5.2s\n",
      "[CV 2/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=0.1, dim_reducer=PCA(), dim_reducer__n_components=0.95, scaler=StandardScaler(with_mean=False), transform__cv__max_features=500, transform__cv__min_df=0.04;, score=0.900 total time=   6.3s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   6.0s\n",
      "[CV 3/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=0.1, dim_reducer=PCA(), dim_reducer__n_components=0.95, scaler=StandardScaler(with_mean=False), transform__cv__max_features=500, transform__cv__min_df=0.04;, score=0.901 total time=   7.1s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   4.9s\n",
      "[CV 4/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=0.1, dim_reducer=PCA(), dim_reducer__n_components=0.95, scaler=StandardScaler(with_mean=False), transform__cv__max_features=500, transform__cv__min_df=0.04;, score=0.900 total time=   6.1s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   5.4s\n",
      "[CV 5/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=0.1, dim_reducer=PCA(), dim_reducer__n_components=0.95, scaler=StandardScaler(with_mean=False), transform__cv__max_features=500, transform__cv__min_df=0.04;, score=0.901 total time=   6.5s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  15.0s\n",
      "[CV 1/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=1000, dim_reducer=PCA(), dim_reducer__n_components=0.9, scaler=StandardScaler(with_mean=False), transform__cv__max_features=500, transform__cv__min_df=0.04;, score=0.904 total time=  16.1s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  26.3s\n",
      "[CV 2/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=1000, dim_reducer=PCA(), dim_reducer__n_components=0.9, scaler=StandardScaler(with_mean=False), transform__cv__max_features=500, transform__cv__min_df=0.04;, score=0.901 total time=  27.4s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  28.5s\n",
      "[CV 3/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=1000, dim_reducer=PCA(), dim_reducer__n_components=0.9, scaler=StandardScaler(with_mean=False), transform__cv__max_features=500, transform__cv__min_df=0.04;, score=0.902 total time=  29.6s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  21.9s\n",
      "[CV 4/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=1000, dim_reducer=PCA(), dim_reducer__n_components=0.9, scaler=StandardScaler(with_mean=False), transform__cv__max_features=500, transform__cv__min_df=0.04;, score=0.900 total time=  23.1s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  26.0s\n",
      "[CV 5/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=1000, dim_reducer=PCA(), dim_reducer__n_components=0.9, scaler=StandardScaler(with_mean=False), transform__cv__max_features=500, transform__cv__min_df=0.04;, score=0.901 total time=  27.1s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  11.4s\n",
      "[CV 1/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=10, dim_reducer=PCA(), dim_reducer__n_components=0.85, scaler=StandardScaler(with_mean=False), transform__cv__max_features=500, transform__cv__min_df=0.05;, score=0.904 total time=  12.5s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   9.9s\n",
      "[CV 2/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=10, dim_reducer=PCA(), dim_reducer__n_components=0.85, scaler=StandardScaler(with_mean=False), transform__cv__max_features=500, transform__cv__min_df=0.05;, score=0.901 total time=  11.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  10.2s\n",
      "[CV 3/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=10, dim_reducer=PCA(), dim_reducer__n_components=0.85, scaler=StandardScaler(with_mean=False), transform__cv__max_features=500, transform__cv__min_df=0.05;, score=0.903 total time=  11.2s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  12.8s\n",
      "[CV 4/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=10, dim_reducer=PCA(), dim_reducer__n_components=0.85, scaler=StandardScaler(with_mean=False), transform__cv__max_features=500, transform__cv__min_df=0.05;, score=0.901 total time=  13.8s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  10.8s\n",
      "[CV 5/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=10, dim_reducer=PCA(), dim_reducer__n_components=0.85, scaler=StandardScaler(with_mean=False), transform__cv__max_features=500, transform__cv__min_df=0.05;, score=0.902 total time=  11.9s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   2.2s\n",
      "[CV 1/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=0.001, dim_reducer=PCA(), dim_reducer__n_components=0.95, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.05;, score=0.889 total time=   3.6s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   2.1s\n",
      "[CV 2/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=0.001, dim_reducer=PCA(), dim_reducer__n_components=0.95, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.05;, score=0.889 total time=   3.4s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   2.2s\n",
      "[CV 3/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=0.001, dim_reducer=PCA(), dim_reducer__n_components=0.95, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.05;, score=0.889 total time=   3.5s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   2.2s\n",
      "[CV 4/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=0.001, dim_reducer=PCA(), dim_reducer__n_components=0.95, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.05;, score=0.889 total time=   3.6s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   1.8s\n",
      "[CV 5/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=0.001, dim_reducer=PCA(), dim_reducer__n_components=0.95, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.05;, score=0.889 total time=   3.1s\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  23.0s\n",
      "[CV 1/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=1, dim_reducer=passthrough, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.05;, score=0.903 total time=  24.0s\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  25.1s\n",
      "[CV 2/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=1, dim_reducer=passthrough, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.05;, score=0.900 total time=  26.1s\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  23.7s\n",
      "[CV 3/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=1, dim_reducer=passthrough, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.05;, score=0.903 total time=  24.7s\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  23.6s\n",
      "[CV 4/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=1, dim_reducer=passthrough, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.05;, score=0.900 total time=  24.6s\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  23.9s\n",
      "[CV 5/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=1, dim_reducer=passthrough, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.05;, score=0.902 total time=  25.0s\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 12.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ............... (step 4 of 4) Processing clf, total= 1.4min\n",
      "[CV 1/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, penalty='l1', random_state=8,\n",
      "                   solver='liblinear'), clf__C=1000, dim_reducer=passthrough, scaler=StandardScaler(), transform__cv__max_features=500, transform__cv__min_df=0.03;, score=0.904 total time= 1.4min\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 12.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ............... (step 4 of 4) Processing clf, total= 1.4min\n",
      "[CV 2/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, penalty='l1', random_state=8,\n",
      "                   solver='liblinear'), clf__C=1000, dim_reducer=passthrough, scaler=StandardScaler(), transform__cv__max_features=500, transform__cv__min_df=0.03;, score=0.900 total time= 1.4min\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 12.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ............... (step 4 of 4) Processing clf, total= 1.5min\n",
      "[CV 3/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, penalty='l1', random_state=8,\n",
      "                   solver='liblinear'), clf__C=1000, dim_reducer=passthrough, scaler=StandardScaler(), transform__cv__max_features=500, transform__cv__min_df=0.03;, score=0.902 total time= 1.5min\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 12.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ............... (step 4 of 4) Processing clf, total= 1.5min\n",
      "[CV 4/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, penalty='l1', random_state=8,\n",
      "                   solver='liblinear'), clf__C=1000, dim_reducer=passthrough, scaler=StandardScaler(), transform__cv__max_features=500, transform__cv__min_df=0.03;, score=0.900 total time= 1.5min\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 12.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ............... (step 4 of 4) Processing clf, total= 1.5min\n",
      "[CV 5/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, penalty='l1', random_state=8,\n",
      "                   solver='liblinear'), clf__C=1000, dim_reducer=passthrough, scaler=StandardScaler(), transform__cv__max_features=500, transform__cv__min_df=0.03;, score=0.902 total time= 1.5min\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   3.4s\n",
      "[CV 1/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=0.001, dim_reducer=passthrough, scaler=StandardScaler(with_mean=False), transform__cv__max_features=500, transform__cv__min_df=0.04;, score=0.905 total time=   4.4s\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   3.0s\n",
      "[CV 2/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=0.001, dim_reducer=passthrough, scaler=StandardScaler(with_mean=False), transform__cv__max_features=500, transform__cv__min_df=0.04;, score=0.902 total time=   4.1s\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   3.3s\n",
      "[CV 3/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=0.001, dim_reducer=passthrough, scaler=StandardScaler(with_mean=False), transform__cv__max_features=500, transform__cv__min_df=0.04;, score=0.903 total time=   4.3s\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   3.1s\n",
      "[CV 4/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=0.001, dim_reducer=passthrough, scaler=StandardScaler(with_mean=False), transform__cv__max_features=500, transform__cv__min_df=0.04;, score=0.901 total time=   4.2s\n",
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   3.2s\n",
      "[CV 5/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=0.001, dim_reducer=passthrough, scaler=StandardScaler(with_mean=False), transform__cv__max_features=500, transform__cv__min_df=0.04;, score=0.903 total time=   4.2s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total= 1.9min\n",
      "[CV 1/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=1000, dim_reducer=PCA(), dim_reducer__n_components=0.95, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.05;, score=0.903 total time= 1.9min\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total= 1.8min\n",
      "[CV 2/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=1000, dim_reducer=PCA(), dim_reducer__n_components=0.95, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.05;, score=0.901 total time= 1.8min\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total= 1.4min\n",
      "[CV 3/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=1000, dim_reducer=PCA(), dim_reducer__n_components=0.95, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.05;, score=0.902 total time= 1.4min\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total= 1.6min\n",
      "[CV 4/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=1000, dim_reducer=PCA(), dim_reducer__n_components=0.95, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.05;, score=0.899 total time= 1.6min\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total= 2.5min\n",
      "[CV 5/5] END clf=LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8), clf__C=1000, dim_reducer=PCA(), dim_reducer__n_components=0.95, scaler=MinMaxScaler(), transform__cv__max_features=500, transform__cv__min_df=0.05;, score=0.901 total time= 2.5min\n",
      "[Pipeline] ......... (step 1 of 4) Processing transform, total=   2.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/sklearn/pipeline.py:359: UserWarning: Persisting input arguments took 0.55s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.9s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.1s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=   9.5s\n"
     ]
    }
   ],
   "source": [
    "#logistic grid search set up \n",
    "log_gs = RandomizedSearchCV(estimator = pipe, \n",
    "                           param_distributions= log_param_grid, \n",
    "                           cv=5, \n",
    "                           n_jobs = 1,\n",
    "                           n_iter = 100,\n",
    "                           verbose = 3,\n",
    "                           refit=True,\n",
    "                           scoring = 'f1_micro', \n",
    "                           random_state = 8)\n",
    "\n",
    "fitted_log_gs = log_gs.fit(X_remainder1, y_remainder1)\n",
    "\n",
    "grid_search_results = fitted_log_gs.cv_results_\n",
    "\n",
    "best_estimator = fitted_log_gs.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3becdbb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_results_to_s3(cv_results, filename, bucket, key):\n",
    "    \"\"\"\n",
    "    Write a DataFrame of grid search results to a CSV file and upload it to an S3 bucket.\n",
    "\n",
    "    Args:\n",
    "    - cv_results: A DataFrame of grid search results.\n",
    "    - filename: The name of the CSV file to write the results to.\n",
    "    - bucket: The name of the S3 bucket to upload the file to.\n",
    "    - key: The S3 key to use for the uploaded file.\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    # Write the results to a CSV file\n",
    "    grid_search_results.to_csv(filename, index=False)\n",
    "\n",
    "    # Upload the file to S3\n",
    "    s3_client = boto3.client('s3')\n",
    "    s3_client.upload_file(Filename=filename, Bucket=bucket, Key=key)\n",
    "\n",
    "     print(f\"{filename} has been saved to {bucket}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jJEiTzVpYAoh",
   "metadata": {
    "id": "jJEiTzVpYAoh"
   },
   "outputs": [],
   "source": [
    "# Example usage\n",
    "write_results_to_s3(grid_search_results, 'log_results_model_1.csv', 'deliverable-slo-bstn-bucket', 'log_results_model_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f1e064",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calling the file\n",
    "log_results_model_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5642f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import boto3\n",
    "\n",
    "def save_best_model(best_estimator, filename, bucket='deliverable-slo-bstn-bucket', key=None):\n",
    "    \"\"\"\n",
    "    Save the best estimator from a grid search to a joblib file and upload it to an S3 bucket (optional).\n",
    "\n",
    "    Args:\n",
    "    - best_estimator: The best estimator from a scikit-learn grid search.\n",
    "    - filename: The name of the file to save the best estimator to.\n",
    "    - bucket (optional): The name of the S3 bucket to upload the file to (if provided).\n",
    "    - key (optional): The S3 key to use for the uploaded file (if provided).\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    joblib.dump(best_estimator, filename)\n",
    "\n",
    "    if bucket is not None and key is not None:\n",
    "        s3_client = boto3.client('s3')\n",
    "        s3_client.upload_file(Filename=filename, Bucket=bucket, Key=key)\n",
    "\n",
    "    print(f\"{filename} has been saved to {bucket}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qtbu3g2fEwUG",
   "metadata": {
    "id": "qtbu3g2fEwUG"
   },
   "outputs": [],
   "source": [
    "# saving logreg pickle file\n",
    "save_best_model(best_estimator, 'pickled_best_log_reg1.pkl', bucket='deliverable-slo-bstn-bucket', key='pickled_best_log_reg1.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e03501f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9e03501f",
    "outputId": "fbb52000-f02d-4f15-c499-b65d7be6f8bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best logistic regression's accuracy on the remainder set: 0.903509221574075\n",
      "The best logistic regression's accuracy on the test set: 0.9040784595144121\n"
     ]
    }
   ],
   "source": [
    "# Sanity check on the pickled model\n",
    "pickled_best_logreg = joblib.load(filename)\n",
    "\n",
    "# Print the accuracies\n",
    "print(f\"The best logistic regression's accuracy on the remainder set: {pickled_best_logreg.score(X_remainder1, y_remainder1)}\")\n",
    "print(f\"The best logistic regression's accuracy on the test set: {pickled_best_logreg.score(X_test1, y_test1)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e51a82",
   "metadata": {
    "id": "96e51a82"
   },
   "source": [
    "#### Support Vector Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08e255f",
   "metadata": {
    "id": "c08e255f"
   },
   "source": [
    "Best results with standard scaler so far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "fd45abb2",
   "metadata": {
    "id": "fd45abb2"
   },
   "outputs": [],
   "source": [
    "#hyper parameters SVM\n",
    "param_range =[0.1,1,10]\n",
    "gamma_range = [0.1,0.01]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "c455c0ab",
   "metadata": {
    "id": "c455c0ab"
   },
   "outputs": [],
   "source": [
    "# Parameter grid\n",
    "svm_param_grid = [\n",
    "    # SVM  without PCA\n",
    "    {'transform__cv__min_df' : vectorize_mindf_list ,\n",
    "     'transform__cv__max_features': [500],\n",
    "     'scaler' : ['passthrough', StandardScaler(with_mean=False)],\n",
    "     'dim_reducer': ['passthrough'],\n",
    "     'clf' : [SVC(random_state=10)],\n",
    "     'clf__kernel': ['rbf','sigmoid'],\n",
    "     'clf__C': param_range,\n",
    "     'clf__gamma':gamma_range\n",
    "     },\n",
    "     \n",
    "     #SVM with PCA\n",
    "     {'transform__cv__min_df' : vectorize_mindf_list ,\n",
    "     'transform__cv__max_features': [500],\n",
    "     'scaler' : [StandardScaler(with_mean=False)],\n",
    "     'dim_reducer': [PCA()],\n",
    "     'dim_reducer__n_components':[0.95 ,0.9 ,0.8],\n",
    "      'clf' : [SVC(random_state=10)], \n",
    "     'clf__kernel': ['rbf','sigmoid'],\n",
    "     'clf__C': param_range,\n",
    "     'clf__gamma':gamma_range\n",
    "     }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "XQXGrHYAWuN5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XQXGrHYAWuN5",
    "outputId": "59336935-1513-413a-f36a-19de195ae614"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n",
      "[Pipeline] ......... (step 1 of 4) Processing transform, total=   1.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mac/opt/anaconda3/envs/capstone/lib/python3.11/site-packages/sklearn/pipeline.py:359: UserWarning: Persisting input arguments took 1.13s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.0s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  43.0s\n",
      "[CV 1/5] END clf=SVC(random_state=10), clf__C=1, clf__gamma=0.1, clf__kernel=rbf, dim_reducer=passthrough, scaler=passthrough, transform__cv__max_features=500, transform__cv__min_df=0.05;, score=0.922 total time= 1.0min\n",
      "[Pipeline] ......... (step 1 of 4) Processing transform, total=   0.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mac/opt/anaconda3/envs/capstone/lib/python3.11/site-packages/sklearn/pipeline.py:359: UserWarning: Persisting input arguments took 0.69s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.0s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  46.2s\n",
      "[CV 2/5] END clf=SVC(random_state=10), clf__C=1, clf__gamma=0.1, clf__kernel=rbf, dim_reducer=passthrough, scaler=passthrough, transform__cv__max_features=500, transform__cv__min_df=0.05;, score=0.924 total time=  59.8s\n",
      "[Pipeline] ......... (step 1 of 4) Processing transform, total=   0.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mac/opt/anaconda3/envs/capstone/lib/python3.11/site-packages/sklearn/pipeline.py:359: UserWarning: Persisting input arguments took 0.71s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ....... (step 2 of 4) Processing dim_reducer, total=   0.0s\n",
      "[Pipeline] ............ (step 3 of 4) Processing scaler, total=   0.0s\n",
      "[Pipeline] ............... (step 4 of 4) Processing clf, total=  40.4s\n",
      "[CV 3/5] END clf=SVC(random_state=10), clf__C=1, clf__gamma=0.1, clf__kernel=rbf, dim_reducer=passthrough, scaler=passthrough, transform__cv__max_features=500, transform__cv__min_df=0.05;, score=0.921 total time=  52.0s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[128], line 11\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m#activating randomized grid search \u001b[39;00m\n\u001b[1;32m      2\u001b[0m svm_random \u001b[39m=\u001b[39m RandomizedSearchCV(estimator \u001b[39m=\u001b[39m pipe, \n\u001b[1;32m      3\u001b[0m                            param_distributions\u001b[39m=\u001b[39m svm_param_grid, \n\u001b[1;32m      4\u001b[0m                            cv\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      8\u001b[0m                            scoring \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mf1_micro\u001b[39m\u001b[39m'\u001b[39m, \n\u001b[1;32m      9\u001b[0m                            random_state \u001b[39m=\u001b[39m \u001b[39m8\u001b[39m)\n\u001b[0;32m---> 11\u001b[0m fitted_svmrandom_gs \u001b[39m=\u001b[39m svm_random\u001b[39m.\u001b[39mfit(X_remainder1_sample, y_remainder1_sample)\n\u001b[1;32m     13\u001b[0m grid_search_results \u001b[39m=\u001b[39m fitted_svmrandom_gs\u001b[39m.\u001b[39mcv_results_\n\u001b[1;32m     15\u001b[0m best_estimator \u001b[39m=\u001b[39m fitted_svmrandom_gs\u001b[39m.\u001b[39mbest_estimator_\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/capstone/lib/python3.11/site-packages/sklearn/model_selection/_search.py:874\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    868\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_results(\n\u001b[1;32m    869\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m    870\u001b[0m     )\n\u001b[1;32m    872\u001b[0m     \u001b[39mreturn\u001b[39;00m results\n\u001b[0;32m--> 874\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run_search(evaluate_candidates)\n\u001b[1;32m    876\u001b[0m \u001b[39m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m    877\u001b[0m \u001b[39m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m    878\u001b[0m first_test_score \u001b[39m=\u001b[39m all_out[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mtest_scores\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/capstone/lib/python3.11/site-packages/sklearn/model_selection/_search.py:1768\u001b[0m, in \u001b[0;36mRandomizedSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1766\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_run_search\u001b[39m(\u001b[39mself\u001b[39m, evaluate_candidates):\n\u001b[1;32m   1767\u001b[0m     \u001b[39m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1768\u001b[0m     evaluate_candidates(\n\u001b[1;32m   1769\u001b[0m         ParameterSampler(\n\u001b[1;32m   1770\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparam_distributions, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_iter, random_state\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrandom_state\n\u001b[1;32m   1771\u001b[0m         )\n\u001b[1;32m   1772\u001b[0m     )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/capstone/lib/python3.11/site-packages/sklearn/model_selection/_search.py:821\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    813\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    814\u001b[0m     \u001b[39mprint\u001b[39m(\n\u001b[1;32m    815\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFitting \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m folds for each of \u001b[39m\u001b[39m{1}\u001b[39;00m\u001b[39m candidates,\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    816\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m totalling \u001b[39m\u001b[39m{2}\u001b[39;00m\u001b[39m fits\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m    817\u001b[0m             n_splits, n_candidates, n_candidates \u001b[39m*\u001b[39m n_splits\n\u001b[1;32m    818\u001b[0m         )\n\u001b[1;32m    819\u001b[0m     )\n\u001b[0;32m--> 821\u001b[0m out \u001b[39m=\u001b[39m parallel(\n\u001b[1;32m    822\u001b[0m     delayed(_fit_and_score)(\n\u001b[1;32m    823\u001b[0m         clone(base_estimator),\n\u001b[1;32m    824\u001b[0m         X,\n\u001b[1;32m    825\u001b[0m         y,\n\u001b[1;32m    826\u001b[0m         train\u001b[39m=\u001b[39mtrain,\n\u001b[1;32m    827\u001b[0m         test\u001b[39m=\u001b[39mtest,\n\u001b[1;32m    828\u001b[0m         parameters\u001b[39m=\u001b[39mparameters,\n\u001b[1;32m    829\u001b[0m         split_progress\u001b[39m=\u001b[39m(split_idx, n_splits),\n\u001b[1;32m    830\u001b[0m         candidate_progress\u001b[39m=\u001b[39m(cand_idx, n_candidates),\n\u001b[1;32m    831\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_and_score_kwargs,\n\u001b[1;32m    832\u001b[0m     )\n\u001b[1;32m    833\u001b[0m     \u001b[39mfor\u001b[39;00m (cand_idx, parameters), (split_idx, (train, test)) \u001b[39min\u001b[39;00m product(\n\u001b[1;32m    834\u001b[0m         \u001b[39menumerate\u001b[39m(candidate_params), \u001b[39menumerate\u001b[39m(cv\u001b[39m.\u001b[39msplit(X, y, groups))\n\u001b[1;32m    835\u001b[0m     )\n\u001b[1;32m    836\u001b[0m )\n\u001b[1;32m    838\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(out) \u001b[39m<\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    839\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    840\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mNo fits were performed. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    841\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWas the CV iterator empty? \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    842\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWere there no candidates?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    843\u001b[0m     )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/capstone/lib/python3.11/site-packages/sklearn/utils/parallel.py:63\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     58\u001b[0m config \u001b[39m=\u001b[39m get_config()\n\u001b[1;32m     59\u001b[0m iterable_with_config \u001b[39m=\u001b[39m (\n\u001b[1;32m     60\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     61\u001b[0m     \u001b[39mfor\u001b[39;00m delayed_func, args, kwargs \u001b[39min\u001b[39;00m iterable\n\u001b[1;32m     62\u001b[0m )\n\u001b[0;32m---> 63\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m(iterable_with_config)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/capstone/lib/python3.11/site-packages/joblib/parallel.py:1088\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1085\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[1;32m   1086\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_original_iterator \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 1088\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[1;32m   1089\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[1;32m   1091\u001b[0m \u001b[39mif\u001b[39;00m pre_dispatch \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mall\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mor\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   1092\u001b[0m     \u001b[39m# The iterable was consumed all at once by the above for loop.\u001b[39;00m\n\u001b[1;32m   1093\u001b[0m     \u001b[39m# No need to wait for async callbacks to trigger to\u001b[39;00m\n\u001b[1;32m   1094\u001b[0m     \u001b[39m# consumption.\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/capstone/lib/python3.11/site-packages/joblib/parallel.py:901\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    899\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    900\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 901\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dispatch(tasks)\n\u001b[1;32m    902\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/capstone/lib/python3.11/site-packages/joblib/parallel.py:819\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m    818\u001b[0m     job_idx \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs)\n\u001b[0;32m--> 819\u001b[0m     job \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend\u001b[39m.\u001b[39mapply_async(batch, callback\u001b[39m=\u001b[39mcb)\n\u001b[1;32m    820\u001b[0m     \u001b[39m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[1;32m    821\u001b[0m     \u001b[39m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[1;32m    822\u001b[0m     \u001b[39m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[1;32m    823\u001b[0m     \u001b[39m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[1;32m    824\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs\u001b[39m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/capstone/lib/python3.11/site-packages/joblib/_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_async\u001b[39m(\u001b[39mself\u001b[39m, func, callback\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    207\u001b[0m     \u001b[39m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[0;32m--> 208\u001b[0m     result \u001b[39m=\u001b[39m ImmediateResult(func)\n\u001b[1;32m    209\u001b[0m     \u001b[39mif\u001b[39;00m callback:\n\u001b[1;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/capstone/lib/python3.11/site-packages/joblib/_parallel_backends.py:597\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    594\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, batch):\n\u001b[1;32m    595\u001b[0m     \u001b[39m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[1;32m    596\u001b[0m     \u001b[39m# arguments in memory\u001b[39;00m\n\u001b[0;32m--> 597\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresults \u001b[39m=\u001b[39m batch()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/capstone/lib/python3.11/site-packages/joblib/parallel.py:288\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    285\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    286\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    287\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 288\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    289\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/capstone/lib/python3.11/site-packages/joblib/parallel.py:288\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    285\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    286\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    287\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 288\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    289\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/capstone/lib/python3.11/site-packages/sklearn/utils/parallel.py:123\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m     config \u001b[39m=\u001b[39m {}\n\u001b[1;32m    122\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconfig):\n\u001b[0;32m--> 123\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunction(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/capstone/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:686\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[1;32m    684\u001b[0m         estimator\u001b[39m.\u001b[39mfit(X_train, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n\u001b[1;32m    685\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 686\u001b[0m         estimator\u001b[39m.\u001b[39mfit(X_train, y_train, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n\u001b[1;32m    688\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[1;32m    689\u001b[0m     \u001b[39m# Note fit time as time until error\u001b[39;00m\n\u001b[1;32m    690\u001b[0m     fit_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m start_time\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/capstone/lib/python3.11/site-packages/sklearn/pipeline.py:401\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[39m\"\"\"Fit the model.\u001b[39;00m\n\u001b[1;32m    376\u001b[0m \n\u001b[1;32m    377\u001b[0m \u001b[39mFit all the transformers one after the other and transform the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    398\u001b[0m \u001b[39m    Pipeline with fitted steps.\u001b[39;00m\n\u001b[1;32m    399\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    400\u001b[0m fit_params_steps \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_fit_params(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n\u001b[0;32m--> 401\u001b[0m Xt \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fit(X, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params_steps)\n\u001b[1;32m    402\u001b[0m \u001b[39mwith\u001b[39;00m _print_elapsed_time(\u001b[39m\"\u001b[39m\u001b[39mPipeline\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_log_message(\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps) \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m)):\n\u001b[1;32m    403\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_final_estimator \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpassthrough\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/capstone/lib/python3.11/site-packages/sklearn/pipeline.py:359\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[0;34m(self, X, y, **fit_params_steps)\u001b[0m\n\u001b[1;32m    357\u001b[0m     cloned_transformer \u001b[39m=\u001b[39m clone(transformer)\n\u001b[1;32m    358\u001b[0m \u001b[39m# Fit or load from cache the current transformer\u001b[39;00m\n\u001b[0;32m--> 359\u001b[0m X, fitted_transformer \u001b[39m=\u001b[39m fit_transform_one_cached(\n\u001b[1;32m    360\u001b[0m     cloned_transformer,\n\u001b[1;32m    361\u001b[0m     X,\n\u001b[1;32m    362\u001b[0m     y,\n\u001b[1;32m    363\u001b[0m     \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    364\u001b[0m     message_clsname\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mPipeline\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    365\u001b[0m     message\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_log_message(step_idx),\n\u001b[1;32m    366\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params_steps[name],\n\u001b[1;32m    367\u001b[0m )\n\u001b[1;32m    368\u001b[0m \u001b[39m# Replace the transformer of the step with the fitted\u001b[39;00m\n\u001b[1;32m    369\u001b[0m \u001b[39m# transformer. This is necessary when loading the transformer\u001b[39;00m\n\u001b[1;32m    370\u001b[0m \u001b[39m# from the cache.\u001b[39;00m\n\u001b[1;32m    371\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps[step_idx] \u001b[39m=\u001b[39m (name, fitted_transformer)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/capstone/lib/python3.11/site-packages/joblib/memory.py:594\u001b[0m, in \u001b[0;36mMemorizedFunc.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    593\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 594\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cached_call(args, kwargs)[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/capstone/lib/python3.11/site-packages/joblib/memory.py:486\u001b[0m, in \u001b[0;36mMemorizedFunc._cached_call\u001b[0;34m(self, args, kwargs, shelving)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_cached_call\u001b[39m(\u001b[39mself\u001b[39m, args, kwargs, shelving\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m    459\u001b[0m     \u001b[39m\"\"\"Call wrapped function and cache result, or read cache if available.\u001b[39;00m\n\u001b[1;32m    460\u001b[0m \n\u001b[1;32m    461\u001b[0m \u001b[39m    This function returns the wrapped function output and some metadata.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    484\u001b[0m \u001b[39m        Some metadata about wrapped function call (see _persist_input()).\u001b[39;00m\n\u001b[1;32m    485\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 486\u001b[0m     func_id, args_id \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_output_identifiers(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    487\u001b[0m     metadata \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    488\u001b[0m     msg \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/capstone/lib/python3.11/site-packages/joblib/memory.py:638\u001b[0m, in \u001b[0;36mMemorizedFunc._get_output_identifiers\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[39m\"\"\"Return the func identifier and input parameter hash of a result.\"\"\"\u001b[39;00m\n\u001b[1;32m    637\u001b[0m func_id \u001b[39m=\u001b[39m _build_func_identifier(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunc)\n\u001b[0;32m--> 638\u001b[0m argument_hash \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_argument_hash(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    639\u001b[0m \u001b[39mreturn\u001b[39;00m func_id, argument_hash\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/capstone/lib/python3.11/site-packages/joblib/memory.py:632\u001b[0m, in \u001b[0;36mMemorizedFunc._get_argument_hash\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_argument_hash\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 632\u001b[0m     \u001b[39mreturn\u001b[39;00m hashing\u001b[39m.\u001b[39mhash(filter_args(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunc, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mignore, args, kwargs),\n\u001b[1;32m    633\u001b[0m                         coerce_mmap\u001b[39m=\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmmap_mode \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m))\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/capstone/lib/python3.11/site-packages/joblib/hashing.py:266\u001b[0m, in \u001b[0;36mhash\u001b[0;34m(obj, hash_name, coerce_mmap)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    265\u001b[0m     hasher \u001b[39m=\u001b[39m Hasher(hash_name\u001b[39m=\u001b[39mhash_name)\n\u001b[0;32m--> 266\u001b[0m \u001b[39mreturn\u001b[39;00m hasher\u001b[39m.\u001b[39mhash(obj)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/capstone/lib/python3.11/site-packages/joblib/hashing.py:63\u001b[0m, in \u001b[0;36mHasher.hash\u001b[0;34m(self, obj, return_digest)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mhash\u001b[39m(\u001b[39mself\u001b[39m, obj, return_digest\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m     62\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 63\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdump(obj)\n\u001b[1;32m     64\u001b[0m     \u001b[39mexcept\u001b[39;00m pickle\u001b[39m.\u001b[39mPicklingError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     65\u001b[0m         e\u001b[39m.\u001b[39margs \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (\u001b[39m'\u001b[39m\u001b[39mPicklingError while hashing \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m (obj, e),)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/capstone/lib/python3.11/pickle.py:487\u001b[0m, in \u001b[0;36m_Pickler.dump\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    485\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mproto \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m4\u001b[39m:\n\u001b[1;32m    486\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mframer\u001b[39m.\u001b[39mstart_framing()\n\u001b[0;32m--> 487\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msave(obj)\n\u001b[1;32m    488\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwrite(STOP)\n\u001b[1;32m    489\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mframer\u001b[39m.\u001b[39mend_framing()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/capstone/lib/python3.11/site-packages/joblib/hashing.py:241\u001b[0m, in \u001b[0;36mNumpyHasher.save\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    239\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_hash\u001b[39m.\u001b[39mupdate(pickle\u001b[39m.\u001b[39mdumps(obj))\n\u001b[1;32m    240\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m--> 241\u001b[0m Hasher\u001b[39m.\u001b[39msave(\u001b[39mself\u001b[39m, obj)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/capstone/lib/python3.11/site-packages/joblib/hashing.py:89\u001b[0m, in \u001b[0;36mHasher.save\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[39mcls\u001b[39m \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39m\u001b[39m__self__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\n\u001b[1;32m     88\u001b[0m         obj \u001b[39m=\u001b[39m _MyHash(func_name, inst, \u001b[39mcls\u001b[39m)\n\u001b[0;32m---> 89\u001b[0m Pickler\u001b[39m.\u001b[39msave(\u001b[39mself\u001b[39m, obj)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/capstone/lib/python3.11/pickle.py:560\u001b[0m, in \u001b[0;36m_Pickler.save\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    558\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch\u001b[39m.\u001b[39mget(t)\n\u001b[1;32m    559\u001b[0m \u001b[39mif\u001b[39;00m f \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 560\u001b[0m     f(\u001b[39mself\u001b[39m, obj)  \u001b[39m# Call unbound method with explicit self\u001b[39;00m\n\u001b[1;32m    561\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m    563\u001b[0m \u001b[39m# Check private dispatch table if any, or else\u001b[39;00m\n\u001b[1;32m    564\u001b[0m \u001b[39m# copyreg.dispatch_table\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/capstone/lib/python3.11/pickle.py:972\u001b[0m, in \u001b[0;36m_Pickler.save_dict\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    969\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwrite(MARK \u001b[39m+\u001b[39m DICT)\n\u001b[1;32m    971\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmemoize(obj)\n\u001b[0;32m--> 972\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_batch_setitems(obj\u001b[39m.\u001b[39mitems())\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/capstone/lib/python3.11/site-packages/joblib/hashing.py:140\u001b[0m, in \u001b[0;36mHasher._batch_setitems\u001b[0;34m(self, items)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_batch_setitems\u001b[39m(\u001b[39mself\u001b[39m, items):\n\u001b[1;32m    134\u001b[0m     \u001b[39m# forces order of keys in dict to ensure consistent hash.\u001b[39;00m\n\u001b[1;32m    135\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    136\u001b[0m         \u001b[39m# Trying first to compare dict assuming the type of keys is\u001b[39;00m\n\u001b[1;32m    137\u001b[0m         \u001b[39m# consistent and orderable.\u001b[39;00m\n\u001b[1;32m    138\u001b[0m         \u001b[39m# This fails on python 3 when keys are unorderable\u001b[39;00m\n\u001b[1;32m    139\u001b[0m         \u001b[39m# but we keep it in a try as it's faster.\u001b[39;00m\n\u001b[0;32m--> 140\u001b[0m         Pickler\u001b[39m.\u001b[39m_batch_setitems(\u001b[39mself\u001b[39m, \u001b[39miter\u001b[39m(\u001b[39msorted\u001b[39m(items)))\n\u001b[1;32m    141\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m    142\u001b[0m         \u001b[39m# If keys are unorderable, sorting them using their hash. This is\u001b[39;00m\n\u001b[1;32m    143\u001b[0m         \u001b[39m# slower but works in any case.\u001b[39;00m\n\u001b[1;32m    144\u001b[0m         Pickler\u001b[39m.\u001b[39m_batch_setitems(\u001b[39mself\u001b[39m, \u001b[39miter\u001b[39m(\u001b[39msorted\u001b[39m((\u001b[39mhash\u001b[39m(k), v)\n\u001b[1;32m    145\u001b[0m                                                   \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m items)))\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/capstone/lib/python3.11/pickle.py:998\u001b[0m, in \u001b[0;36m_Pickler._batch_setitems\u001b[0;34m(self, items)\u001b[0m\n\u001b[1;32m    996\u001b[0m     \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m tmp:\n\u001b[1;32m    997\u001b[0m         save(k)\n\u001b[0;32m--> 998\u001b[0m         save(v)\n\u001b[1;32m    999\u001b[0m     write(SETITEMS)\n\u001b[1;32m   1000\u001b[0m \u001b[39melif\u001b[39;00m n:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/capstone/lib/python3.11/site-packages/joblib/hashing.py:241\u001b[0m, in \u001b[0;36mNumpyHasher.save\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    239\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_hash\u001b[39m.\u001b[39mupdate(pickle\u001b[39m.\u001b[39mdumps(obj))\n\u001b[1;32m    240\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m--> 241\u001b[0m Hasher\u001b[39m.\u001b[39msave(\u001b[39mself\u001b[39m, obj)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/capstone/lib/python3.11/site-packages/joblib/hashing.py:89\u001b[0m, in \u001b[0;36mHasher.save\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[39mcls\u001b[39m \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39m\u001b[39m__self__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\n\u001b[1;32m     88\u001b[0m         obj \u001b[39m=\u001b[39m _MyHash(func_name, inst, \u001b[39mcls\u001b[39m)\n\u001b[0;32m---> 89\u001b[0m Pickler\u001b[39m.\u001b[39msave(\u001b[39mself\u001b[39m, obj)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/capstone/lib/python3.11/pickle.py:603\u001b[0m, in \u001b[0;36m_Pickler.save\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    599\u001b[0m     \u001b[39mraise\u001b[39;00m PicklingError(\u001b[39m\"\u001b[39m\u001b[39mTuple returned by \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m must have \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    600\u001b[0m                         \u001b[39m\"\u001b[39m\u001b[39mtwo to six elements\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m reduce)\n\u001b[1;32m    602\u001b[0m \u001b[39m# Save the reduce() output and finally memoize the object\u001b[39;00m\n\u001b[0;32m--> 603\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msave_reduce(obj\u001b[39m=\u001b[39mobj, \u001b[39m*\u001b[39mrv)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/capstone/lib/python3.11/pickle.py:717\u001b[0m, in \u001b[0;36m_Pickler.save_reduce\u001b[0;34m(self, func, args, state, listitems, dictitems, state_setter, obj)\u001b[0m\n\u001b[1;32m    715\u001b[0m \u001b[39mif\u001b[39;00m state \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    716\u001b[0m     \u001b[39mif\u001b[39;00m state_setter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 717\u001b[0m         save(state)\n\u001b[1;32m    718\u001b[0m         write(BUILD)\n\u001b[1;32m    719\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    720\u001b[0m         \u001b[39m# If a state_setter is specified, call it instead of load_build\u001b[39;00m\n\u001b[1;32m    721\u001b[0m         \u001b[39m# to update obj's with its previous state.\u001b[39;00m\n\u001b[1;32m    722\u001b[0m         \u001b[39m# First, push state_setter and its tuple of expected arguments\u001b[39;00m\n\u001b[1;32m    723\u001b[0m         \u001b[39m# (obj, state) onto the stack.\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/capstone/lib/python3.11/site-packages/joblib/hashing.py:241\u001b[0m, in \u001b[0;36mNumpyHasher.save\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    239\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_hash\u001b[39m.\u001b[39mupdate(pickle\u001b[39m.\u001b[39mdumps(obj))\n\u001b[1;32m    240\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m--> 241\u001b[0m Hasher\u001b[39m.\u001b[39msave(\u001b[39mself\u001b[39m, obj)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/capstone/lib/python3.11/site-packages/joblib/hashing.py:89\u001b[0m, in \u001b[0;36mHasher.save\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[39mcls\u001b[39m \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39m\u001b[39m__self__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\n\u001b[1;32m     88\u001b[0m         obj \u001b[39m=\u001b[39m _MyHash(func_name, inst, \u001b[39mcls\u001b[39m)\n\u001b[0;32m---> 89\u001b[0m Pickler\u001b[39m.\u001b[39msave(\u001b[39mself\u001b[39m, obj)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/capstone/lib/python3.11/pickle.py:560\u001b[0m, in \u001b[0;36m_Pickler.save\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    558\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch\u001b[39m.\u001b[39mget(t)\n\u001b[1;32m    559\u001b[0m \u001b[39mif\u001b[39;00m f \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 560\u001b[0m     f(\u001b[39mself\u001b[39m, obj)  \u001b[39m# Call unbound method with explicit self\u001b[39;00m\n\u001b[1;32m    561\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m    563\u001b[0m \u001b[39m# Check private dispatch table if any, or else\u001b[39;00m\n\u001b[1;32m    564\u001b[0m \u001b[39m# copyreg.dispatch_table\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/capstone/lib/python3.11/pickle.py:972\u001b[0m, in \u001b[0;36m_Pickler.save_dict\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    969\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwrite(MARK \u001b[39m+\u001b[39m DICT)\n\u001b[1;32m    971\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmemoize(obj)\n\u001b[0;32m--> 972\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_batch_setitems(obj\u001b[39m.\u001b[39mitems())\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/capstone/lib/python3.11/site-packages/joblib/hashing.py:140\u001b[0m, in \u001b[0;36mHasher._batch_setitems\u001b[0;34m(self, items)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_batch_setitems\u001b[39m(\u001b[39mself\u001b[39m, items):\n\u001b[1;32m    134\u001b[0m     \u001b[39m# forces order of keys in dict to ensure consistent hash.\u001b[39;00m\n\u001b[1;32m    135\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    136\u001b[0m         \u001b[39m# Trying first to compare dict assuming the type of keys is\u001b[39;00m\n\u001b[1;32m    137\u001b[0m         \u001b[39m# consistent and orderable.\u001b[39;00m\n\u001b[1;32m    138\u001b[0m         \u001b[39m# This fails on python 3 when keys are unorderable\u001b[39;00m\n\u001b[1;32m    139\u001b[0m         \u001b[39m# but we keep it in a try as it's faster.\u001b[39;00m\n\u001b[0;32m--> 140\u001b[0m         Pickler\u001b[39m.\u001b[39m_batch_setitems(\u001b[39mself\u001b[39m, \u001b[39miter\u001b[39m(\u001b[39msorted\u001b[39m(items)))\n\u001b[1;32m    141\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m    142\u001b[0m         \u001b[39m# If keys are unorderable, sorting them using their hash. This is\u001b[39;00m\n\u001b[1;32m    143\u001b[0m         \u001b[39m# slower but works in any case.\u001b[39;00m\n\u001b[1;32m    144\u001b[0m         Pickler\u001b[39m.\u001b[39m_batch_setitems(\u001b[39mself\u001b[39m, \u001b[39miter\u001b[39m(\u001b[39msorted\u001b[39m((\u001b[39mhash\u001b[39m(k), v)\n\u001b[1;32m    145\u001b[0m                                                   \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m items)))\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/capstone/lib/python3.11/pickle.py:998\u001b[0m, in \u001b[0;36m_Pickler._batch_setitems\u001b[0;34m(self, items)\u001b[0m\n\u001b[1;32m    996\u001b[0m     \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m tmp:\n\u001b[1;32m    997\u001b[0m         save(k)\n\u001b[0;32m--> 998\u001b[0m         save(v)\n\u001b[1;32m    999\u001b[0m     write(SETITEMS)\n\u001b[1;32m   1000\u001b[0m \u001b[39melif\u001b[39;00m n:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/capstone/lib/python3.11/site-packages/joblib/hashing.py:241\u001b[0m, in \u001b[0;36mNumpyHasher.save\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    239\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_hash\u001b[39m.\u001b[39mupdate(pickle\u001b[39m.\u001b[39mdumps(obj))\n\u001b[1;32m    240\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m--> 241\u001b[0m Hasher\u001b[39m.\u001b[39msave(\u001b[39mself\u001b[39m, obj)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/capstone/lib/python3.11/site-packages/joblib/hashing.py:89\u001b[0m, in \u001b[0;36mHasher.save\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[39mcls\u001b[39m \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39m\u001b[39m__self__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\n\u001b[1;32m     88\u001b[0m         obj \u001b[39m=\u001b[39m _MyHash(func_name, inst, \u001b[39mcls\u001b[39m)\n\u001b[0;32m---> 89\u001b[0m Pickler\u001b[39m.\u001b[39msave(\u001b[39mself\u001b[39m, obj)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/capstone/lib/python3.11/pickle.py:603\u001b[0m, in \u001b[0;36m_Pickler.save\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    599\u001b[0m     \u001b[39mraise\u001b[39;00m PicklingError(\u001b[39m\"\u001b[39m\u001b[39mTuple returned by \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m must have \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    600\u001b[0m                         \u001b[39m\"\u001b[39m\u001b[39mtwo to six elements\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m reduce)\n\u001b[1;32m    602\u001b[0m \u001b[39m# Save the reduce() output and finally memoize the object\u001b[39;00m\n\u001b[0;32m--> 603\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msave_reduce(obj\u001b[39m=\u001b[39mobj, \u001b[39m*\u001b[39mrv)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/capstone/lib/python3.11/pickle.py:692\u001b[0m, in \u001b[0;36m_Pickler.save_reduce\u001b[0;34m(self, func, args, state, listitems, dictitems, state_setter, obj)\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    691\u001b[0m     save(func)\n\u001b[0;32m--> 692\u001b[0m     save(args)\n\u001b[1;32m    693\u001b[0m     write(REDUCE)\n\u001b[1;32m    695\u001b[0m \u001b[39mif\u001b[39;00m obj \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    696\u001b[0m     \u001b[39m# If the object is already in the memo, this means it is\u001b[39;00m\n\u001b[1;32m    697\u001b[0m     \u001b[39m# recursive. In this case, throw away everything we put on the\u001b[39;00m\n\u001b[1;32m    698\u001b[0m     \u001b[39m# stack, and fetch the object back from the memo.\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/capstone/lib/python3.11/site-packages/joblib/hashing.py:241\u001b[0m, in \u001b[0;36mNumpyHasher.save\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    239\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_hash\u001b[39m.\u001b[39mupdate(pickle\u001b[39m.\u001b[39mdumps(obj))\n\u001b[1;32m    240\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m--> 241\u001b[0m Hasher\u001b[39m.\u001b[39msave(\u001b[39mself\u001b[39m, obj)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/capstone/lib/python3.11/site-packages/joblib/hashing.py:89\u001b[0m, in \u001b[0;36mHasher.save\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[39mcls\u001b[39m \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39m\u001b[39m__self__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\n\u001b[1;32m     88\u001b[0m         obj \u001b[39m=\u001b[39m _MyHash(func_name, inst, \u001b[39mcls\u001b[39m)\n\u001b[0;32m---> 89\u001b[0m Pickler\u001b[39m.\u001b[39msave(\u001b[39mself\u001b[39m, obj)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/capstone/lib/python3.11/pickle.py:560\u001b[0m, in \u001b[0;36m_Pickler.save\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    558\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch\u001b[39m.\u001b[39mget(t)\n\u001b[1;32m    559\u001b[0m \u001b[39mif\u001b[39;00m f \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 560\u001b[0m     f(\u001b[39mself\u001b[39m, obj)  \u001b[39m# Call unbound method with explicit self\u001b[39;00m\n\u001b[1;32m    561\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m    563\u001b[0m \u001b[39m# Check private dispatch table if any, or else\u001b[39;00m\n\u001b[1;32m    564\u001b[0m \u001b[39m# copyreg.dispatch_table\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/capstone/lib/python3.11/pickle.py:887\u001b[0m, in \u001b[0;36m_Pickler.save_tuple\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    885\u001b[0m \u001b[39mif\u001b[39;00m n \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m3\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mproto \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[1;32m    886\u001b[0m     \u001b[39mfor\u001b[39;00m element \u001b[39min\u001b[39;00m obj:\n\u001b[0;32m--> 887\u001b[0m         save(element)\n\u001b[1;32m    888\u001b[0m     \u001b[39m# Subtle.  Same as in the big comment below.\u001b[39;00m\n\u001b[1;32m    889\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mid\u001b[39m(obj) \u001b[39min\u001b[39;00m memo:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/capstone/lib/python3.11/site-packages/joblib/hashing.py:241\u001b[0m, in \u001b[0;36mNumpyHasher.save\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    239\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_hash\u001b[39m.\u001b[39mupdate(pickle\u001b[39m.\u001b[39mdumps(obj))\n\u001b[1;32m    240\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m--> 241\u001b[0m Hasher\u001b[39m.\u001b[39msave(\u001b[39mself\u001b[39m, obj)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/capstone/lib/python3.11/site-packages/joblib/hashing.py:89\u001b[0m, in \u001b[0;36mHasher.save\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[39mcls\u001b[39m \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39m\u001b[39m__self__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\n\u001b[1;32m     88\u001b[0m         obj \u001b[39m=\u001b[39m _MyHash(func_name, inst, \u001b[39mcls\u001b[39m)\n\u001b[0;32m---> 89\u001b[0m Pickler\u001b[39m.\u001b[39msave(\u001b[39mself\u001b[39m, obj)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/capstone/lib/python3.11/pickle.py:560\u001b[0m, in \u001b[0;36m_Pickler.save\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    558\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch\u001b[39m.\u001b[39mget(t)\n\u001b[1;32m    559\u001b[0m \u001b[39mif\u001b[39;00m f \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 560\u001b[0m     f(\u001b[39mself\u001b[39m, obj)  \u001b[39m# Call unbound method with explicit self\u001b[39;00m\n\u001b[1;32m    561\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m    563\u001b[0m \u001b[39m# Check private dispatch table if any, or else\u001b[39;00m\n\u001b[1;32m    564\u001b[0m \u001b[39m# copyreg.dispatch_table\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/capstone/lib/python3.11/pickle.py:887\u001b[0m, in \u001b[0;36m_Pickler.save_tuple\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    885\u001b[0m \u001b[39mif\u001b[39;00m n \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m3\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mproto \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[1;32m    886\u001b[0m     \u001b[39mfor\u001b[39;00m element \u001b[39min\u001b[39;00m obj:\n\u001b[0;32m--> 887\u001b[0m         save(element)\n\u001b[1;32m    888\u001b[0m     \u001b[39m# Subtle.  Same as in the big comment below.\u001b[39;00m\n\u001b[1;32m    889\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mid\u001b[39m(obj) \u001b[39min\u001b[39;00m memo:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/capstone/lib/python3.11/site-packages/joblib/hashing.py:241\u001b[0m, in \u001b[0;36mNumpyHasher.save\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    239\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_hash\u001b[39m.\u001b[39mupdate(pickle\u001b[39m.\u001b[39mdumps(obj))\n\u001b[1;32m    240\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m--> 241\u001b[0m Hasher\u001b[39m.\u001b[39msave(\u001b[39mself\u001b[39m, obj)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/capstone/lib/python3.11/site-packages/joblib/hashing.py:89\u001b[0m, in \u001b[0;36mHasher.save\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[39mcls\u001b[39m \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39m\u001b[39m__self__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\n\u001b[1;32m     88\u001b[0m         obj \u001b[39m=\u001b[39m _MyHash(func_name, inst, \u001b[39mcls\u001b[39m)\n\u001b[0;32m---> 89\u001b[0m Pickler\u001b[39m.\u001b[39msave(\u001b[39mself\u001b[39m, obj)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/capstone/lib/python3.11/pickle.py:603\u001b[0m, in \u001b[0;36m_Pickler.save\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    599\u001b[0m     \u001b[39mraise\u001b[39;00m PicklingError(\u001b[39m\"\u001b[39m\u001b[39mTuple returned by \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m must have \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    600\u001b[0m                         \u001b[39m\"\u001b[39m\u001b[39mtwo to six elements\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m reduce)\n\u001b[1;32m    602\u001b[0m \u001b[39m# Save the reduce() output and finally memoize the object\u001b[39;00m\n\u001b[0;32m--> 603\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msave_reduce(obj\u001b[39m=\u001b[39mobj, \u001b[39m*\u001b[39mrv)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/capstone/lib/python3.11/pickle.py:692\u001b[0m, in \u001b[0;36m_Pickler.save_reduce\u001b[0;34m(self, func, args, state, listitems, dictitems, state_setter, obj)\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    691\u001b[0m     save(func)\n\u001b[0;32m--> 692\u001b[0m     save(args)\n\u001b[1;32m    693\u001b[0m     write(REDUCE)\n\u001b[1;32m    695\u001b[0m \u001b[39mif\u001b[39;00m obj \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    696\u001b[0m     \u001b[39m# If the object is already in the memo, this means it is\u001b[39;00m\n\u001b[1;32m    697\u001b[0m     \u001b[39m# recursive. In this case, throw away everything we put on the\u001b[39;00m\n\u001b[1;32m    698\u001b[0m     \u001b[39m# stack, and fetch the object back from the memo.\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/capstone/lib/python3.11/site-packages/joblib/hashing.py:241\u001b[0m, in \u001b[0;36mNumpyHasher.save\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    239\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_hash\u001b[39m.\u001b[39mupdate(pickle\u001b[39m.\u001b[39mdumps(obj))\n\u001b[1;32m    240\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m--> 241\u001b[0m Hasher\u001b[39m.\u001b[39msave(\u001b[39mself\u001b[39m, obj)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/capstone/lib/python3.11/site-packages/joblib/hashing.py:89\u001b[0m, in \u001b[0;36mHasher.save\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[39mcls\u001b[39m \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39m\u001b[39m__self__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\n\u001b[1;32m     88\u001b[0m         obj \u001b[39m=\u001b[39m _MyHash(func_name, inst, \u001b[39mcls\u001b[39m)\n\u001b[0;32m---> 89\u001b[0m Pickler\u001b[39m.\u001b[39msave(\u001b[39mself\u001b[39m, obj)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/capstone/lib/python3.11/pickle.py:560\u001b[0m, in \u001b[0;36m_Pickler.save\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    558\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch\u001b[39m.\u001b[39mget(t)\n\u001b[1;32m    559\u001b[0m \u001b[39mif\u001b[39;00m f \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 560\u001b[0m     f(\u001b[39mself\u001b[39m, obj)  \u001b[39m# Call unbound method with explicit self\u001b[39;00m\n\u001b[1;32m    561\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m    563\u001b[0m \u001b[39m# Check private dispatch table if any, or else\u001b[39;00m\n\u001b[1;32m    564\u001b[0m \u001b[39m# copyreg.dispatch_table\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/capstone/lib/python3.11/pickle.py:887\u001b[0m, in \u001b[0;36m_Pickler.save_tuple\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    885\u001b[0m \u001b[39mif\u001b[39;00m n \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m3\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mproto \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[1;32m    886\u001b[0m     \u001b[39mfor\u001b[39;00m element \u001b[39min\u001b[39;00m obj:\n\u001b[0;32m--> 887\u001b[0m         save(element)\n\u001b[1;32m    888\u001b[0m     \u001b[39m# Subtle.  Same as in the big comment below.\u001b[39;00m\n\u001b[1;32m    889\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mid\u001b[39m(obj) \u001b[39min\u001b[39;00m memo:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/capstone/lib/python3.11/site-packages/joblib/hashing.py:241\u001b[0m, in \u001b[0;36mNumpyHasher.save\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    239\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_hash\u001b[39m.\u001b[39mupdate(pickle\u001b[39m.\u001b[39mdumps(obj))\n\u001b[1;32m    240\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m--> 241\u001b[0m Hasher\u001b[39m.\u001b[39msave(\u001b[39mself\u001b[39m, obj)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/capstone/lib/python3.11/site-packages/joblib/hashing.py:89\u001b[0m, in \u001b[0;36mHasher.save\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[39mcls\u001b[39m \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39m\u001b[39m__self__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\n\u001b[1;32m     88\u001b[0m         obj \u001b[39m=\u001b[39m _MyHash(func_name, inst, \u001b[39mcls\u001b[39m)\n\u001b[0;32m---> 89\u001b[0m Pickler\u001b[39m.\u001b[39msave(\u001b[39mself\u001b[39m, obj)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/capstone/lib/python3.11/pickle.py:603\u001b[0m, in \u001b[0;36m_Pickler.save\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    599\u001b[0m     \u001b[39mraise\u001b[39;00m PicklingError(\u001b[39m\"\u001b[39m\u001b[39mTuple returned by \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m must have \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    600\u001b[0m                         \u001b[39m\"\u001b[39m\u001b[39mtwo to six elements\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m reduce)\n\u001b[1;32m    602\u001b[0m \u001b[39m# Save the reduce() output and finally memoize the object\u001b[39;00m\n\u001b[0;32m--> 603\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msave_reduce(obj\u001b[39m=\u001b[39mobj, \u001b[39m*\u001b[39mrv)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/capstone/lib/python3.11/pickle.py:717\u001b[0m, in \u001b[0;36m_Pickler.save_reduce\u001b[0;34m(self, func, args, state, listitems, dictitems, state_setter, obj)\u001b[0m\n\u001b[1;32m    715\u001b[0m \u001b[39mif\u001b[39;00m state \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    716\u001b[0m     \u001b[39mif\u001b[39;00m state_setter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 717\u001b[0m         save(state)\n\u001b[1;32m    718\u001b[0m         write(BUILD)\n\u001b[1;32m    719\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    720\u001b[0m         \u001b[39m# If a state_setter is specified, call it instead of load_build\u001b[39;00m\n\u001b[1;32m    721\u001b[0m         \u001b[39m# to update obj's with its previous state.\u001b[39;00m\n\u001b[1;32m    722\u001b[0m         \u001b[39m# First, push state_setter and its tuple of expected arguments\u001b[39;00m\n\u001b[1;32m    723\u001b[0m         \u001b[39m# (obj, state) onto the stack.\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/capstone/lib/python3.11/site-packages/joblib/hashing.py:241\u001b[0m, in \u001b[0;36mNumpyHasher.save\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    239\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_hash\u001b[39m.\u001b[39mupdate(pickle\u001b[39m.\u001b[39mdumps(obj))\n\u001b[1;32m    240\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m--> 241\u001b[0m Hasher\u001b[39m.\u001b[39msave(\u001b[39mself\u001b[39m, obj)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/capstone/lib/python3.11/site-packages/joblib/hashing.py:89\u001b[0m, in \u001b[0;36mHasher.save\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[39mcls\u001b[39m \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39m\u001b[39m__self__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\n\u001b[1;32m     88\u001b[0m         obj \u001b[39m=\u001b[39m _MyHash(func_name, inst, \u001b[39mcls\u001b[39m)\n\u001b[0;32m---> 89\u001b[0m Pickler\u001b[39m.\u001b[39msave(\u001b[39mself\u001b[39m, obj)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/capstone/lib/python3.11/pickle.py:560\u001b[0m, in \u001b[0;36m_Pickler.save\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    558\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch\u001b[39m.\u001b[39mget(t)\n\u001b[1;32m    559\u001b[0m \u001b[39mif\u001b[39;00m f \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 560\u001b[0m     f(\u001b[39mself\u001b[39m, obj)  \u001b[39m# Call unbound method with explicit self\u001b[39;00m\n\u001b[1;32m    561\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m    563\u001b[0m \u001b[39m# Check private dispatch table if any, or else\u001b[39;00m\n\u001b[1;32m    564\u001b[0m \u001b[39m# copyreg.dispatch_table\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/capstone/lib/python3.11/pickle.py:902\u001b[0m, in \u001b[0;36m_Pickler.save_tuple\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    900\u001b[0m write(MARK)\n\u001b[1;32m    901\u001b[0m \u001b[39mfor\u001b[39;00m element \u001b[39min\u001b[39;00m obj:\n\u001b[0;32m--> 902\u001b[0m     save(element)\n\u001b[1;32m    904\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mid\u001b[39m(obj) \u001b[39min\u001b[39;00m memo:\n\u001b[1;32m    905\u001b[0m     \u001b[39m# Subtle.  d was not in memo when we entered save_tuple(), so\u001b[39;00m\n\u001b[1;32m    906\u001b[0m     \u001b[39m# the process of saving the tuple's elements must have saved\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    910\u001b[0m     \u001b[39m# could have been done in the \"for element\" loop instead, but\u001b[39;00m\n\u001b[1;32m    911\u001b[0m     \u001b[39m# recursive tuples are a rare thing.\u001b[39;00m\n\u001b[1;32m    912\u001b[0m     get \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget(memo[\u001b[39mid\u001b[39m(obj)][\u001b[39m0\u001b[39m])\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/capstone/lib/python3.11/site-packages/joblib/hashing.py:241\u001b[0m, in \u001b[0;36mNumpyHasher.save\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    239\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_hash\u001b[39m.\u001b[39mupdate(pickle\u001b[39m.\u001b[39mdumps(obj))\n\u001b[1;32m    240\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m--> 241\u001b[0m Hasher\u001b[39m.\u001b[39msave(\u001b[39mself\u001b[39m, obj)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/capstone/lib/python3.11/site-packages/joblib/hashing.py:89\u001b[0m, in \u001b[0;36mHasher.save\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[39mcls\u001b[39m \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39m\u001b[39m__self__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\n\u001b[1;32m     88\u001b[0m         obj \u001b[39m=\u001b[39m _MyHash(func_name, inst, \u001b[39mcls\u001b[39m)\n\u001b[0;32m---> 89\u001b[0m Pickler\u001b[39m.\u001b[39msave(\u001b[39mself\u001b[39m, obj)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/capstone/lib/python3.11/pickle.py:560\u001b[0m, in \u001b[0;36m_Pickler.save\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    558\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch\u001b[39m.\u001b[39mget(t)\n\u001b[1;32m    559\u001b[0m \u001b[39mif\u001b[39;00m f \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 560\u001b[0m     f(\u001b[39mself\u001b[39m, obj)  \u001b[39m# Call unbound method with explicit self\u001b[39;00m\n\u001b[1;32m    561\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m    563\u001b[0m \u001b[39m# Check private dispatch table if any, or else\u001b[39;00m\n\u001b[1;32m    564\u001b[0m \u001b[39m# copyreg.dispatch_table\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/capstone/lib/python3.11/pickle.py:932\u001b[0m, in \u001b[0;36m_Pickler.save_list\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    929\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwrite(MARK \u001b[39m+\u001b[39m LIST)\n\u001b[1;32m    931\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmemoize(obj)\n\u001b[0;32m--> 932\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_batch_appends(obj)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/capstone/lib/python3.11/pickle.py:956\u001b[0m, in \u001b[0;36m_Pickler._batch_appends\u001b[0;34m(self, items)\u001b[0m\n\u001b[1;32m    954\u001b[0m     write(MARK)\n\u001b[1;32m    955\u001b[0m     \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m tmp:\n\u001b[0;32m--> 956\u001b[0m         save(x)\n\u001b[1;32m    957\u001b[0m     write(APPENDS)\n\u001b[1;32m    958\u001b[0m \u001b[39melif\u001b[39;00m n:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/capstone/lib/python3.11/site-packages/joblib/hashing.py:241\u001b[0m, in \u001b[0;36mNumpyHasher.save\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    239\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_hash\u001b[39m.\u001b[39mupdate(pickle\u001b[39m.\u001b[39mdumps(obj))\n\u001b[1;32m    240\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m--> 241\u001b[0m Hasher\u001b[39m.\u001b[39msave(\u001b[39mself\u001b[39m, obj)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/capstone/lib/python3.11/site-packages/joblib/hashing.py:72\u001b[0m, in \u001b[0;36mHasher.save\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[39mif\u001b[39;00m return_digest:\n\u001b[1;32m     70\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_hash\u001b[39m.\u001b[39mhexdigest()\n\u001b[0;32m---> 72\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msave\u001b[39m(\u001b[39mself\u001b[39m, obj):\n\u001b[1;32m     73\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(obj, (types\u001b[39m.\u001b[39mMethodType, \u001b[39mtype\u001b[39m({}\u001b[39m.\u001b[39mpop))):\n\u001b[1;32m     74\u001b[0m         \u001b[39m# the Pickler cannot pickle instance methods; here we decompose\u001b[39;00m\n\u001b[1;32m     75\u001b[0m         \u001b[39m# them into components that make them uniquely identifiable\u001b[39;00m\n\u001b[1;32m     76\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(obj, \u001b[39m'\u001b[39m\u001b[39m__func__\u001b[39m\u001b[39m'\u001b[39m):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#activating randomized grid search \n",
    "svm_random = RandomizedSearchCV(estimator = pipe, \n",
    "                           param_distributions= svm_param_grid, \n",
    "                           cv=5, \n",
    "                           n_iter = 100,\n",
    "                           verbose = 6,\n",
    "                           refit=True,\n",
    "                           scoring = 'f1_micro', \n",
    "                           random_state = 8)\n",
    "\n",
    "fitted_svmrandom_gs = svm_random.fit(X_remainder1_sample, y_remainder1_sample)\n",
    "\n",
    "grid_search_results = fitted_svmrandom_gs.cv_results_\n",
    "\n",
    "best_estimator = fitted_svmrandom_gs.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffbe788",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving results to s3 bucket\n",
    "write_results_to_s3(grid_search_results, 'svm_results_random_model1.csv', 'deliverable-slo-bstn-bucket', 'svm_results_random_model1.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2c62bd93",
   "metadata": {},
   "source": [
    "Best parameters: \n",
    "- min_df = 0.05 \n",
    "- PCA(0.95)\n",
    "- Param_clf = rbf \n",
    "- Scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495c9646",
   "metadata": {},
   "outputs": [],
   "source": [
    "#more refined parameters\n",
    "# Parameter grid\n",
    "svm_param_grid2 = [ \n",
    "     #SVM with PCA\n",
    "     {'transform__cv__min_df' : [0.05],\n",
    "     'transform__cv__max_features': [500],\n",
    "     'scaler' : [StandardScaler(with_mean=False)],\n",
    "     'dim_reducer': [PCA()],\n",
    "     'dim_reducer__n_components':[0.95 ,0.92,0.90],\n",
    "      'clf' : [SVC(random_state=10)], \n",
    "     'clf__kernel': ['rbf'],\n",
    "     'clf__C': [10,1],\n",
    "     'clf__gamma':[1,0.1]\n",
    "     }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e86aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#more refined grid search\n",
    "svm_gs = GridSearchCV(estimator = pipe, \n",
    "                           param_grid= svm_param_grid2, \n",
    "                           cv=5, \n",
    "                           verbose = 4,\n",
    "                           refit=True,\n",
    "                           scoring = 'f1_micro', \n",
    "                           random_state = 8)\n",
    "\n",
    "fitted_svm_gs = svm_gs.fit(X_remainder1, y_remainder1)\n",
    "\n",
    "grid_search_results = fitted_svm_gs.cv_results_\n",
    "\n",
    "best_estimator = fitted_svm_gs.best_estimator_\n",
    "print(best_estimator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e7CYuELHSxL",
   "metadata": {
    "id": "0e7CYuELHSxL"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'boto3'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#saving results to an s3 bucket\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mboto3\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpickle\u001b[39;00m\n\u001b[1;32m      5\u001b[0m bucket\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdeliverable-slo-bstn-bucket\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'boto3'"
     ]
    }
   ],
   "source": [
    "# saving svmreg pickle file\n",
    "save_best_model(best_estimator, 'pickled_best_svm_model1.pkl', bucket='deliverable-slo-bstn-bucket', key='pickled_best_svm_model1.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ec6a47",
   "metadata": {
    "id": "10ec6a47"
   },
   "outputs": [],
   "source": [
    "# Sanity check on the pickled model\n",
    "pklfilename = 'pickled_best_svm_model1.pkl'\n",
    "\n",
    "pickled_best_svm1 = joblib.load(pklfilename)\n",
    "\n",
    "# Print the accuracies\n",
    "print(f\"The best SVM accuracy on the remainder set: {pickled_best_svm1.score(X_remainder1, y_remainder1)}\")\n",
    "print(f\"The best SVM accuracy on the test set: {pickled_best_svm1.score(X_val1, y_val1)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa2b308",
   "metadata": {
    "id": "dfa2b308"
   },
   "source": [
    "### Random Forest Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c120af2",
   "metadata": {
    "id": "3c120af2"
   },
   "outputs": [],
   "source": [
    "#hyper parameters Random Forest \n",
    "n_estimators_range = list(range(100,1600,200))\n",
    "max_depth_range = list(range(10,50,10))\n",
    "max_features_range = np.arange(0.2,0.6,0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f613f0",
   "metadata": {
    "id": "97f613f0"
   },
   "outputs": [],
   "source": [
    "# Parameter grid\n",
    "\n",
    "rf_param_grid = [\n",
    "    # Random Forest without PCA\n",
    "    {'transform__cv__min_df' : vectorize_mindf_list ,\n",
    "     'transform__cv__max_features': [500],\n",
    "     'dim_reducer': ['passthrough'],\n",
    "     'scaler' : ['passthrough'],\n",
    "     'clf': [RandomForestClassifier(n_jobs = 1)],\n",
    "     'clf__max_depth': max_depth_range,\n",
    "     'clf__n_estimators':n_estimators_range,\n",
    "     'clf__max_features' :max_features_range\n",
    "     },\n",
    "     \n",
    "     #Random Forest with PCA\n",
    "     {'transform__cv__min_df' : vectorize_mindf_list ,\n",
    "     'transform__cv__max_features': [500],\n",
    "     'scaler' : ['passthrough'],\n",
    "     'dim_reducer': [PCA()],\n",
    "     'dim_reducer__n_components':[0.95, 0.8], \n",
    "     'clf': [RandomForestClassifier(n_jobs = 1)],\n",
    "     'clf__max_depth': max_depth_range,\n",
    "     'clf__n_estimators':n_estimators_range,\n",
    "     'clf__max_features' :max_features_range\n",
    "     }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c22501",
   "metadata": {
    "id": "e3c22501"
   },
   "outputs": [],
   "source": [
    "#activating grid search \n",
    "rf_random = RandomizedSearchCV(estimator = pipe, \n",
    "                           param_distributions= rf_param_grid, \n",
    "                           cv=5, \n",
    "                           n_iter = 75,\n",
    "                           verbose = 4,\n",
    "                           refit=True,\n",
    "                           scoring = 'f1_micro', \n",
    "                           random_state = 8)\n",
    "\n",
    "fitted_rf_gs = rf_random.fit(X_remainder1_sample, y_remainder1_sample)\n",
    "\n",
    "grid_search_results = fitted_rf_gs.cv_results_\n",
    "\n",
    "best_estimator = fitted_rf_gs.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba35b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calling the save function\n",
    "write_results_to_s3(grid_search_results, 'rf_random_results_model_1.csv', 'deliverable-slo-bstn-bucket', 'rf_random_results_model_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a338b9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#more refined grid search to total train set \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3ce6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#more refined grid search to total train set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a51f6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving random forest pickle file\n",
    "save_best_model(best_estimator, 'pickled_best_rf_model1.pkl', bucket='deliverable-slo-bstn-bucket', key='pickled_best_rf_model1.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc52f97",
   "metadata": {
    "id": "7cc52f97"
   },
   "outputs": [],
   "source": [
    "# Sanity check on the pickled model\n",
    "\n",
    "filename = 'pickled_best_rf_reg1.pkl'\n",
    "pickled_best = joblib.load(filename)\n",
    "\n",
    "# Print the accuracies\n",
    "print(f\"The best Random Forest accuracy on the remainder set: {pickled_best.score(X_train1, y_train1)}\")\n",
    "print(f\"The best Random Forest accuracy on the validation set: {pickled_best.score(X_val1, y_val1)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "WUBdNWshLQ09",
   "metadata": {
    "id": "WUBdNWshLQ09"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f57fa7ba",
   "metadata": {
    "id": "f57fa7ba"
   },
   "source": [
    "1. A Random Forest works by fitting several deep decision trees. Each tree is fitted to a random subset of the data and features. The trees are trained in parallel and their decisions are totally independent of each other. The only thing we can control for is the depth of the trees and number of trees we fit. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2fc849",
   "metadata": {
    "id": "af2fc849"
   },
   "source": [
    "### XGBOOST Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b45749",
   "metadata": {
    "id": "85b45749"
   },
   "source": [
    "2. Boosting-based methods work by fitting one model at a time. Each subsequent model is fitted to correct for the mistakes made by the previous models. In addition to the number of models and their complexity, we can control for how strong of a correction each model makes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c535776",
   "metadata": {
    "id": "2c535776"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "fe17c22c",
   "metadata": {
    "id": "fe17c22c"
   },
   "outputs": [],
   "source": [
    "#hyper parameters XGBoost \n",
    "\n",
    "gamma_range = [i/10.0 for i in range(0,5)]\n",
    "# Percentage of columns to be randomly samples for each tree\n",
    "colsample_bytree_range = [i/10.0 for i in range(3,10)]\n",
    " # Maximum depth of the tree, increasing it increases the model complexity\n",
    "max_depth_range = list(range(3,21,3))\n",
    "  # Learning rate shrinks the weights to make the boosting process more conservative\n",
    "learning_rate_range = [0.01 , 0.1, 1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677972df",
   "metadata": {
    "id": "677972df"
   },
   "outputs": [],
   "source": [
    "# Parameter grid\n",
    "\n",
    "boost_param_grid = [\n",
    "    # boost without PCA\n",
    "    {'transform__cv__min_df' : vectorize_mindf_list ,\n",
    "     'transform__cv__max_features': [500],\n",
    "     'scaler' : ['passthrough'],\n",
    "     'dim_reducer': ['passthrough'],\n",
    "     'clf': [XGBClassifier()],\n",
    "     'clf__gamma': gamma_range,\n",
    "     'clf__colsample_bytree':colsample_bytree_range,\n",
    "     'clf__max_depth':max_depth_range,\n",
    "     'clf__learning_rate' :learning_rate_range\n",
    "     },\n",
    "     \n",
    "     #boost with PCA\n",
    "     {'transform__cv__min_df' : vectorize_mindf_list ,\n",
    "     'transform__cv__max_features': [500],\n",
    "     'scaler' : ['passthrough'],\n",
    "     'dim_reducer': [XGBClassifier()],\n",
    "     'dim_reducer__n_components':[0.95, 0.8], \n",
    "     'clf': [XGBClassifier()],\n",
    "     'clf__gamma': gamma_range,\n",
    "     'clf__colsample_bytree':colsample_bytree_range,\n",
    "     'clf__max_depth':max_depth_range,\n",
    "     'clf__learning_rate' :learning_rate_range\n",
    "     }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1e03a2",
   "metadata": {
    "id": "7b1e03a2"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#activating grid search \n",
    "boost_random = RandomizedSearchCV(estimator = pipe, \n",
    "                           param_distributions= boost_param_grid, \n",
    "                           cv=5, \n",
    "                           n_iter = 60,\n",
    "                           verbose = 4,\n",
    "                           refit=True,\n",
    "                           scoring = 'f1_micro', \n",
    "                           random_state = 8)\n",
    "\n",
    "fitted_boost_random = boost_random.fit(X_remainder1_sample, y_remainder1_sample)\n",
    "\n",
    "\n",
    "grid_search_results = fitted_boost_random.cv_results_\n",
    "\n",
    "best_estimator = fitted_boost_random.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "b8662d84",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'boost_random' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[98], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m boost_random_results_model_1  \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(boost_random\u001b[39m.\u001b[39mcv_results_)\n\u001b[1;32m      2\u001b[0m boost_random_results_model_1\u001b[39m.\u001b[39mto_csv(\u001b[39m'\u001b[39m\u001b[39mboost_random_results_model_1.csv\u001b[39m\u001b[39m'\u001b[39m,index\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'boost_random' is not defined"
     ]
    }
   ],
   "source": [
    "#calling the save function\n",
    "write_results_to_s3(grid_search_results, 'boost_random_results_model_1.csv', 'deliverable-slo-bstn-bucket', 'boost_random_results_model_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70c973e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "c022088b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'boost_random_results_model_1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[97], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mstr\u001b[39m(boost_random_results_model_1)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'boost_random_results_model_1' is not defined"
     ]
    }
   ],
   "source": [
    "s3_client.upload_file(Filename='boost_random_results_model_1.csv',\n",
    "                  Bucket='deliverable-slo-bstn-bucket',\n",
    "                  Key='boost_random_results_model_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uyKOMYpsKKE8",
   "metadata": {
    "id": "uyKOMYpsKKE8"
   },
   "outputs": [],
   "source": [
    "#more refined grid search with train sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d999f3",
   "metadata": {
    "id": "40d999f3"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6b1dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving random forest pickle file\n",
    "save_best_model(best_estimator, 'pickled_best_boost_model1.pkl', bucket='deliverable-slo-bstn-bucket', key='pickled_best_boost_model1.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e1d692",
   "metadata": {
    "id": "f3e1d692"
   },
   "outputs": [],
   "source": [
    "# Sanity check on the pickled model\n",
    "pickled_best_boost1 = joblib.load('pickled_best_boost_reg1.pkl')\n",
    "\n",
    "# Print the accuracies\n",
    "print(f\"The best XGBoost accuracy on the remainder set: {pickled_best_boost1.score(X_train1, y_train1)}\")\n",
    "print(f\"The best XGBoost accuracy on the validation set: {pickled_best_boost1.score(X_val1, y_val1)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd5c523",
   "metadata": {
    "id": "cfd5c523"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982c133a",
   "metadata": {
    "id": "982c133a"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f56246",
   "metadata": {
    "id": "a8f56246"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0f6e4b",
   "metadata": {
    "id": "7b0f6e4b"
   },
   "outputs": [],
   "source": [
    "#figuring out baseline to compare against\n",
    "pct = 100 - (y_train.sum()/y_train.shape[0]*100)\n",
    "print(f\"Number of review in the training dataset that have trending products: {y_train.sum()} which means our baseline for the model is: {round(pct,4)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15deae5",
   "metadata": {
    "id": "a15deae5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c3fe27",
   "metadata": {
    "id": "e6c3fe27"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b108f9c5",
   "metadata": {
    "id": "b108f9c5"
   },
   "source": [
    "## Count Vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3bde95",
   "metadata": {
    "id": "4b3bde95"
   },
   "source": [
    "We will start by inputting a count vectorizer on the model for the review data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161f5596",
   "metadata": {
    "id": "161f5596"
   },
   "outputs": [],
   "source": [
    "def length(text):    \n",
    "    '''a function which returns the length of text'''\n",
    "    return len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955530fb",
   "metadata": {
    "id": "955530fb"
   },
   "outputs": [],
   "source": [
    "train[\"length\"]=train[\"reviewText\"].apply(length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed48cc7",
   "metadata": {
    "id": "4ed48cc7"
   },
   "outputs": [],
   "source": [
    "def get_top_tweet_bigrams(corpus, n=None):\n",
    "    vec = CountVectorizer(ngram_range=(2, 2)).fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    return words_freq[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1efd27f",
   "metadata": {
    "id": "e1efd27f"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d2b097",
   "metadata": {
    "id": "b5d2b097",
    "outputId": "c47f68df-6e71-4fe7-895e-5dd17c28e5e3"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plotting it out\n",
    "\n",
    "pos_word_counts = pd.DataFrame(\n",
    "    {\"counts\": train_transformed.toarray().sum(axis=0)},\n",
    "    index=bagofwords.get_feature_names_out()\n",
    ").sort_values(\"counts\", ascending=False)\n",
    "\n",
    "pos_word_counts.head(20).plot(kind=\"barh\", figsize=(15, 5), legend=False, color = \"navy\")\n",
    "plt.title(\"Figure: Top most frequently occurring words in review text\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4f02f4",
   "metadata": {
    "id": "1c4f02f4",
    "outputId": "71be606e-2006-43c1-ffd1-464dcbda7e18"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>words</th>\n",
       "      <th>r_product</th>\n",
       "      <th>r_hair</th>\n",
       "      <th>r_love</th>\n",
       "      <th>r_great</th>\n",
       "      <th>r_use</th>\n",
       "      <th>r_skin</th>\n",
       "      <th>r_like</th>\n",
       "      <th>r_color</th>\n",
       "      <th>r_good</th>\n",
       "      <th>r_just</th>\n",
       "      <th>...</th>\n",
       "      <th>r_bit</th>\n",
       "      <th>r_easy</th>\n",
       "      <th>r_looks</th>\n",
       "      <th>r_got</th>\n",
       "      <th>r_need</th>\n",
       "      <th>r_definitely</th>\n",
       "      <th>r_worth</th>\n",
       "      <th>r_smooth</th>\n",
       "      <th>r_goes</th>\n",
       "      <th>r_favorite</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42593</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42594</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42595</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42596</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42597</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>42598 rows Ã— 63 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "words  r_product  r_hair  r_love  r_great  r_use  r_skin  r_like  r_color  \\\n",
       "0              0       0       0        0      0       0       0        0   \n",
       "1              0       0       0        0      0       0       0        0   \n",
       "2              0       0       0        0      0       0       0        0   \n",
       "3              0       0       0        0      0       0       0        0   \n",
       "4              0       0       0        1      0       0       0        0   \n",
       "...          ...     ...     ...      ...    ...     ...     ...      ...   \n",
       "42593          0       0       0        0      0       0       0        0   \n",
       "42594          0       0       0        0      0       0       0        0   \n",
       "42595          0       0       1        0      0       0       0        0   \n",
       "42596          0       0       0        0      0       0       0        0   \n",
       "42597          0       0       0        0      0       0       0        0   \n",
       "\n",
       "words  r_good  r_just  ...  r_bit  r_easy  r_looks  r_got  r_need  \\\n",
       "0           0       0  ...      0       0        0      0       0   \n",
       "1           0       0  ...      0       0        0      0       0   \n",
       "2           0       0  ...      0       0        0      0       0   \n",
       "3           0       0  ...      0       2        0      0       0   \n",
       "4           0       0  ...      0       3        0      0       0   \n",
       "...       ...     ...  ...    ...     ...      ...    ...     ...   \n",
       "42593       0       0  ...      0       0        0      0       0   \n",
       "42594       0       0  ...      0       0        0      1       0   \n",
       "42595       0       1  ...      0       0        0      0       0   \n",
       "42596       0       0  ...      0       0        0      0       0   \n",
       "42597       0       0  ...      0       0        0      0       0   \n",
       "\n",
       "words  r_definitely  r_worth  r_smooth  r_goes  r_favorite  \n",
       "0                 0        0         0       0           0  \n",
       "1                 0        0         0       0           0  \n",
       "2                 0        0         0       0           0  \n",
       "3                 0        0         0       0           0  \n",
       "4                 1        0         0       0           0  \n",
       "...             ...      ...       ...     ...         ...  \n",
       "42593             0        1         0       0           0  \n",
       "42594             0        0         1       0           0  \n",
       "42595             0        0         0       0           0  \n",
       "42596             0        0         0       0           0  \n",
       "42597             0        0         0       0           0  \n",
       "\n",
       "[42598 rows x 63 columns]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_vect_df = pd.DataFrame(train_transformed.todense(), columns= review_word_counts[\"words\"])\n",
    "review_vect_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd51a8e8",
   "metadata": {
    "id": "dd51a8e8"
   },
   "outputs": [],
   "source": [
    "trending_graph = pd.concat([train[\"trending_asin\"].reset_index(drop=True), review_vect_df.reset_index(drop=True)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62cfd11",
   "metadata": {
    "id": "f62cfd11"
   },
   "outputs": [],
   "source": [
    "columns = trending_graph.columns[trending_graph.columns.str.startswith('r_')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f6dd04",
   "metadata": {
    "id": "37f6dd04",
    "outputId": "58f977bb-2a1d-449b-bc38-b420064ced33"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>trending_asin</th>\n",
       "      <th>index</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>r_way</td>\n",
       "      <td>6330</td>\n",
       "      <td>1430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>r_cream</td>\n",
       "      <td>11544</td>\n",
       "      <td>1231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>r_buy</td>\n",
       "      <td>9070</td>\n",
       "      <td>1039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>r_easy</td>\n",
       "      <td>6835</td>\n",
       "      <td>976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>r_don</td>\n",
       "      <td>3039</td>\n",
       "      <td>827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>r_polish</td>\n",
       "      <td>6594</td>\n",
       "      <td>775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>r_years</td>\n",
       "      <td>8566</td>\n",
       "      <td>773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>r_little</td>\n",
       "      <td>4752</td>\n",
       "      <td>572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>r_good</td>\n",
       "      <td>5511</td>\n",
       "      <td>529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>r_smell</td>\n",
       "      <td>4512</td>\n",
       "      <td>489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>r_makes</td>\n",
       "      <td>4000</td>\n",
       "      <td>445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>r_best</td>\n",
       "      <td>2697</td>\n",
       "      <td>440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>r_got</td>\n",
       "      <td>3935</td>\n",
       "      <td>424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>r_looks</td>\n",
       "      <td>4267</td>\n",
       "      <td>406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>r_bought</td>\n",
       "      <td>1619</td>\n",
       "      <td>395</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "trending_asin     index      0     1\n",
       "58                r_way   6330  1430\n",
       "9               r_cream  11544  1231\n",
       "7                 r_buy   9070  1039\n",
       "18               r_easy   6835   976\n",
       "16                r_don   3039   827\n",
       "40             r_polish   6594   775\n",
       "62              r_years   8566   773\n",
       "30             r_little   4752   572\n",
       "23               r_good   5511   529\n",
       "48              r_smell   4512   489\n",
       "36              r_makes   4000   445\n",
       "2                r_best   2697   440\n",
       "24                r_got   3935   424\n",
       "33              r_looks   4267   406\n",
       "6              r_bought   1619   395"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#show graph with different count vectorizer counts, melt function (transpose, trending ,non-trending )\n",
    "df = pd.pivot_table(trending_graph, index = \"trending_asin\",aggfunc = \"sum\").T\n",
    "df= df.reset_index()\n",
    "df= df.sort_values(by=1, ascending=False).head(15)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4122f11d",
   "metadata": {
    "id": "4122f11d"
   },
   "outputs": [],
   "source": [
    "df[\"total\"] = df[0]+df[1]\n",
    "df[0]= df[0]/df[\"total\"]\n",
    "df[1]= df[1]/df[\"total\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34b3fcc",
   "metadata": {
    "id": "d34b3fcc"
   },
   "outputs": [],
   "source": [
    "df = df.rename({'index': 'word', 0: 'not trending', 1: 'trending'}, axis=1)\n",
    "df = df.drop(columns='total')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555ad08e",
   "metadata": {
    "id": "555ad08e"
   },
   "outputs": [],
   "source": [
    "df = df.melt(id_vars = 'word').rename(columns = str.title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d12d9d0",
   "metadata": {
    "id": "0d12d9d0"
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4a7dd9",
   "metadata": {
    "id": "6e4a7dd9",
    "outputId": "d29414f8-239c-46a6-ce75-9b13cc1778b5"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.barplot(x='Word',y='Value', hue='Trending_Asin', data = df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce6c0e6",
   "metadata": {
    "id": "3ce6c0e6",
    "outputId": "dec964b5-466f-4ba5-bfab-97ebf8c8526e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>overall</th>\n",
       "      <th>vote</th>\n",
       "      <th>verified</th>\n",
       "      <th>asin</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>summary</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>trending_asin</th>\n",
       "      <th>title</th>\n",
       "      <th>...</th>\n",
       "      <th>r_bit</th>\n",
       "      <th>r_easy</th>\n",
       "      <th>r_looks</th>\n",
       "      <th>r_got</th>\n",
       "      <th>r_need</th>\n",
       "      <th>r_definitely</th>\n",
       "      <th>r_worth</th>\n",
       "      <th>r_smooth</th>\n",
       "      <th>r_goes</th>\n",
       "      <th>r_favorite</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7982</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>B000142FVW</td>\n",
       "      <td>nice color and OPI is high quality nail enamel</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>2016</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>OPI Nail Lacquer  Not So Bora Bora ing Pink   ...</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7370</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>B006RBR7NO</td>\n",
       "      <td>Caused a rash all over my face.</td>\n",
       "      <td>Threw it away!</td>\n",
       "      <td>2017</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>Bioderma Sensibio H O Micellar Water  Cleansin...</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 133 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      overall  vote  verified        asin  \\\n",
       "7982      5.0   0.0         1  B000142FVW   \n",
       "7370      1.0   2.0         1  B006RBR7NO   \n",
       "\n",
       "                                          reviewText         summary  year  \\\n",
       "7982  nice color and OPI is high quality nail enamel      Five Stars  2016   \n",
       "7370                 Caused a rash all over my face.  Threw it away!  2017   \n",
       "\n",
       "      month  trending_asin                                              title  \\\n",
       "7982      7              0  OPI Nail Lacquer  Not So Bora Bora ing Pink   ...   \n",
       "7370     12              1  Bioderma Sensibio H O Micellar Water  Cleansin...   \n",
       "\n",
       "      ...  r_bit  r_easy r_looks  r_got  r_need r_definitely r_worth  \\\n",
       "7982  ...      0       0       0      0       0            0       0   \n",
       "7370  ...      0       0       0      0       0            0       0   \n",
       "\n",
       "      r_smooth  r_goes  r_favorite  \n",
       "7982         0       0           0  \n",
       "7370         0       0           0  \n",
       "\n",
       "[2 rows x 133 columns]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#concatenating review vector to train \n",
    "train = pd.concat([train.reset_index(drop=True), review_vect_df.reset_index(drop=True)], axis=1)\n",
    "train.sample(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0d4759",
   "metadata": {
    "id": "6b0d4759"
   },
   "source": [
    "### count vectorizer on review headline description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6a504d",
   "metadata": {
    "id": "ab6a504d"
   },
   "outputs": [],
   "source": [
    "fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\n",
    "review_len=train[train['trending_asin']==1]['reviewText'].str.len()\n",
    "ax1.hist(review_len,color='blue',bins=100)\n",
    "ax1.set_title('Trending Products')\n",
    "tweet_len=train[train['trending_asin']==0]['reviewText'].str.len()\n",
    "ax2.hist(review_len,color='red',bins=100)\n",
    "ax2.set_title('Non Trending products')\n",
    "fig.suptitle('Characters in Review Text')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab3728f",
   "metadata": {
    "id": "dab3728f",
    "outputId": "3c356ed8-2267-4312-b6f0-effcef23c805"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['overall', 'vote', 'verified', 'asin', 'reviewText', 'summary', 'year',\n",
       "       'month', 'trending_asin', 'title',\n",
       "       ...\n",
       "       'r_bit', 'r_easy', 'r_looks', 'r_got', 'r_need', 'r_definitely',\n",
       "       'r_worth', 'r_smooth', 'r_goes', 'r_favorite'],\n",
       "      dtype='object', length=133)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94dd0a95",
   "metadata": {
    "id": "94dd0a95",
    "outputId": "ed204c1b-8fd4-4228-9e6f-876703d51843"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mac/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>039</th>\n",
       "      <th>090d845f</th>\n",
       "      <th>10</th>\n",
       "      <th>1000</th>\n",
       "      <th>1000_pt0_sx300__</th>\n",
       "      <th>15</th>\n",
       "      <th>17200</th>\n",
       "      <th>1981</th>\n",
       "      <th>20</th>\n",
       "      <th>200</th>\n",
       "      <th>...</th>\n",
       "      <th>women</th>\n",
       "      <th>work</th>\n",
       "      <th>works</th>\n",
       "      <th>world</th>\n",
       "      <th>worldwide</th>\n",
       "      <th>year</th>\n",
       "      <th>years</th>\n",
       "      <th>yet</th>\n",
       "      <th>you</th>\n",
       "      <th>your</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42593</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42594</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42595</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42596</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42597</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>42598 rows Ã— 534 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       039  090d845f  10  1000  1000_pt0_sx300__  15  17200  1981  20  200  \\\n",
       "0        0         0   0     0                 0   0      0     0   0    0   \n",
       "1        0         0   0     0                 0   0      0     0   0    0   \n",
       "2        0         0   0     0                 0   0      0     0   0    0   \n",
       "3        0         0   0     0                 0   0      1     0   1    0   \n",
       "4        0         0   0     0                 0   0      0     0   0    0   \n",
       "...    ...       ...  ..   ...               ...  ..    ...   ...  ..  ...   \n",
       "42593    0         0   0     0                 0   0      0     0   0    0   \n",
       "42594    0         0   0     0                 0   0      0     0   0    0   \n",
       "42595    1         0   0     0                 0   0      0     0   0    0   \n",
       "42596    0         0   0     0                 0   0      0     0   0    0   \n",
       "42597    0         0   0     0                 0   0      0     0   0    0   \n",
       "\n",
       "       ...  women  work  works  world  worldwide  year  years  yet  you  your  \n",
       "0      ...      0     0      0      0          0     0      0    0    0     0  \n",
       "1      ...      0     0      1      0          0     0      0    0    1     0  \n",
       "2      ...      0     0      0      0          0     0      0    1    0     0  \n",
       "3      ...      0     0      0      0          0     0      0    0    0     1  \n",
       "4      ...      0     0      0      0          0     0      0    1    1     1  \n",
       "...    ...    ...   ...    ...    ...        ...   ...    ...  ...  ...   ...  \n",
       "42593  ...      0     0      0      0          0     0      0    0    1     0  \n",
       "42594  ...      0     0      0      1          0     0      0    0    0     0  \n",
       "42595  ...      0     2      0      1          0     0      0    0    1     0  \n",
       "42596  ...      0     0      0      1          0     0      0    0    1     0  \n",
       "42597  ...      0     0      0      0          0     0      0    0    0     0  \n",
       "\n",
       "[42598 rows x 534 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### count vectorizer on review headline description\n",
    "bagofwords = CountVectorizer(min_df = 0.05).fit(train[\"product_description\"])\n",
    "small_transformed = bagofwords.transform(train[\"product_description\"])\n",
    "\n",
    "my_df = pd.DataFrame(columns=bagofwords.get_feature_names(), data=small_transformed.toarray())\n",
    "display(my_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61294d6f",
   "metadata": {
    "id": "61294d6f"
   },
   "outputs": [],
   "source": [
    "#starting TF-IDF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(min_df = 0.05, max_features= 200\n",
    "                         ).fit(train[\"product_description\"])\n",
    "small_transformed = tfidf.transform(train[\"product_description\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d7a39e",
   "metadata": {
    "id": "97d7a39e"
   },
   "outputs": [],
   "source": [
    "my_df = pd.DataFrame(columns=(f\"d_{tfidf.get_feature_names()}\"), data=small_transformed.toarray())\n",
    "display(my_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732f7f21",
   "metadata": {
    "id": "732f7f21",
    "outputId": "4ce5e0f6-b2dc-445d-c660-81ad8a1ca422"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(max_features=200, min_df=0.05)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f29a1e",
   "metadata": {
    "id": "56f29a1e",
    "outputId": "a120b3de-dbcc-467a-ce5d-c5161659b7e6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mac/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>039</th>\n",
       "      <th>1000</th>\n",
       "      <th>1000_pt0_sx300__</th>\n",
       "      <th>_blank</th>\n",
       "      <th>_cr0</th>\n",
       "      <th>_sl220__</th>\n",
       "      <th>_sl300__</th>\n",
       "      <th>acid</th>\n",
       "      <th>after</th>\n",
       "      <th>all</th>\n",
       "      <th>...</th>\n",
       "      <th>which</th>\n",
       "      <th>while</th>\n",
       "      <th>will</th>\n",
       "      <th>with</th>\n",
       "      <th>without</th>\n",
       "      <th>world</th>\n",
       "      <th>worldwide</th>\n",
       "      <th>years</th>\n",
       "      <th>you</th>\n",
       "      <th>your</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.101884</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.063490</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.034358</td>\n",
       "      <td>0.078736</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.119753</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.161538</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.123912</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.203939</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.071004</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.081673</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.070864</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.047795</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.070410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.059267</td>\n",
       "      <td>0.041532</td>\n",
       "      <td>...</td>\n",
       "      <td>0.219174</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.112047</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.042974</td>\n",
       "      <td>0.041266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42593</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.077114</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.292119</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.064022</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42594</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.066395</td>\n",
       "      <td>...</td>\n",
       "      <td>0.116794</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.179123</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.094017</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42595</th>\n",
       "      <td>0.062414</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.076332</td>\n",
       "      <td>0.057536</td>\n",
       "      <td>0.040319</td>\n",
       "      <td>...</td>\n",
       "      <td>0.070924</td>\n",
       "      <td>0.050250</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.108774</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.057093</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.041719</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42596</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.118066</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.049576</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.036226</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42597</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.101884</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.063490</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.034358</td>\n",
       "      <td>0.078736</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>42598 rows Ã— 200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            039  1000  1000_pt0_sx300__  _blank      _cr0  _sl220__  _sl300__  \\\n",
       "0      0.000000   0.0               0.0     0.0  0.000000       0.0       0.0   \n",
       "1      0.000000   0.0               0.0     0.0  0.000000       0.0       0.0   \n",
       "2      0.000000   0.0               0.0     0.0  0.203939       0.0       0.0   \n",
       "3      0.000000   0.0               0.0     0.0  0.000000       0.0       0.0   \n",
       "4      0.000000   0.0               0.0     0.0  0.000000       0.0       0.0   \n",
       "...         ...   ...               ...     ...       ...       ...       ...   \n",
       "42593  0.000000   0.0               0.0     0.0  0.000000       0.0       0.0   \n",
       "42594  0.000000   0.0               0.0     0.0  0.000000       0.0       0.0   \n",
       "42595  0.062414   0.0               0.0     0.0  0.000000       0.0       0.0   \n",
       "42596  0.000000   0.0               0.0     0.0  0.000000       0.0       0.0   \n",
       "42597  0.000000   0.0               0.0     0.0  0.000000       0.0       0.0   \n",
       "\n",
       "           acid     after       all  ...     which     while  will      with  \\\n",
       "0      0.000000  0.000000  0.101884  ...  0.000000  0.063490   0.0  0.034358   \n",
       "1      0.000000  0.000000  0.119753  ...  0.000000  0.000000   0.0  0.161538   \n",
       "2      0.000000  0.000000  0.000000  ...  0.071004  0.000000   0.0  0.081673   \n",
       "3      0.000000  0.000000  0.070864  ...  0.000000  0.000000   0.0  0.047795   \n",
       "4      0.000000  0.059267  0.041532  ...  0.219174  0.000000   0.0  0.112047   \n",
       "...         ...       ...       ...  ...       ...       ...   ...       ...   \n",
       "42593  0.000000  0.000000  0.000000  ...  0.000000  0.077114   0.0  0.292119   \n",
       "42594  0.000000  0.000000  0.066395  ...  0.116794  0.000000   0.0  0.179123   \n",
       "42595  0.076332  0.057536  0.040319  ...  0.070924  0.050250   0.0  0.108774   \n",
       "42596  0.000000  0.000000  0.000000  ...  0.000000  0.000000   0.0  0.118066   \n",
       "42597  0.000000  0.000000  0.101884  ...  0.000000  0.063490   0.0  0.034358   \n",
       "\n",
       "        without     world  worldwide  years       you      your  \n",
       "0      0.078736  0.000000        0.0    0.0  0.000000  0.000000  \n",
       "1      0.000000  0.000000        0.0    0.0  0.123912  0.000000  \n",
       "2      0.000000  0.000000        0.0    0.0  0.000000  0.000000  \n",
       "3      0.000000  0.000000        0.0    0.0  0.000000  0.070410  \n",
       "4      0.000000  0.000000        0.0    0.0  0.042974  0.041266  \n",
       "...         ...       ...        ...    ...       ...       ...  \n",
       "42593  0.000000  0.000000        0.0    0.0  0.064022  0.000000  \n",
       "42594  0.000000  0.094017        0.0    0.0  0.000000  0.000000  \n",
       "42595  0.000000  0.057093        0.0    0.0  0.041719  0.000000  \n",
       "42596  0.000000  0.049576        0.0    0.0  0.036226  0.000000  \n",
       "42597  0.078736  0.000000        0.0    0.0  0.000000  0.000000  \n",
       "\n",
       "[42598 rows x 200 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "my_df = pd.DataFrame(columns=tfidf.get_feature_names(), data=small_transformed.toarray())\n",
    "display(my_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaac3436",
   "metadata": {
    "id": "eaac3436",
    "outputId": "09d91aee-c0d2-424f-8b4b-bcbb7eb6bacf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mac/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>d_039</th>\n",
       "      <th>d_1000</th>\n",
       "      <th>d_1000_pt0_sx300__</th>\n",
       "      <th>d__blank</th>\n",
       "      <th>d__cr0</th>\n",
       "      <th>d__sl220__</th>\n",
       "      <th>d__sl300__</th>\n",
       "      <th>d_acid</th>\n",
       "      <th>d_after</th>\n",
       "      <th>d_all</th>\n",
       "      <th>...</th>\n",
       "      <th>d_which</th>\n",
       "      <th>d_while</th>\n",
       "      <th>d_will</th>\n",
       "      <th>d_with</th>\n",
       "      <th>d_without</th>\n",
       "      <th>d_world</th>\n",
       "      <th>d_worldwide</th>\n",
       "      <th>d_years</th>\n",
       "      <th>d_you</th>\n",
       "      <th>d_your</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>81887</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64609</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17752</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44538</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68507</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66975</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15881</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54151</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81926</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>42598 rows Ã— 200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      d_039 d_1000 d_1000_pt0_sx300__ d__blank d__cr0 d__sl220__ d__sl300__  \\\n",
       "81887   NaN    NaN                NaN      NaN    NaN        NaN        NaN   \n",
       "64609   NaN    NaN                NaN      NaN    NaN        NaN        NaN   \n",
       "17752   NaN    NaN                NaN      NaN    NaN        NaN        NaN   \n",
       "44538   NaN    NaN                NaN      NaN    NaN        NaN        NaN   \n",
       "68507   NaN    NaN                NaN      NaN    NaN        NaN        NaN   \n",
       "...     ...    ...                ...      ...    ...        ...        ...   \n",
       "66975   NaN    NaN                NaN      NaN    NaN        NaN        NaN   \n",
       "15881   NaN    NaN                NaN      NaN    NaN        NaN        NaN   \n",
       "280     NaN    NaN                NaN      NaN    NaN        NaN        NaN   \n",
       "54151   NaN    NaN                NaN      NaN    NaN        NaN        NaN   \n",
       "81926   NaN    NaN                NaN      NaN    NaN        NaN        NaN   \n",
       "\n",
       "      d_acid d_after d_all  ... d_which d_while d_will d_with d_without  \\\n",
       "81887    NaN     NaN   NaN  ...     NaN     NaN    NaN    NaN       NaN   \n",
       "64609    NaN     NaN   NaN  ...     NaN     NaN    NaN    NaN       NaN   \n",
       "17752    NaN     NaN   NaN  ...     NaN     NaN    NaN    NaN       NaN   \n",
       "44538    NaN     NaN   NaN  ...     NaN     NaN    NaN    NaN       NaN   \n",
       "68507    NaN     NaN   NaN  ...     NaN     NaN    NaN    NaN       NaN   \n",
       "...      ...     ...   ...  ...     ...     ...    ...    ...       ...   \n",
       "66975    NaN     NaN   NaN  ...     NaN     NaN    NaN    NaN       NaN   \n",
       "15881    NaN     NaN   NaN  ...     NaN     NaN    NaN    NaN       NaN   \n",
       "280      NaN     NaN   NaN  ...     NaN     NaN    NaN    NaN       NaN   \n",
       "54151    NaN     NaN   NaN  ...     NaN     NaN    NaN    NaN       NaN   \n",
       "81926    NaN     NaN   NaN  ...     NaN     NaN    NaN    NaN       NaN   \n",
       "\n",
       "      d_world d_worldwide d_years d_you d_your  \n",
       "81887     NaN         NaN     NaN   NaN    NaN  \n",
       "64609     NaN         NaN     NaN   NaN    NaN  \n",
       "17752     NaN         NaN     NaN   NaN    NaN  \n",
       "44538     NaN         NaN     NaN   NaN    NaN  \n",
       "68507     NaN         NaN     NaN   NaN    NaN  \n",
       "...       ...         ...     ...   ...    ...  \n",
       "66975     NaN         NaN     NaN   NaN    NaN  \n",
       "15881     NaN         NaN     NaN   NaN    NaN  \n",
       "280       NaN         NaN     NaN   NaN    NaN  \n",
       "54151     NaN         NaN     NaN   NaN    NaN  \n",
       "81926     NaN         NaN     NaN   NaN    NaN  \n",
       "\n",
       "[42598 rows x 200 columns]"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "description_counts_train = pd.DataFrame(\n",
    "    columns=['d_' + col for col in tfidf.get_feature_names()], # add 'p_' to the start of each column name\n",
    "    index=X_train.index # keep original train index to ensure matching with the numeric data\n",
    ")\n",
    "description_counts_train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177a881c",
   "metadata": {
    "id": "177a881c"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66a51fa",
   "metadata": {
    "id": "e66a51fa"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b71964c7",
   "metadata": {
    "id": "b71964c7"
   },
   "source": [
    "### count vectorizer on review title headline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa95654",
   "metadata": {
    "id": "4fa95654",
    "outputId": "edf55408-c6ac-4aaf-99d1-2b57201cf142"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<42598x12 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 34281 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# 1. Instantiate \n",
    "bagofwords = CountVectorizer(stop_words=\"english\",#removing stop words\n",
    "                             min_df=0.02,\n",
    "                             max_features= 2000,        #showing up in minimum 10% of documents     #only selecting 100 columns  \n",
    "                             )    \n",
    "\n",
    "# 2. Fit \n",
    "bagofwords.fit(train['summary'])\n",
    "\n",
    "# 3. Transform\n",
    "train_transformed = bagofwords.transform(train['summary'])\n",
    "train_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40ae0d9",
   "metadata": {
    "id": "d40ae0d9",
    "outputId": "96ed364b-fe1e-4fed-a0e3-7843df1f8f8b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42598, 12)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#making into an array\n",
    "train_transformed.toarray().sum(axis=0)\n",
    "train_transformed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0cbad07",
   "metadata": {
    "id": "b0cbad07",
    "outputId": "972fbe0f-cfce-4698-bc5f-3dcbd4c6469f"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plotting it out\n",
    "\n",
    "pos_word_counts = pd.DataFrame(\n",
    "    {\"counts\": train_transformed.toarray().sum(axis=0)},\n",
    "    index=bagofwords.get_feature_names_out()\n",
    ").sort_values(\"counts\", ascending=False)\n",
    "\n",
    "pos_word_counts.head(20).plot(kind=\"barh\", figsize=(15, 5), legend=False, color = \"navy\")\n",
    "plt.title(\"Figure: Top most frequently occurring words in review text\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e17fe78",
   "metadata": {
    "id": "8e17fe78"
   },
   "source": [
    "### count vectorizer on description headline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19277244",
   "metadata": {
    "id": "19277244"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "222cfe35",
   "metadata": {
    "id": "222cfe35"
   },
   "source": [
    "Porter stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b30fbe",
   "metadata": {
    "id": "16b30fbe",
    "outputId": "ef5642ff-4842-4271-cecb-1c7aaedc3e0b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "# the python string package has a helpful punctuation list\n",
    "import string\n",
    "print(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e749cc1",
   "metadata": {
    "id": "0e749cc1"
   },
   "outputs": [],
   "source": [
    "# library to clean data\n",
    "import re\n",
    " \n",
    "#to remove stopword\n",
    "from nltk.corpus import stopwords\n",
    " \n",
    "# for Stemming propose\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "ENGLISH_STOP_WORDS = stopwords.words('english')\n",
    "\n",
    "def my_tokenizer(sentence):\n",
    "    '''\n",
    "    Custom tokernizer for preprocessing. \n",
    "    \n",
    "    Input: string, raw document\n",
    "    Output:  \n",
    "    \n",
    "    '''\n",
    "    # remove punctuation and set to lower case\n",
    "    for punctuation_mark in string.punctuation:\n",
    "        sentence = sentence.replace(punctuation_mark,'').lower()\n",
    "\n",
    "    # split sentence into words\n",
    "    listofwords = sentence.split(' ')\n",
    "    listofstemmed_words = []\n",
    "    \n",
    "    ps = PorterStemmer()\n",
    "    \n",
    "    # remove stopwords and any tokens that are just empty strings\n",
    "    for word in listofwords:\n",
    "        if (not word in ENGLISH_STOP_WORDS) and (word!=''):\n",
    "            # Stem words\n",
    "            stemmed_word = ps.stem(word)\n",
    "            listofstemmed_words.append(stemmed_word)\n",
    "\n",
    "    return listofstemmed_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497c8ce8",
   "metadata": {
    "id": "497c8ce8"
   },
   "outputs": [],
   "source": [
    "sentence  = \"the quick fox jumped over the lazy dog\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13edcb6e",
   "metadata": {
    "id": "13edcb6e",
    "outputId": "917070cd-07eb-49f3-f67e-506dec8c8c29"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['quick', 'fox', 'jump', 'lazi', 'dog']"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_tokenizer(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ca5865",
   "metadata": {
    "id": "a2ca5865"
   },
   "outputs": [],
   "source": [
    "train, test  = train_test_split(combined_df, test_size=0.3, stratify= y, random_state = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47552894",
   "metadata": {
    "id": "47552894"
   },
   "outputs": [],
   "source": [
    "train_review = train[\"reviewText\"]\n",
    "test_review = test[\"reviewText\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d456a9f5",
   "metadata": {
    "id": "d456a9f5",
    "outputId": "379e655b-6ca7-4fc1-e62e-10db837001cd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mac/opt/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:516: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(97722, 7343)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bagofwords = CountVectorizer(min_df=5, \n",
    "                             tokenizer=my_tokenizer)\n",
    "bagofwords.fit(train_review)\n",
    "\n",
    "X_train_transformed = bagofwords.transform(train_review)\n",
    "X_test_transformed = bagofwords.transform(test_review)\n",
    "\n",
    "X_train_transformed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c09d9af",
   "metadata": {
    "id": "9c09d9af",
    "outputId": "84b123c9-1bf4-4ebd-b5f4-877b3b32c353"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mac/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "word_counts = pd.DataFrame(\n",
    "    {\"counts\": X_train_transformed.toarray().sum(axis=0)},\n",
    "    index=bagofwords.get_feature_names()\n",
    ").sort_values(\"counts\", ascending=False)\n",
    "\n",
    "word_counts.head(20).plot(kind=\"bar\", figsize=(15, 5), legend=False)\n",
    "plt.title(\"Top 20 most frequently occurring words\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a7a5ce",
   "metadata": {
    "id": "32a7a5ce"
   },
   "source": [
    "bi-grams "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32393cb4",
   "metadata": {
    "id": "32393cb4"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4653773f",
   "metadata": {
    "id": "4653773f"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610937a6",
   "metadata": {
    "id": "610937a6"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95bb5d3a",
   "metadata": {
    "id": "95bb5d3a"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f031c527",
   "metadata": {
    "id": "f031c527"
   },
   "source": [
    "Let's refit our best parameters to the model: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82baa978",
   "metadata": {
    "id": "82baa978",
    "outputId": "f904b163-272d-4b60-be1c-3413f286e567"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': LogisticRegression(max_iter=10000, n_jobs=-1, random_state=8),\n",
       " 'model__C': 0.1,\n",
       " 'normalise': RobustScaler(),\n",
       " 'reduce_dim': None}"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fitted_logreg.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b13e9e",
   "metadata": {
    "id": "93b13e9e"
   },
   "source": [
    "From our final data set with log transformation we can see that the best hyper parameters have remained the same. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01d38cc",
   "metadata": {
    "id": "d01d38cc"
   },
   "source": [
    "Let's fit this to our final model and see the test accuracy score: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ba414d",
   "metadata": {
    "id": "89ba414d",
    "outputId": "49e87aaa-bb14-4a21-fb30-dee294173bc9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.70      0.73      1809\n",
      "           1       0.79      0.84      0.82      2458\n",
      "\n",
      "    accuracy                           0.78      4267\n",
      "   macro avg       0.78      0.77      0.77      4267\n",
      "weighted avg       0.78      0.78      0.78      4267\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get class predictions\n",
    "y_pred = logreg2.predict(X_test_rs)\n",
    "\n",
    "#classification\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "report_logreg = classification_report(y_test, y_pred)\n",
    "print(report_logreg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7790da9a",
   "metadata": {
    "id": "7790da9a"
   },
   "source": [
    "In the end we have dropped the accuracy score by 0.3% but increased our f1-score by 0.01 so we may need to make a few more tweaks but overall we can say that our model can predict sentiment given logistic regression. "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "capstone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "e0ba38779ffb9a4a239c9d3f794622790caf3152bd20f11a748f045f68414ade"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
